"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9962],{1259:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-04-vla/week-12/ch19-grasping","title":"Robotic Grasping","description":"Learning Objectives","source":"@site/docs/module-04-vla/week-12/ch19-grasping.md","sourceDirName":"module-04-vla/week-12","slug":"/module-04-vla/week-12/ch19-grasping","permalink":"/AI_COURSE_book/docs/module-04-vla/week-12/ch19-grasping","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vla/week-12/ch19-grasping.md","tags":[],"version":"current","sidebarPosition":20,"frontMatter":{"id":"ch19-grasping","title":"Robotic Grasping","sidebar_label":"Robotic Grasping","sidebar_position":20},"sidebar":"textbookSidebar","previous":{"title":"Joint-Level Control","permalink":"/AI_COURSE_book/docs/module-04-vla/week-11/ch18-joint-control"},"next":{"title":"Bipedal Walking and Gaits","permalink":"/AI_COURSE_book/docs/module-04-vla/week-12/ch20-walking-gaits"}}');var i=s(4848),a=s(8453);const t={id:"ch19-grasping",title:"Robotic Grasping",sidebar_label:"Robotic Grasping",sidebar_position:20},o="Robotic Grasping",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Grasp Planning Algorithms",id:"1-grasp-planning-algorithms",level:2},{value:"Analytical Grasp Synthesis",id:"analytical-grasp-synthesis",level:3},{value:"Sampling-Based Grasp Planning",id:"sampling-based-grasp-planning",level:3},{value:"Database-Driven Grasp Planning",id:"database-driven-grasp-planning",level:3},{value:"2. Force Closure Analysis",id:"2-force-closure-analysis",level:2},{value:"Contact Mechanics and Friction Cones",id:"contact-mechanics-and-friction-cones",level:3},{value:"Grasp Matrix and Wrench Space",id:"grasp-matrix-and-wrench-space",level:3},{value:"Grasp Quality Metrics",id:"grasp-quality-metrics",level:3},{value:"3. Tactile Sensing",id:"3-tactile-sensing",level:2},{value:"Tactile Sensor Technologies",id:"tactile-sensor-technologies",level:3},{value:"Force/Torque Sensing",id:"forcetorque-sensing",level:3},{value:"4. Manipulation Primitives",id:"4-manipulation-primitives",level:2},{value:"Pick and Place",id:"pick-and-place",level:3},{value:"In-Hand Manipulation",id:"in-hand-manipulation",level:3},{value:"5. Learning-Based Grasping",id:"5-learning-based-grasping",level:2},{value:"Data-Driven Grasp Synthesis",id:"data-driven-grasp-synthesis",level:3},{value:"Reinforcement Learning for Grasping",id:"reinforcement-learning-for-grasping",level:3},{value:"Vision-Language-Action Models for Grasping",id:"vision-language-action-models-for-grasping",level:3},{value:"Practical Example",id:"practical-example",level:2},{value:"Common Challenges and Solutions",id:"common-challenges-and-solutions",level:2},{value:"Challenge 1: Grasp Failure Due to Perception Errors",id:"challenge-1-grasp-failure-due-to-perception-errors",level:3},{value:"Challenge 2: Slip During Manipulation",id:"challenge-2-slip-during-manipulation",level:3},{value:"Challenge 3: Grasping Novel Objects",id:"challenge-3-grasping-novel-objects",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"robotic-grasping",children:"Robotic Grasping"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design and implement grasp planning algorithms that generate stable grasps for diverse object geometries"}),"\n",(0,i.jsx)(n.li,{children:"Analyze force closure properties to ensure grasps can resist external disturbances"}),"\n",(0,i.jsx)(n.li,{children:"Integrate tactile sensing for feedback-driven manipulation and grasp adjustment"}),"\n",(0,i.jsx)(n.li,{children:"Implement manipulation primitives for pick-and-place, reorientation, and assembly tasks"}),"\n",(0,i.jsx)(n.li,{children:"Apply learning-based approaches to improve grasping performance through data-driven methods"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Robotic grasping is the fundamental skill that enables robots to physically interact with their environment, manipulate objects, and perform useful tasks. From industrial assembly lines to household assistance, the ability to reliably grasp and manipulate objects of varying shapes, sizes, materials, and weights is essential for practical robot deployment. Grasping involves complex interactions between perception, planning, control, and mechanical design, making it one of the most challenging problems in robotics."}),"\n",(0,i.jsx)(n.p,{children:"A successful grasp must satisfy multiple constraints simultaneously: the gripper must make contact with the object at appropriate locations, apply sufficient force to prevent slipping, avoid collisions with the environment, and enable subsequent manipulation actions. These requirements are complicated by uncertainties in object pose estimation, surface friction properties, and sensor noise. Classical approaches to grasping rely on analytical models of contact mechanics and optimization-based planning, while modern learning-based methods leverage large datasets and neural networks to discover effective grasping strategies."}),"\n",(0,i.jsx)(n.p,{children:"This chapter builds on URDF modeling (Chapter 17) and manipulation concepts to provide a comprehensive treatment of robotic grasping. We'll cover grasp planning algorithms that search for optimal contact configurations, force closure analysis that ensures stability, tactile sensing for closed-loop control, manipulation primitives that compose complex behaviors, and learning-based approaches that improve through experience. These techniques are critical for Vision-Language-Action (VLA) models that must translate high-level commands into precise manipulation actions."}),"\n",(0,i.jsx)(n.h2,{id:"1-grasp-planning-algorithms",children:"1. Grasp Planning Algorithms"}),"\n",(0,i.jsx)(n.h3,{id:"analytical-grasp-synthesis",children:"Analytical Grasp Synthesis"}),"\n",(0,i.jsx)(n.p,{children:"Grasp planning algorithms determine where and how a gripper should make contact with an object to achieve a stable grasp. Analytical approaches model the object geometry, gripper kinematics, and contact physics to compute candidate grasps. A common method is to sample points on the object surface, compute surface normals, and evaluate grasp quality metrics such as force closure, distance from center of mass, and collision avoidance."}),"\n",(0,i.jsx)(n.p,{children:"For a parallel-jaw gripper, the grasp planning problem simplifies to finding two opposing contact points on the object surface such that:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"The line connecting the contacts passes through or near the object's center of mass"}),"\n",(0,i.jsx)(n.li,{children:"The contact normals are anti-parallel (opposite directions)"}),"\n",(0,i.jsx)(n.li,{children:"The gripper width accommodates the object"}),"\n",(0,i.jsx)(n.li,{children:"No collisions occur with the object or environment"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example grasp sampling for a cylinder:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass GraspSampler:\n    def __init__(self, gripper_width_min=0.02, gripper_width_max=0.12):\n        self.gripper_width_min = gripper_width_min\n        self.gripper_width_max = gripper_width_max\n\n    def sample_cylinder_grasps(self, radius, height, num_samples=100):\n        """\n        Sample parallel-jaw grasps around a cylinder.\n\n        Args:\n            radius: Cylinder radius (m)\n            height: Cylinder height (m)\n            num_samples: Number of grasp candidates to generate\n\n        Returns:\n            List of grasp poses (position, orientation, width)\n        """\n        grasps = []\n\n        for i in range(num_samples):\n            # Random approach angle around cylinder\n            theta = np.random.uniform(0, 2 * np.pi)\n\n            # Random height along cylinder\n            z = np.random.uniform(-height / 2, height / 2)\n\n            # Grasp position on cylinder surface\n            x = radius * np.cos(theta)\n            y = radius * np.sin(theta)\n            position = np.array([x, y, z])\n\n            # Grasp orientation (gripper z-axis points toward cylinder center)\n            approach_vector = -np.array([x, y, 0]) / np.linalg.norm([x, y, 0])\n            # Binormal (perpendicular to approach and world z)\n            binormal = np.array([0, 0, 1])\n            # Normal (completes right-handed frame)\n            normal = np.cross(approach_vector, binormal)\n\n            # Rotation matrix from gripper frame to world frame\n            rotation = np.column_stack([normal, binormal, approach_vector])\n\n            # Gripper width (diameter of cylinder)\n            width = 2 * radius\n\n            if self.gripper_width_min <= width <= self.gripper_width_max:\n                grasps.append({\n                    \'position\': position,\n                    \'rotation\': rotation,\n                    \'width\': width,\n                    \'score\': self.evaluate_grasp(position, rotation, width)\n                })\n\n        return sorted(grasps, key=lambda g: g[\'score\'], reverse=True)\n\n    def evaluate_grasp(self, position, rotation, width):\n        """\n        Score a grasp based on heuristics.\n        Higher score is better.\n        """\n        # Prefer grasps near vertical axis (z-axis)\n        vertical_alignment = abs(position[2])\n\n        # Prefer wider grasps (more stable)\n        width_score = width / self.gripper_width_max\n\n        # Combine scores (weights can be tuned)\n        return 0.5 * vertical_alignment + 0.5 * width_score\n'})}),"\n",(0,i.jsx)(n.p,{children:"This sampler generates grasp candidates by randomly placing the gripper around the cylinder and evaluating each grasp based on stability heuristics. More sophisticated methods use volumetric representations (voxel grids, point clouds) or shape primitives (boxes, cylinders, spheres) to handle arbitrary object geometries."}),"\n",(0,i.jsx)(n.h3,{id:"sampling-based-grasp-planning",children:"Sampling-Based Grasp Planning"}),"\n",(0,i.jsx)(n.p,{children:"For complex objects, sampling-based methods generate many candidate grasps and rank them using quality metrics. A popular approach is GraspIt!, which represents objects as triangle meshes and searches for stable grasps by:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Placing gripper at random poses relative to the object"}),"\n",(0,i.jsx)(n.li,{children:"Closing gripper fingers until contact is made"}),"\n",(0,i.jsx)(n.li,{children:"Computing grasp quality (force closure, epsilon quality)"}),"\n",(0,i.jsx)(n.li,{children:"Iteratively refining grasp pose using optimization"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example sampling-based grasp planner interface:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from scipy.spatial.transform import Rotation\n\nclass SamplingGraspPlanner:\n    def __init__(self, object_mesh, gripper_model):\n        self.object_mesh = object_mesh\n        self.gripper_model = gripper_model\n\n    def plan_grasps(self, num_samples=1000, top_k=10):\n        """\n        Generate and rank grasp candidates.\n\n        Args:\n            num_samples: Number of grasps to sample\n            top_k: Number of top grasps to return\n\n        Returns:\n            List of top-ranked grasps\n        """\n        candidates = []\n\n        for _ in range(num_samples):\n            # Sample random grasp pose\n            grasp_pose = self.sample_grasp_pose()\n\n            # Check collision\n            if self.check_collision(grasp_pose):\n                continue\n\n            # Compute contacts\n            contacts = self.compute_contacts(grasp_pose)\n\n            if len(contacts) < 2:\n                continue  # Need at least 2 contacts\n\n            # Evaluate grasp quality\n            quality = self.evaluate_force_closure(contacts)\n\n            candidates.append({\n                \'pose\': grasp_pose,\n                \'contacts\': contacts,\n                \'quality\': quality\n            })\n\n        # Return top-k grasps\n        candidates.sort(key=lambda g: g[\'quality\'], reverse=True)\n        return candidates[:top_k]\n\n    def sample_grasp_pose(self):\n        """Sample random SE(3) grasp pose near object."""\n        # Random position near object center of mass\n        com = self.object_mesh.center_mass\n        position = com + np.random.randn(3) * 0.05  # 5cm std dev\n\n        # Random orientation\n        rotation = Rotation.random().as_matrix()\n\n        return {\'position\': position, \'rotation\': rotation}\n\n    def check_collision(self, grasp_pose):\n        """Check if gripper collides with object at given pose."""\n        # Transform gripper mesh to grasp pose\n        gripper_mesh = self.gripper_model.transform(\n            grasp_pose[\'position\'],\n            grasp_pose[\'rotation\']\n        )\n        # Check intersection with object mesh\n        return self.object_mesh.intersects(gripper_mesh)\n\n    def compute_contacts(self, grasp_pose):\n        """Compute contact points between gripper and object."""\n        # Simulate gripper closure and find contact points\n        # (Simplified: returns contact positions and normals)\n        contacts = []\n        # ... contact detection logic ...\n        return contacts\n\n    def evaluate_force_closure(self, contacts):\n        """\n        Compute force closure quality metric.\n        Returns value in [0, 1], higher is better.\n        """\n        # Simplified force closure check (see next section for details)\n        if len(contacts) < 3:\n            return 0.0  # Need at least 3 contacts for 3D force closure\n\n        # Compute grasp matrix and evaluate rank\n        G = self.compute_grasp_matrix(contacts)\n        # Compute epsilon quality (largest perturbation wrench that can be resisted)\n        epsilon = self.compute_epsilon_quality(G)\n        return epsilon\n'})}),"\n",(0,i.jsx)(n.p,{children:"Sampling-based planners are flexible and can handle arbitrary object geometries, but they require many samples to find high-quality grasps. Modern GPU-accelerated simulators like NVIDIA Isaac Sim can evaluate thousands of grasp candidates in parallel, enabling real-time grasp planning."}),"\n",(0,i.jsx)(n.h3,{id:"database-driven-grasp-planning",children:"Database-Driven Grasp Planning"}),"\n",(0,i.jsx)(n.p,{children:"An alternative to analytical planning is to use pre-computed grasp databases. These databases store successful grasps for common object categories (mugs, bottles, boxes), indexed by shape descriptors. At runtime, the planner:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Computes shape descriptor for the target object"}),"\n",(0,i.jsx)(n.li,{children:"Retrieves similar objects from the database"}),"\n",(0,i.jsx)(n.li,{children:"Transfers grasps from database objects to the target"}),"\n",(0,i.jsx)(n.li,{children:"Refines grasps using local optimization"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This approach is fast and leverages prior successful grasps, but requires a large, diverse database and may not generalize to novel object categories."}),"\n",(0,i.jsx)(n.h2,{id:"2-force-closure-analysis",children:"2. Force Closure Analysis"}),"\n",(0,i.jsx)(n.h3,{id:"contact-mechanics-and-friction-cones",children:"Contact Mechanics and Friction Cones"}),"\n",(0,i.jsx)(n.p,{children:"A grasp is in force closure if it can resist arbitrary external wrenches (forces and torques) applied to the object. This requires that the contact forces can balance any disturbance. For a single contact point with Coulomb friction, the feasible contact forces lie within a friction cone:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"f \u2208 FC = {f : ||f_tangent|| \u2264 \u03bc f_normal, f_normal \u2265 0}"})}),"\n",(0,i.jsx)(n.p,{children:"where \u03bc is the coefficient of friction, f_normal is the force component along the surface normal, and f_tangent is the tangential component. For hard contact (no friction), the cone degenerates to a ray along the normal. For soft contact with friction, the cone opens with angle \u03b1 = arctan(\u03bc)."}),"\n",(0,i.jsx)(n.p,{children:"Example friction cone computation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def compute_friction_cone(contact_normal, mu=0.5, num_edges=4):\n    """\n    Compute friction cone basis vectors.\n\n    Args:\n        contact_normal: 3D unit vector normal to contact surface\n        mu: Coefficient of friction\n        num_edges: Number of cone edge vectors to generate\n\n    Returns:\n        Array of friction cone edge vectors (num_edges, 3)\n    """\n    # Find two orthogonal tangent vectors\n    if abs(contact_normal[2]) < 0.9:\n        tangent1 = np.cross(contact_normal, np.array([0, 0, 1]))\n    else:\n        tangent1 = np.cross(contact_normal, np.array([1, 0, 0]))\n    tangent1 /= np.linalg.norm(tangent1)\n    tangent2 = np.cross(contact_normal, tangent1)\n    tangent2 /= np.linalg.norm(tangent2)\n\n    # Generate cone edge vectors\n    cone_edges = []\n    for i in range(num_edges):\n        angle = 2 * np.pi * i / num_edges\n        tangent = np.cos(angle) * tangent1 + np.sin(angle) * tangent2\n        edge = contact_normal + mu * tangent\n        edge /= np.linalg.norm(edge)\n        cone_edges.append(edge)\n\n    return np.array(cone_edges)\n'})}),"\n",(0,i.jsx)(n.p,{children:"For a grasp with n contact points, the combined friction cone is the Minkowski sum of individual cones. A grasp achieves force closure if the origin of the wrench space lies in the interior of the convex hull of the friction cone wrenches."}),"\n",(0,i.jsx)(n.h3,{id:"grasp-matrix-and-wrench-space",children:"Grasp Matrix and Wrench Space"}),"\n",(0,i.jsx)(n.p,{children:"The grasp matrix G maps contact forces to object wrenches (6D force-torque vectors). For n contact points with position r_i and friction cone edges f_ij, the grasp matrix is:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"G = [f_11, ..., f_1k, f_21, ..., f_nk]"}),"\n**  = [f_ij]**\n**    [r_i \xd7 f_ij]**"]}),"\n",(0,i.jsx)(n.p,{children:"Each column represents a unit wrench generated by a friction cone edge. The grasp achieves force closure if the origin is in the interior of the convex hull of G's columns, which can be tested using linear programming or computational geometry algorithms."}),"\n",(0,i.jsx)(n.p,{children:"Example grasp matrix computation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def compute_grasp_matrix(contacts, mu=0.5, num_edges=4):\n    """\n    Compute grasp matrix from contact points.\n\n    Args:\n        contacts: List of contact dicts with \'position\' and \'normal\' keys\n        mu: Coefficient of friction\n        num_edges: Number of friction cone edges per contact\n\n    Returns:\n        Grasp matrix G (6, n*num_edges)\n    """\n    G_columns = []\n\n    for contact in contacts:\n        position = contact[\'position\']\n        normal = contact[\'normal\']\n\n        # Compute friction cone for this contact\n        cone_edges = compute_friction_cone(normal, mu, num_edges)\n\n        for edge in cone_edges:\n            # Force component\n            force = edge\n\n            # Torque component (r \xd7 f)\n            torque = np.cross(position, force)\n\n            # Wrench (6D: force + torque)\n            wrench = np.concatenate([force, torque])\n            G_columns.append(wrench)\n\n    return np.column_stack(G_columns)\n\n\ndef check_force_closure(G, tolerance=1e-6):\n    """\n    Check if grasp matrix G represents a force closure grasp.\n\n    Args:\n        G: Grasp matrix (6, m) where m = n_contacts * n_edges\n        tolerance: Numerical tolerance for interior point test\n\n    Returns:\n        Boolean indicating force closure\n    """\n    from scipy.optimize import linprog\n\n    # Check if origin is in interior of convex hull of G\'s columns\n    # Formulate as LP: min c^T \u03bb subject to G \u03bb = 0, \u03bb \u2265 0, sum(\u03bb) = 1\n    # If optimal objective is close to zero, origin is in convex hull\n\n    m = G.shape[1]\n    c = np.zeros(m)  # Dummy objective\n\n    # Equality constraint: G \u03bb = 0 (origin is in span)\n    A_eq = G\n    b_eq = np.zeros(6)\n\n    # Inequality constraint: \u03bb \u2265 0, sum(\u03bb) = 1\n    A_ub = -np.eye(m)\n    b_ub = np.zeros(m)\n\n    # Additional constraint: sum(\u03bb) = 1\n    A_eq = np.vstack([A_eq, np.ones(m)])\n    b_eq = np.append(b_eq, 1)\n\n    result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, method=\'highs\')\n\n    return result.success and np.linalg.norm(G @ result.x) < tolerance\n'})}),"\n",(0,i.jsx)(n.p,{children:"This force closure test checks whether the origin lies in the convex hull of the grasp matrix columns, indicating that any external wrench can be balanced by contact forces within the friction cones."}),"\n",(0,i.jsx)(n.h3,{id:"grasp-quality-metrics",children:"Grasp Quality Metrics"}),"\n",(0,i.jsx)(n.p,{children:"Beyond binary force closure, quality metrics quantify how robust a grasp is to disturbances. Common metrics include:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Epsilon quality (\u03b5)"}),": Largest magnitude of uniform wrench that can be resisted. Computed as the distance from the origin to the boundary of the grasp wrench space."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Volume quality"}),": Volume of the grasp wrench space, normalized by a reference ellipsoid. Larger volume indicates more disturbances can be resisted."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Ferrari-Canny metric"}),": Minimum effort to generate a unit wrench in any direction. Related to the smallest singular value of the grasp matrix."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example epsilon quality computation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def compute_epsilon_quality(G):\n    """\n    Compute epsilon quality (largest uniform wrench that can be resisted).\n\n    Args:\n        G: Grasp matrix (6, m)\n\n    Returns:\n        Epsilon quality (scalar)\n    """\n    from scipy.spatial import ConvexHull\n\n    # Normalize grasp matrix columns to unit length\n    G_normalized = G / np.linalg.norm(G, axis=0, keepdims=True)\n\n    # Compute convex hull of normalized columns\n    hull = ConvexHull(G_normalized.T)\n\n    # Epsilon is distance from origin to closest facet\n    epsilon = np.inf\n    for eq in hull.equations:\n        # Facet equation: a^T x + b = 0 where ||a|| = 1\n        a = eq[:-1]\n        b = eq[-1]\n        # Distance from origin to facet\n        distance = abs(b)\n        epsilon = min(epsilon, distance)\n\n    return epsilon\n'})}),"\n",(0,i.jsx)(n.p,{children:"Higher epsilon quality indicates a more robust grasp that can resist larger disturbances before the object slips."}),"\n",(0,i.jsx)(n.h2,{id:"3-tactile-sensing",children:"3. Tactile Sensing"}),"\n",(0,i.jsx)(n.h3,{id:"tactile-sensor-technologies",children:"Tactile Sensor Technologies"}),"\n",(0,i.jsx)(n.p,{children:"Tactile sensors provide local force, pressure, or deformation measurements at the gripper-object interface. They enable feedback-driven manipulation, slip detection, and adaptive grasping. Common tactile sensor technologies include:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Resistive sensors"}),": Measure resistance change under pressure (e.g., piezoresistive, force-sensing resistors). Simple and low-cost but limited spatial resolution."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Capacitive sensors"}),": Measure capacitance change due to deformation. Higher resolution than resistive sensors, used in GelSight and similar optical tactile sensors."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Optical sensors"}),": Camera-based sensors that track surface deformation using markers or illumination patterns. Very high resolution (submillimeter) and can measure 3D deformation fields. Examples: GelSight, TacTip, DIGIT."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Piezoelectric sensors"}),": Measure dynamic force changes. Good for vibration and slip detection but not static forces."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"For robotic grasping, optical tactile sensors like GelSight provide rich information about contact geometry, forces, and slip:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\n\nclass GelSightSensor:\n    def __init__(self, camera_id=0, resolution=(640, 480)):\n        self.cap = cv2.VideoCapture(camera_id)\n        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, resolution[0])\n        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, resolution[1])\n        self.reference_frame = None\n\n    def capture_reference(self):\n        """Capture reference image (no contact)."""\n        ret, frame = self.cap.read()\n        if ret:\n            self.reference_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        return ret\n\n    def get_contact_image(self):\n        """Capture current contact image."""\n        ret, frame = self.cap.read()\n        if ret:\n            return cv2.cvtColor(frame, cv2.COLOR_BGR_GRAY)\n        return None\n\n    def compute_depth_map(self, contact_image):\n        """\n        Compute depth map from contact image.\n\n        Args:\n            contact_image: Grayscale contact image\n\n        Returns:\n            Depth map (normalized displacement field)\n        """\n        if self.reference_frame is None:\n            raise ValueError("Reference frame not captured")\n\n        # Compute difference from reference\n        diff = cv2.absdiff(contact_image, self.reference_frame)\n\n        # Apply Gaussian blur to reduce noise\n        diff_blurred = cv2.GaussianBlur(diff, (5, 5), 0)\n\n        # Normalize to [0, 1]\n        depth_map = diff_blurred / 255.0\n\n        return depth_map\n\n    def detect_slip(self, depth_map_current, depth_map_previous, threshold=0.05):\n        """\n        Detect slip by comparing consecutive depth maps.\n\n        Args:\n            depth_map_current: Current depth map\n            depth_map_previous: Previous depth map\n            threshold: Motion threshold for slip detection\n\n        Returns:\n            Boolean indicating slip detected\n        """\n        # Compute optical flow between depth maps\n        flow = cv2.calcOpticalFlowFarneback(\n            (depth_map_previous * 255).astype(np.uint8),\n            (depth_map_current * 255).astype(np.uint8),\n            None, 0.5, 3, 15, 3, 5, 1.2, 0\n        )\n\n        # Compute flow magnitude\n        flow_magnitude = np.linalg.norm(flow, axis=2)\n\n        # Slip detected if mean flow exceeds threshold\n        mean_flow = np.mean(flow_magnitude)\n        return mean_flow > threshold\n'})}),"\n",(0,i.jsx)(n.p,{children:"This GelSight interface captures depth maps from contact deformation and detects slip by tracking optical flow between consecutive frames. Slip detection enables reactive behaviors like increasing grip force or adjusting grasp pose."}),"\n",(0,i.jsx)(n.h3,{id:"forcetorque-sensing",children:"Force/Torque Sensing"}),"\n",(0,i.jsx)(n.p,{children:"Wrist-mounted force/torque (F/T) sensors measure net forces and torques applied to the gripper. Unlike tactile sensors, F/T sensors provide global measurements but don't resolve individual contact locations. They are useful for detecting contact events, measuring object weight, and impedance control."}),"\n",(0,i.jsx)(n.p,{children:"Example F/T sensor integration:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from geometry_msgs.msg import WrenchStamped\n\nclass ForceTorqueSensor:\n    def __init__(self, node):\n        self.node = node\n        self.wrench = None\n        self.bias = np.zeros(6)  # Force and torque bias\n\n        # Subscribe to F/T sensor topic\n        self.subscriber = node.create_subscription(\n            WrenchStamped,\n            \'/ft_sensor/wrench\',\n            self.wrench_callback,\n            10\n        )\n\n    def wrench_callback(self, msg):\n        """Store latest wrench measurement."""\n        force = np.array([msg.wrench.force.x, msg.wrench.force.y, msg.wrench.force.z])\n        torque = np.array([msg.wrench.torque.x, msg.wrench.torque.y, msg.wrench.torque.z])\n        self.wrench = np.concatenate([force, torque]) - self.bias\n\n    def calibrate(self, num_samples=100):\n        """\n        Calibrate sensor by averaging measurements (no contact).\n        """\n        measurements = []\n        rate = self.node.create_rate(100)  # 100 Hz\n\n        for _ in range(num_samples):\n            if self.wrench is not None:\n                measurements.append(self.wrench + self.bias)\n            rate.sleep()\n\n        self.bias = np.mean(measurements, axis=0)\n        self.node.get_logger().info(f\'F/T sensor calibrated with bias: {self.bias}\')\n\n    def detect_contact(self, force_threshold=1.0):\n        """\n        Detect contact based on force magnitude.\n\n        Args:\n            force_threshold: Minimum force (N) to consider contact\n\n        Returns:\n            Boolean indicating contact detected\n        """\n        if self.wrench is None:\n            return False\n\n        force_magnitude = np.linalg.norm(self.wrench[:3])\n        return force_magnitude > force_threshold\n'})}),"\n",(0,i.jsx)(n.p,{children:"F/T sensors are commonly used in compliant manipulation strategies where the robot adjusts its motion based on sensed forces, enabling gentle contact and robust insertion tasks."}),"\n",(0,i.jsx)(n.h2,{id:"4-manipulation-primitives",children:"4. Manipulation Primitives"}),"\n",(0,i.jsx)(n.h3,{id:"pick-and-place",children:"Pick and Place"}),"\n",(0,i.jsx)(n.p,{children:"Pick-and-place is the fundamental manipulation primitive consisting of four phases:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Approach"}),": Move gripper to pre-grasp pose above object"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grasp"}),": Close gripper and secure object"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lift"}),": Raise object to clear workspace"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Place"}),": Move to target location and release"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example pick-and-place implementation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class PickAndPlaceController:\n    def __init__(self, move_group, gripper):\n        self.move_group = move_group  # MoveIt move group\n        self.gripper = gripper  # Gripper controller\n\n    def pick(self, grasp_pose, approach_distance=0.1):\n        """\n        Execute pick primitive.\n\n        Args:\n            grasp_pose: Target grasp pose (SE(3))\n            approach_distance: Distance to retract before approach (m)\n\n        Returns:\n            Success boolean\n        """\n        # 1. Move to pre-grasp pose (approach from above)\n        pre_grasp_pose = self.compute_pre_grasp_pose(grasp_pose, approach_distance)\n        if not self.move_group.go(pre_grasp_pose, wait=True):\n            return False\n\n        # 2. Open gripper\n        self.gripper.open()\n\n        # 3. Approach grasp pose\n        if not self.move_group.go(grasp_pose, wait=True):\n            return False\n\n        # 4. Close gripper\n        self.gripper.close()\n\n        # 5. Lift object\n        lift_pose = self.compute_lift_pose(grasp_pose, lift_distance=0.15)\n        if not self.move_group.go(lift_pose, wait=True):\n            self.gripper.open()  # Release if lift fails\n            return False\n\n        return True\n\n    def place(self, target_pose, approach_distance=0.1):\n        """\n        Execute place primitive.\n\n        Args:\n            target_pose: Target placement pose (SE(3))\n            approach_distance: Distance to approach before placing (m)\n\n        Returns:\n            Success boolean\n        """\n        # 1. Move to pre-place pose\n        pre_place_pose = self.compute_pre_grasp_pose(target_pose, approach_distance)\n        if not self.move_group.go(pre_place_pose, wait=True):\n            return False\n\n        # 2. Lower to place pose\n        if not self.move_group.go(target_pose, wait=True):\n            return False\n\n        # 3. Open gripper (release object)\n        self.gripper.open()\n\n        # 4. Retract\n        retract_pose = self.compute_pre_grasp_pose(target_pose, approach_distance)\n        self.move_group.go(retract_pose, wait=True)\n\n        return True\n\n    def compute_pre_grasp_pose(self, grasp_pose, distance):\n        """Compute pre-grasp pose by retracting along approach direction."""\n        approach_vector = grasp_pose[\'rotation\'][:, 2]  # z-axis of gripper\n        pre_grasp_position = grasp_pose[\'position\'] - distance * approach_vector\n        return {\'position\': pre_grasp_position, \'rotation\': grasp_pose[\'rotation\']}\n\n    def compute_lift_pose(self, grasp_pose, lift_distance):\n        """Compute lift pose by moving vertically upward."""\n        lift_position = grasp_pose[\'position\'] + np.array([0, 0, lift_distance])\n        return {\'position\': lift_position, \'rotation\': grasp_pose[\'rotation\']}\n'})}),"\n",(0,i.jsx)(n.p,{children:"This controller sequences motion primitives (move, open, close, lift) to perform reliable pick-and-place operations. Robustness can be improved with force feedback, slip detection, and error recovery strategies."}),"\n",(0,i.jsx)(n.h3,{id:"in-hand-manipulation",children:"In-Hand Manipulation"}),"\n",(0,i.jsx)(n.p,{children:"In-hand manipulation reorients an object within the gripper without placing it down. Techniques include:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Finger gaiting"}),": Alternately releasing and re-grasping with individual fingers to walk around the object"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pivoting"}),": Rotating object about a fixed contact point"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rolling"}),": Moving object by coordinating finger joint motions"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example pivoting controller:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class PivotController:\n    def __init__(self, gripper):\n        self.gripper = gripper\n\n    def pivot(self, angle, pivot_finger='left'):\n        \"\"\"\n        Pivot object by rotating about one contact point.\n\n        Args:\n            angle: Desired rotation angle (radians)\n            pivot_finger: Which finger to keep fixed ('left' or 'right')\n\n        Returns:\n            Success boolean\n        \"\"\"\n        # 1. Reduce grip force on non-pivot finger\n        if pivot_finger == 'left':\n            self.gripper.set_finger_force('left', high_force=True)\n            self.gripper.set_finger_force('right', high_force=False)\n        else:\n            self.gripper.set_finger_force('left', high_force=False)\n            self.gripper.set_finger_force('right', high_force=True)\n\n        # 2. Move non-pivot finger to create rotation\n        if pivot_finger == 'left':\n            finger_displacement = self.compute_arc_displacement(angle, radius=0.05)\n            self.gripper.move_finger('right', displacement=finger_displacement)\n        else:\n            finger_displacement = self.compute_arc_displacement(angle, radius=0.05)\n            self.gripper.move_finger('left', displacement=finger_displacement)\n\n        # 3. Restore equal grip force\n        self.gripper.set_finger_force('left', high_force=True)\n        self.gripper.set_finger_force('right', high_force=True)\n\n        return True\n\n    def compute_arc_displacement(self, angle, radius):\n        \"\"\"Compute arc length for given angle and radius.\"\"\"\n        return angle * radius\n"})}),"\n",(0,i.jsx)(n.p,{children:"In-hand manipulation enables dexterous tasks like screwdriver manipulation, where the object must be continuously reoriented during use."}),"\n",(0,i.jsx)(n.h2,{id:"5-learning-based-grasping",children:"5. Learning-Based Grasping"}),"\n",(0,i.jsx)(n.h3,{id:"data-driven-grasp-synthesis",children:"Data-Driven Grasp Synthesis"}),"\n",(0,i.jsx)(n.p,{children:"Learning-based methods train neural networks to predict grasp success directly from sensor data (images, point clouds), bypassing analytical models. A common approach is to train a convolutional neural network (CNN) to predict grasp quality for candidate grasps in a depth image."}),"\n",(0,i.jsx)(n.p,{children:"Example CNN-based grasp detector:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\n\nclass GraspQualityCNN(nn.Module):\n    def __init__(self, input_channels=1):\n        super().__init__()\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(input_channels, 64, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n\n        self.fc_layers = nn.Sequential(\n            nn.Linear(256 * 8 * 8, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 1),\n            nn.Sigmoid()  # Output grasp quality in [0, 1]\n        )\n\n    def forward(self, depth_image):\n        """\n        Predict grasp quality from depth image patch.\n\n        Args:\n            depth_image: (B, 1, 64, 64) depth patches around grasp candidates\n\n        Returns:\n            Grasp quality scores (B,)\n        """\n        features = self.conv_layers(depth_image)\n        features_flat = features.view(features.size(0), -1)\n        quality = self.fc_layers(features_flat)\n        return quality.squeeze()\n\n\nclass GraspNetworkTrainer:\n    def __init__(self, model, device=\'cuda\'):\n        self.model = model.to(device)\n        self.device = device\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n        self.criterion = nn.BCELoss()  # Binary cross-entropy for success/failure\n\n    def train_epoch(self, dataloader):\n        """Train for one epoch."""\n        self.model.train()\n        total_loss = 0.0\n\n        for depth_patches, labels in dataloader:\n            depth_patches = depth_patches.to(self.device)\n            labels = labels.to(self.device)\n\n            self.optimizer.zero_grad()\n            predictions = self.model(depth_patches)\n            loss = self.criterion(predictions, labels)\n            loss.backward()\n            self.optimizer.step()\n\n            total_loss += loss.item()\n\n        return total_loss / len(dataloader)\n\n    def evaluate(self, dataloader):\n        """Evaluate on validation set."""\n        self.model.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for depth_patches, labels in dataloader:\n                depth_patches = depth_patches.to(self.device)\n                labels = labels.to(self.device)\n\n                predictions = self.model(depth_patches)\n                predicted_labels = (predictions > 0.5).float()\n                correct += (predicted_labels == labels).sum().item()\n                total += labels.size(0)\n\n        return correct / total\n'})}),"\n",(0,i.jsx)(n.p,{children:"The network is trained on thousands of grasp attempts labeled with success/failure. At test time, candidate grasps are evaluated by extracting depth patches and feeding them through the network to predict success probability."}),"\n",(0,i.jsx)(n.h3,{id:"reinforcement-learning-for-grasping",children:"Reinforcement Learning for Grasping"}),"\n",(0,i.jsx)(n.p,{children:"Reinforcement learning (RL) enables robots to learn grasping policies through trial and error. The agent receives rewards for successful grasps and penalties for failures, gradually improving its policy. Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO) are popular RL algorithms for grasping."}),"\n",(0,i.jsx)(n.p,{children:"Example RL grasp policy structure:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class GraspPolicyNetwork(nn.Module):\n    def __init__(self, input_dim, action_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        )\n\n    def forward(self, state):\n        """\n        Compute action values for given state.\n\n        Args:\n            state: Robot state (joint angles, gripper pose, object features)\n\n        Returns:\n            Action values (Q-values for DQN or action logits for PPO)\n        """\n        return self.network(state)\n\n\nclass GraspEnvironment:\n    def __init__(self, simulator):\n        self.simulator = simulator\n        self.object = None\n\n    def reset(self):\n        """Reset environment and return initial state."""\n        self.object = self.simulator.spawn_random_object()\n        robot_state = self.simulator.get_robot_state()\n        object_state = self.simulator.get_object_state(self.object)\n        return np.concatenate([robot_state, object_state])\n\n    def step(self, action):\n        """\n        Execute action and return next state, reward, done.\n\n        Args:\n            action: Gripper pose (x, y, z, roll, pitch, yaw)\n\n        Returns:\n            next_state, reward, done, info\n        """\n        # Execute grasp attempt\n        success = self.simulator.attempt_grasp(action)\n\n        # Compute reward\n        if success:\n            reward = 1.0\n            done = True\n        else:\n            reward = -0.1\n            done = False\n\n        next_state = self.get_state()\n        return next_state, reward, done, {}\n'})}),"\n",(0,i.jsx)(n.p,{children:"RL-based grasping can learn complex, non-intuitive strategies and generalize to novel objects, but requires many training episodes (often millions) and careful reward shaping."}),"\n",(0,i.jsx)(n.h3,{id:"vision-language-action-models-for-grasping",children:"Vision-Language-Action Models for Grasping"}),"\n",(0,i.jsx)(n.p,{children:'Modern VLA models like RT-1, RT-2, and PaLM-E integrate vision, language understanding, and action prediction to enable natural language-commanded grasping. These models are trained on large-scale datasets of robot demonstrations with language annotations, learning to map instructions like "pick up the red cup" to grasp actions.'}),"\n",(0,i.jsx)(n.p,{children:"VLA pipeline:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision encoder"}),": Processes RGB-D images to extract visual features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language encoder"}),': Encodes task description ("pick up the red cup")']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fusion module"}),": Combines vision and language features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action decoder"}),": Predicts gripper pose and control commands"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example VLA interface for grasping:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class VLAGraspController:\n    def __init__(self, vla_model, camera, robot_controller):\n        self.vla_model = vla_model\n        self.camera = camera\n        self.robot_controller = robot_controller\n\n    def execute_language_command(self, command):\n        \"\"\"\n        Execute grasp based on natural language command.\n\n        Args:\n            command: Natural language instruction (e.g., \"pick up the blue bottle\")\n\n        Returns:\n            Success boolean\n        \"\"\"\n        # 1. Capture current scene\n        rgb_image = self.camera.get_rgb()\n        depth_image = self.camera.get_depth()\n\n        # 2. Query VLA model for grasp action\n        action = self.vla_model.predict(\n            rgb=rgb_image,\n            depth=depth_image,\n            instruction=command\n        )\n\n        # 3. Execute predicted grasp\n        grasp_pose = {\n            'position': action['position'],\n            'rotation': action['rotation'],\n            'width': action['gripper_width']\n        }\n\n        success = self.robot_controller.execute_grasp(grasp_pose)\n        return success\n"})}),"\n",(0,i.jsx)(n.p,{children:"VLA models enable intuitive, language-based robot control and can generalize to new tasks through few-shot learning or fine-tuning."}),"\n",(0,i.jsx)(n.h2,{id:"practical-example",children:"Practical Example"}),"\n",(0,i.jsx)(n.p,{children:"Let's implement a complete grasping pipeline that integrates perception, planning, and execution:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2, Image\nfrom geometry_msgs.msg import PoseStamped\nimport numpy as np\n\nclass GraspingPipeline(Node):\n    def __init__(self):\n        super().__init__(\'grasping_pipeline\')\n\n        # Subscribers\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth/image\', self.depth_callback, 10\n        )\n\n        # Publishers\n        self.grasp_pub = self.create_publisher(\n            PoseStamped, \'/grasp_pose\', 10\n        )\n\n        # Grasp planner\n        self.planner = SamplingGraspPlanner(object_mesh=None, gripper_model=None)\n\n        # Latest depth image\n        self.depth_image = None\n\n        self.get_logger().info(\'Grasping pipeline initialized\')\n\n    def depth_callback(self, msg):\n        """Store latest depth image."""\n        # Convert ROS Image to numpy array\n        self.depth_image = np.frombuffer(msg.data, dtype=np.float32).reshape(\n            (msg.height, msg.width)\n        )\n\n    def segment_object(self, depth_image):\n        """\n        Segment object from depth image.\n\n        Returns:\n            Object point cloud\n        """\n        # Simple segmentation: cluster points within distance range\n        mask = (depth_image > 0.3) & (depth_image < 1.5)  # 30cm to 150cm\n        object_points = np.column_stack(np.where(mask))\n        return object_points\n\n    def plan_grasp(self, object_points):\n        """\n        Plan grasp for segmented object.\n\n        Returns:\n            Best grasp pose\n        """\n        # Compute object bounding box\n        min_coords = np.min(object_points, axis=0)\n        max_coords = np.max(object_points, axis=0)\n        center = (min_coords + max_coords) / 2\n        size = max_coords - min_coords\n\n        # Simple heuristic: grasp from top, centered\n        grasp_pose = PoseStamped()\n        grasp_pose.header.frame_id = \'camera_depth_optical_frame\'\n        grasp_pose.header.stamp = self.get_clock().now().to_msg()\n        grasp_pose.pose.position.x = center[1]  # Column (x in camera frame)\n        grasp_pose.pose.position.y = center[0]  # Row (y in camera frame)\n        grasp_pose.pose.position.z = min_coords[2] - 0.05  # 5cm above object\n        grasp_pose.pose.orientation.w = 1.0  # No rotation\n\n        return grasp_pose\n\n    def execute_grasp_pipeline(self):\n        """Run complete grasping pipeline."""\n        if self.depth_image is None:\n            self.get_logger().warn(\'No depth image available\')\n            return\n\n        # 1. Segment object\n        object_points = self.segment_object(self.depth_image)\n        self.get_logger().info(f\'Segmented {len(object_points)} object points\')\n\n        # 2. Plan grasp\n        grasp_pose = self.plan_grasp(object_points)\n        self.get_logger().info(f\'Planned grasp at position {grasp_pose.pose.position}\')\n\n        # 3. Publish grasp pose\n        self.grasp_pub.publish(grasp_pose)\n\n\ndef main():\n    rclpy.init()\n    node = GraspingPipeline()\n\n    # Run pipeline every 2 seconds\n    timer = node.create_timer(2.0, node.execute_grasp_pipeline)\n\n    rclpy.spin(node)\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Implementation Points:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The pipeline subscribes to depth images, segments objects, plans grasps, and publishes grasp poses for execution"}),"\n",(0,i.jsx)(n.li,{children:"Object segmentation uses simple depth thresholding; production systems use learned segmentation models"}),"\n",(0,i.jsx)(n.li,{children:"Grasp planning uses a top-down heuristic; can be replaced with sampling-based or learning-based planners"}),"\n",(0,i.jsx)(n.li,{children:"The grasp pose is published for a separate motion controller to execute via MoveIt or similar"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"common-challenges-and-solutions",children:"Common Challenges and Solutions"}),"\n",(0,i.jsx)(n.h3,{id:"challenge-1-grasp-failure-due-to-perception-errors",children:"Challenge 1: Grasp Failure Due to Perception Errors"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Incorrect object pose estimation from noisy depth images or occlusions leads to grasp failures where the gripper misses the object or collides with surfaces."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Integrate multi-view perception by capturing images from multiple camera viewpoints and fusing them into a complete 3D model. Use learned segmentation and pose estimation networks (e.g., DOPE, PVN3D) that are robust to partial occlusions. Implement grasp verification by checking tactile sensor readings after grasp execution\u2014if no contact is detected, retry with adjusted pose."]}),"\n",(0,i.jsx)(n.h3,{id:"challenge-2-slip-during-manipulation",children:"Challenge 2: Slip During Manipulation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Objects slip from the gripper during lifting or transport due to insufficient grip force, smooth surfaces, or unexpected disturbances."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Integrate tactile sensors (GelSight, force/torque sensors) to detect slip in real-time. When slip is detected, immediately increase grip force or adjust grasp pose. Implement closed-loop grip force control that maintains minimum force needed to prevent slip without crushing the object. Use compliant grippers (underactuated, soft robotics) that conform to object shape and distribute forces more evenly."]}),"\n",(0,i.jsx)(n.h3,{id:"challenge-3-grasping-novel-objects",children:"Challenge 3: Grasping Novel Objects"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Grasp planners trained on specific object datasets fail when encountering novel object geometries, materials, or weights not seen during training."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Use shape-based grasp planning that decomposes objects into primitive shapes (boxes, cylinders, spheres) and applies pre-computed grasps for each primitive. Train grasp networks on highly diverse synthetic datasets generated in simulation (domain randomization). Implement few-shot learning where the robot quickly adapts to new objects after a small number of grasp attempts, updating its model based on success/failure feedback."]}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Start with analytical methods before learning"}),": Implement analytical grasp planning (force closure, contact mechanics) first to understand the fundamentals. Learning-based methods can then augment or replace analytical components where they struggle."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use simulation for rapid iteration"}),": Develop and test grasping algorithms in physics simulators (Gazebo, Isaac Sim) where thousands of grasp attempts can be evaluated quickly without hardware wear. Transfer learned policies to real robots using sim-to-real techniques."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implement grasp recovery strategies"}),": Not all grasps succeed on the first attempt. Design recovery behaviors: re-grasp after failure, adjust approach angle, or switch to alternative grasp candidates. Robustness comes from graceful failure handling."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tune grip force carefully"}),": Too little force causes slips; too much crushes delicate objects. Use force feedback to modulate grip force dynamically based on object properties (estimated weight, compliance) and task requirements."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Collect diverse training data"}),": If using learning-based approaches, ensure training datasets include diverse objects, poses, lighting conditions, and backgrounds. Data augmentation and domain randomization improve generalization."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Key takeaways from this chapter:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robotic grasping requires integrating perception (object detection, pose estimation), planning (grasp synthesis, force closure analysis), sensing (tactile, force/torque), and control (manipulation primitives) into a cohesive system"}),"\n",(0,i.jsx)(n.li,{children:"Analytical grasp planning methods use contact mechanics, friction cone models, and force closure tests to generate provably stable grasps, while learning-based approaches discover effective strategies from data"}),"\n",(0,i.jsx)(n.li,{children:"Tactile and force/torque sensors provide critical feedback for slip detection, grip force control, and adaptive manipulation, enabling robust interaction with uncertain environments"}),"\n",(0,i.jsx)(n.li,{children:"Manipulation primitives like pick-and-place and in-hand manipulation decompose complex tasks into reusable building blocks that can be sequenced and parameterized for diverse applications"}),"\n",(0,i.jsx)(n.li,{children:"Modern Vision-Language-Action models enable natural language-commanded grasping and can generalize to novel objects and tasks through large-scale pre-training and fine-tuning"}),"\n",(0,i.jsx)(n.li,{children:"Successful grasping systems require careful integration of multiple components, simulation-based development, and robust error recovery strategies"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Official Documentation"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://moveit.picknik.ai/main/doc/examples/pick_place/pick_place_tutorial.html",children:"MoveIt 2 Grasping Pipeline"})," - End-to-end grasp planning with collision checking"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"http://gelsight.csail.mit.edu/",children:"GelSight Sensor Documentation"})," - Optical tactile sensing technology"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Research Papers"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Miller & Allen (2004), "GraspIt!: A Versatile Simulator for Robotic Grasping" - Foundational grasp simulation platform'}),"\n",(0,i.jsx)(n.li,{children:'Mahler et al. (2017), "Dex-Net 2.0: Deep Learning to Plan Robust Grasps" - CNN-based grasp quality prediction'}),"\n",(0,i.jsx)(n.li,{children:'Levine et al. (2018), "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning" - End-to-end learning from pixels'}),"\n",(0,i.jsx)(n.li,{children:'Brohan et al. (2023), "RT-2: Vision-Language-Action Models" - VLA for robotic manipulation'}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tutorials"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Advanced/Pick-and-Place.html",children:"ROS 2 Pick and Place Tutorial"})," - Implementing manipulation primitives"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://pybullet.org/wordpress/",children:"PyBullet Grasp Simulation"})," - Physics-based grasp testing"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Community Resources"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://discourse.ros.org/c/manipulation",children:"RoboGrasp Community"})," - Discussion forum for manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/atenpas/gpd",children:"Grasp Pose Detection Benchmark"})," - Standard datasets and evaluation"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Explain the difference between force closure and form closure for robotic grasps. Why is force closure more commonly used in practice?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A parallel-jaw gripper with coefficient of friction \u03bc = 0.5 grasps a box at two opposing faces. What is the minimum normal force required at each contact to support a 2 kg object against gravity? (Assume g = 9.81 m/s\xb2)"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Compare sampling-based grasp planning with learning-based approaches. In what scenarios would you prefer one over the other?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Describe how optical tactile sensors like GelSight can detect slip. What information from the sensor is used, and how is it processed?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Explain the pick-and-place primitive phases (approach, grasp, lift, place). Why is it important to include approach and lift phases rather than directly moving to the grasp and place poses?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A grasp quality metric returns \u03b5 = 0.05 for candidate grasp A and \u03b5 = 0.15 for candidate grasp B. Which grasp is more robust to external disturbances? If both grasps achieve force closure, why might you still prefer grasp B?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Describe how Vision-Language-Action (VLA) models enable natural language-commanded grasping. What are the key components of a VLA pipeline, and how do they work together?"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise Title"}),": Implementing a Grasp Planner for Cylinder Objects"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Objective"}),": Develop a grasp planning system that generates and evaluates parallel-jaw grasps for cylindrical objects using depth images."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prerequisites"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Completed previous chapters on perception, URDF modeling, and simulation"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble installed"}),"\n",(0,i.jsx)(n.li,{children:"Python 3.10+ with NumPy, SciPy, OpenCV"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Set up a simulated environment with Gazebo containing cylindrical objects (bottles, cans) and a parallel-jaw gripper robot."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Implement the ",(0,i.jsx)(n.code,{children:"GraspSampler"})," class from Section 1 to generate candidate grasps for cylinders. Test with various cylinder dimensions (radius 2-5cm, height 10-30cm)."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Implement force closure checking using the ",(0,i.jsx)(n.code,{children:"compute_grasp_matrix"})," and ",(0,i.jsx)(n.code,{children:"check_force_closure"})," functions from Section 2. Verify that top-down grasps on cylinders achieve force closure with \u03bc \u2265 0.3."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create a ROS 2 node that:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Subscribes to depth images from a simulated camera"}),"\n",(0,i.jsx)(n.li,{children:"Segments cylindrical objects using RANSAC cylinder fitting"}),"\n",(0,i.jsx)(n.li,{children:"Generates grasp candidates using your sampler"}),"\n",(0,i.jsx)(n.li,{children:"Evaluates candidates using force closure and quality metrics"}),"\n",(0,i.jsx)(n.li,{children:"Publishes the best grasp pose to a visualization topic"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Visualize the grasp candidates in RViz using markers. Color-code grasps by quality (green = high, yellow = medium, red = low)."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Test your planner by commanding the robot to execute the top-ranked grasp. Measure success rate over 20 trials with randomly placed cylinders."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Outcome"}),": Your grasp planner should consistently generate force-closure grasps for cylinders with 80%+ success rate in simulation. Top-ranked grasps should approach from the side (diametric grasp) rather than the top, as these typically have higher quality metrics."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Extension Challenges"})," (Optional):"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Extend the planner to handle boxes by implementing a box grasp sampler"}),"\n",(0,i.jsx)(n.li,{children:"Integrate tactile sensing simulation to detect slip during lifting"}),"\n",(0,i.jsx)(n.li,{children:"Implement grasp refinement using gradient-based optimization to improve initial samples"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Complete code available in"}),": ",(0,i.jsx)(n.code,{children:"/static/code/vla/chapter19/"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,i.jsx)(n.a,{href:"/docs/module-04-vla/week-11/ch17-humanoid-urdf",children:"Humanoid Robot Modeling"}),"\n",(0,i.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,i.jsx)(n.a,{href:"/docs/module-04-vla/week-13/ch21-whisper-voice",children:"Voice Interfaces with Whisper"}),"\n",(0,i.jsx)(n.strong,{children:"Module Overview"}),": ",(0,i.jsx)(n.a,{href:"/docs/module-04-vla/",children:"VLA Systems"})]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var r=s(6540);const i={},a=r.createContext(i);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);