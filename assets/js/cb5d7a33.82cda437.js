"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1781],{7512:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-03-isaac/week-09/ch14-perception","title":"Perception Pipelines","description":"Learning Objectives","source":"@site/docs/module-03-isaac/week-09/ch14-perception.md","sourceDirName":"module-03-isaac/week-09","slug":"/module-03-isaac/week-09/ch14-perception","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-09/ch14-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-isaac/week-09/ch14-perception.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{"id":"ch14-perception","title":"Perception Pipelines","sidebar_label":"Perception Pipelines","sidebar_position":15},"sidebar":"textbookSidebar","previous":{"title":"Visual SLAM and Navigation","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-09/ch13-vslam-nav2"},"next":{"title":"Reinforcement Learning...","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-10/ch15-rl-sim2real"}}');var s=i(4848),o=i(8453);const r={id:"ch14-perception",title:"Perception Pipelines",sidebar_label:"Perception Pipelines",sidebar_position:15},a="Perception Pipelines",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Object Detection Pipeline",id:"object-detection-pipeline",level:2},{value:"YOLOv8 and RT-DETR",id:"yolov8-and-rt-detr",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Class Filtering for Robotics",id:"class-filtering-for-robotics",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Segformer and DeepLabV3",id:"segformer-and-deeplabv3",level:3},{value:"Free Space Detection for Navigation",id:"free-space-detection-for-navigation",level:3},{value:"3D Perception from Depth",id:"3d-perception-from-depth",level:2},{value:"Point Cloud Generation",id:"point-cloud-generation",level:3},{value:"3D Bounding Box Estimation",id:"3d-bounding-box-estimation",level:3},{value:"6D Pose Estimation",id:"6d-pose-estimation",level:3},{value:"Perception for Navigation",id:"perception-for-navigation",level:2},{value:"Dynamic Obstacle Detection",id:"dynamic-obstacle-detection",level:3},{value:"Person Tracking and Following",id:"person-tracking-and-following",level:3},{value:"Real-Time Performance Optimization",id:"real-time-performance-optimization",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: YOLOv8 Object Detection",id:"exercise-1-yolov8-object-detection",level:3},{value:"Exercise 2: Free Space Segmentation",id:"exercise-2-free-space-segmentation",level:3},{value:"Exercise 3: 3D Object Localization",id:"exercise-3-3d-object-localization",level:3},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"perception-pipelines",children:"Perception Pipelines"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Build end-to-end perception pipelines for object detection and segmentation"}),"\n",(0,s.jsx)(n.li,{children:"Use state-of-the-art models (YOLOv8, RT-DETR, Segformer) in ROS 2"}),"\n",(0,s.jsx)(n.li,{children:"Process RGB-D data for 3D object localization"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception with Nav2 for dynamic obstacle avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Optimize pipelines for real-time performance (<100ms latency)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),' is the ability of a robot to interpret sensor data and build a semantic understanding of its environment. While sensors like cameras and LiDAR provide raw data (images, point clouds), perception algorithms transform this data into actionable information: "There is a person 2 meters ahead," "The table is 0.8 meters high," or "The doorway is clear for navigation."']}),"\n",(0,s.jsxs)(n.p,{children:["For Physical AI systems, robust perception is critical for safe and effective operation. A warehouse robot must detect pallets, forklifts, and people in real-time; a domestic assistant robot must recognize objects for manipulation; an outdoor autonomous vehicle must segment drivable surfaces from obstacles. These tasks require ",(0,s.jsx)(n.strong,{children:"sensor fusion"})," (combining camera, depth, and LiDAR data), ",(0,s.jsx)(n.strong,{children:"real-time processing"})," (latency <100ms for 10 Hz control loops), and ",(0,s.jsx)(n.strong,{children:"reliability"})," (high precision and recall to avoid false positives and missed detections)."]}),"\n",(0,s.jsx)(n.p,{children:"Common perception tasks in robotics include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Detection"}),': Identifying and localizing objects with 2D bounding boxes (e.g., "person at (320, 240) with 80x120 box")']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic Segmentation"}),": Classifying every pixel in an image (e.g., road, sidewalk, obstacle)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"3D Object Localization"}),": Estimating 3D position and orientation from RGB-D or stereo cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scene Understanding"}),': Recognizing higher-level concepts like "doorway," "staircase," or "cluttered workspace"']}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, we'll build production-ready perception pipelines using modern deep learning models, integrate them with ROS 2, and deploy them for real-time navigation and manipulation tasks."}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-pipeline",children:"Object Detection Pipeline"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Object detection"})," identifies objects of interest in images and returns their 2D bounding boxes and class labels. For robotics, we prioritize ",(0,s.jsx)(n.strong,{children:"real-time models"})," that run at 30+ FPS on edge devices (NVIDIA Jetson) while maintaining high accuracy."]}),"\n",(0,s.jsx)(n.h3,{id:"yolov8-and-rt-detr",children:"YOLOv8 and RT-DETR"}),"\n",(0,s.jsx)(n.p,{children:"Two state-of-the-art architectures dominate real-time object detection:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"YOLOv8"})," (You Only Look Once v8): Single-stage detector with ~20-80 FPS on Jetson Orin, mAP 50-53% on COCO. Best for general-purpose detection."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-DETR"})," (Real-Time Detection Transformer): Transformer-based detector with dynamic anchor generation, ~30-60 FPS, mAP 53-56%. Better accuracy but slightly slower."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Both models support:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pre-trained weights"})," on COCO (80 classes: person, car, chair, etc.)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom training"})," via transfer learning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TensorRT optimization"})," for 2-3x speedup on NVIDIA GPUs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Export to ONNX"})," for cross-platform deployment"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,s.jsx)(n.p,{children:"Here's a complete YOLOv8 detection node for ROS 2:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nfrom ultralytics import YOLO\nimport numpy as np\n\nclass YOLOv8DetectorNode(Node):\n    \"\"\"Real-time object detection using YOLOv8\"\"\"\n\n    def __init__(self):\n        super().__init__('yolov8_detector')\n\n        # Parameters\n        self.declare_parameter('model_path', 'yolov8n.pt')  # nano model\n        self.declare_parameter('confidence_threshold', 0.5)\n        self.declare_parameter('classes_of_interest', [0])  # 0 = person\n\n        model_path = self.get_parameter('model_path').value\n        self.conf_threshold = self.get_parameter('confidence_threshold').value\n        self.classes = self.get_parameter('classes_of_interest').value\n\n        # Load YOLO model\n        self.model = YOLO(model_path)\n        self.get_logger().info(f'Loaded YOLOv8 model: {model_path}')\n\n        # ROS interfaces\n        self.bridge = CvBridge()\n        self.image_sub = self.create_subscription(\n            Image, '/camera/color/image_raw', self.image_callback, 10\n        )\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/detections', 10\n        )\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image and publish detections\"\"\"\n        # Convert ROS Image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n\n        # Run YOLOv8 inference\n        results = self.model(\n            cv_image,\n            conf=self.conf_threshold,\n            classes=self.classes,  # Filter to classes of interest\n            verbose=False\n        )[0]\n\n        # Convert to Detection2DArray\n        detection_array = Detection2DArray()\n        detection_array.header = msg.header\n\n        for box in results.boxes:\n            detection = Detection2D()\n\n            # Bounding box (center x, y, width, height)\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            detection.bbox.center.position.x = float((x1 + x2) / 2)\n            detection.bbox.center.position.y = float((y1 + y2) / 2)\n            detection.bbox.size_x = float(x2 - x1)\n            detection.bbox.size_y = float(y2 - y1)\n\n            # Class and confidence\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = str(int(box.cls[0]))\n            hypothesis.hypothesis.score = float(box.conf[0])\n            detection.results.append(hypothesis)\n\n            detection_array.detections.append(detection)\n\n        self.detection_pub.publish(detection_array)\n        self.get_logger().info(\n            f'Published {len(detection_array.detections)} detections'\n        )\n\ndef main():\n    rclpy.init()\n    node = YOLOv8DetectorNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"class-filtering-for-robotics",children:"Class Filtering for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"For robotics applications, we typically filter detections to relevant classes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation"}),": person, chair, table, door"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Warehouse"}),": pallet, forklift, box, person"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Domestic"}),": bottle, cup, book, remote"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This reduces false positives and improves performance by skipping irrelevant classes during inference."}),"\n",(0,s.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Semantic segmentation"})," assigns a class label to every pixel in an image, providing dense scene understanding. For mobile robots, segmentation is used to:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Identify ",(0,s.jsx)(n.strong,{children:"free space"})," (drivable/walkable regions)"]}),"\n",(0,s.jsxs)(n.li,{children:["Detect ",(0,s.jsx)(n.strong,{children:"obstacles"})," (walls, furniture, people)"]}),"\n",(0,s.jsxs)(n.li,{children:["Recognize ",(0,s.jsx)(n.strong,{children:"terrain types"})," (grass, pavement, gravel)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"segformer-and-deeplabv3",children:"Segformer and DeepLabV3"}),"\n",(0,s.jsx)(n.p,{children:"Two popular segmentation architectures:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Segformer"}),": Transformer-based, ~40-60 FPS on Jetson Orin, mIoU 50-55% on ADE20K. Excellent for outdoor navigation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DeepLabV3+"}),": CNN-based with atrous convolution, ~30-50 FPS, mIoU 45-50%. Better for indoor scenes."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"free-space-detection-for-navigation",children:"Free Space Detection for Navigation"}),"\n",(0,s.jsx)(n.p,{children:"Here's a segmentation node that publishes a binary occupancy grid for free space:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom nav_msgs.msg import OccupancyGrid\nfrom cv_bridge import CvBridge\nimport torch\nfrom transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\nimport numpy as np\n\nclass FreeSpaceDetector(Node):\n    \"\"\"Semantic segmentation for free space detection\"\"\"\n\n    def __init__(self):\n        super().__init__('free_space_detector')\n\n        # Load Segformer model\n        self.processor = SegformerImageProcessor.from_pretrained(\n            \"nvidia/segformer-b0-finetuned-ade-512-512\"\n        )\n        self.model = SegformerForSemanticSegmentation.from_pretrained(\n            \"nvidia/segformer-b0-finetuned-ade-512-512\"\n        )\n        self.model.eval()\n\n        # Free space class IDs (road=6, sidewalk=11, floor=3)\n        self.free_space_classes = [3, 6, 11]\n\n        self.bridge = CvBridge()\n        self.image_sub = self.create_subscription(\n            Image, '/camera/color/image_raw', self.image_callback, 10\n        )\n        self.grid_pub = self.create_publisher(\n            OccupancyGrid, '/free_space_grid', 10\n        )\n\n    def image_callback(self, msg):\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n\n        # Preprocess and run inference\n        inputs = self.processor(images=cv_image, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits  # Shape: (1, num_classes, H, W)\n\n        # Upsample to original size\n        seg_map = torch.nn.functional.interpolate(\n            logits,\n            size=cv_image.shape[:2],\n            mode='bilinear',\n            align_corners=False\n        ).argmax(dim=1)[0].cpu().numpy()\n\n        # Create binary free space mask\n        free_space_mask = np.isin(seg_map, self.free_space_classes).astype(np.uint8)\n\n        # Convert to OccupancyGrid (0=free, 100=occupied, -1=unknown)\n        grid = OccupancyGrid()\n        grid.header = msg.header\n        grid.info.resolution = 0.05  # 5cm per pixel\n        grid.info.width = free_space_mask.shape[1]\n        grid.info.height = free_space_mask.shape[0]\n        grid.data = ((1 - free_space_mask) * 100).flatten().tolist()\n\n        self.grid_pub.publish(grid)\n\ndef main():\n    rclpy.init()\n    node = FreeSpaceDetector()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"3d-perception-from-depth",children:"3D Perception from Depth"}),"\n",(0,s.jsxs)(n.p,{children:["RGB-D cameras (e.g., Intel RealSense, Azure Kinect) provide both color images and depth maps, enabling ",(0,s.jsx)(n.strong,{children:"3D object localization"}),'. This is critical for manipulation tasks where the robot needs to know not just "where is the object in the image" but "where is the object in 3D space relative to the robot."']}),"\n",(0,s.jsx)(n.h3,{id:"point-cloud-generation",children:"Point Cloud Generation"}),"\n",(0,s.jsx)(n.p,{children:"Given an RGB-D image, we can generate a 3D point cloud:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom sensor_msgs.msg import PointCloud2, PointField\nimport struct\n\ndef rgbd_to_pointcloud(rgb_image, depth_image, camera_intrinsics):\n    \"\"\"\n    Convert RGB-D image to point cloud\n\n    Args:\n        rgb_image: (H, W, 3) numpy array\n        depth_image: (H, W) numpy array in millimeters\n        camera_intrinsics: dict with fx, fy, cx, cy\n    \"\"\"\n    fx, fy, cx, cy = camera_intrinsics['fx'], camera_intrinsics['fy'], \\\n                     camera_intrinsics['cx'], camera_intrinsics['cy']\n\n    H, W = depth_image.shape\n    points = []\n\n    for v in range(H):\n        for u in range(W):\n            z = depth_image[v, u] / 1000.0  # Convert mm to meters\n\n            if z == 0:  # Invalid depth\n                continue\n\n            # Backproject to 3D\n            x = (u - cx) * z / fx\n            y = (v - cy) * z / fy\n\n            r, g, b = rgb_image[v, u]\n            rgb_packed = struct.unpack('f', struct.pack('I', (r << 16 | g << 8 | b)))[0]\n\n            points.append([x, y, z, rgb_packed])\n\n    return np.array(points)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3d-bounding-box-estimation",children:"3D Bounding Box Estimation"}),"\n",(0,s.jsx)(n.p,{children:"Combining 2D detection with depth, we can estimate 3D bounding boxes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def estimate_3d_bbox(detection_2d, depth_image, camera_intrinsics):\n    \"\"\"\n    Estimate 3D bounding box from 2D detection and depth\n\n    Returns:\n        center_3d: (x, y, z) in meters\n        size_3d: (width, height, depth) in meters\n    \"\"\"\n    # Extract 2D bbox\n    x_center = int(detection_2d.bbox.center.position.x)\n    y_center = int(detection_2d.bbox.center.position.y)\n    width = int(detection_2d.bbox.size_x)\n    height = int(detection_2d.bbox.size_y)\n\n    # Get depth at bbox center (median for robustness)\n    x1 = max(0, x_center - width // 2)\n    x2 = min(depth_image.shape[1], x_center + width // 2)\n    y1 = max(0, y_center - height // 2)\n    y2 = min(depth_image.shape[0], y_center + height // 2)\n\n    depth_patch = depth_image[y1:y2, x1:x2]\n    depth_patch = depth_patch[depth_patch > 0]  # Remove invalid\n\n    if len(depth_patch) == 0:\n        return None\n\n    z = np.median(depth_patch) / 1000.0  # mm to meters\n\n    # Backproject center to 3D\n    fx, fy, cx, cy = camera_intrinsics['fx'], camera_intrinsics['fy'], \\\n                     camera_intrinsics['cx'], camera_intrinsics['cy']\n    x = (x_center - cx) * z / fx\n    y = (y_center - cy) * z / fy\n\n    # Estimate 3D size (rough approximation)\n    width_3d = (width / fx) * z\n    height_3d = (height / fy) * z\n    depth_3d = 0.3  # Assume 30cm depth for person/object\n\n    return {\n        'center': (x, y, z),\n        'size': (width_3d, height_3d, depth_3d)\n    }\n"})}),"\n",(0,s.jsx)(n.h3,{id:"6d-pose-estimation",children:"6D Pose Estimation"}),"\n",(0,s.jsxs)(n.p,{children:["For manipulation, we often need the full ",(0,s.jsx)(n.strong,{children:"6D pose"})," (position + orientation) of objects. Methods include:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOPE"})," (Deep Object Pose Estimation): CNN-based, works on RGB images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PVNet"}),": Keypoint-based 6D pose estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FoundationPose"}),": Foundation model for 6D pose (novel objects)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["These are integrated in Isaac ROS (",(0,s.jsx)(n.code,{children:"isaac_ros_dope"}),") for GPU acceleration."]}),"\n",(0,s.jsx)(n.h2,{id:"perception-for-navigation",children:"Perception for Navigation"}),"\n",(0,s.jsx)(n.p,{children:"Perception pipelines must integrate with the Nav2 navigation stack to enable dynamic obstacle avoidance and environment-aware planning."}),"\n",(0,s.jsx)(n.h3,{id:"dynamic-obstacle-detection",children:"Dynamic Obstacle Detection"}),"\n",(0,s.jsx)(n.p,{children:"Use object detection to identify moving obstacles (people, vehicles) and publish them to Nav2's costmap:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from nav_msgs.msg import OccupancyGrid\nfrom geometry_msgs.msg import PointStamped\n\nclass DynamicObstaclePublisher(Node):\n    \"\"\"Publish detected people as dynamic obstacles\"\"\"\n\n    def __init__(self):\n        super().__init__('dynamic_obstacle_publisher')\n\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.detection_callback, 10\n        )\n        self.costmap_pub = self.create_publisher(\n            OccupancyGrid, '/dynamic_obstacles_costmap', 10\n        )\n\n    def detection_callback(self, msg):\n        # Create costmap with detected obstacles\n        costmap = OccupancyGrid()\n        costmap.header = msg.header\n        costmap.info.resolution = 0.05  # 5cm resolution\n        costmap.info.width = 200  # 10m x 10m map\n        costmap.info.height = 200\n        costmap.data = [0] * (200 * 200)  # Initialize as free\n\n        for detection in msg.detections:\n            # Mark detected person as occupied in costmap\n            # (This is simplified; in practice, use 3D position)\n            x_pixel = int(detection.bbox.center.position.x / costmap.info.resolution)\n            y_pixel = int(detection.bbox.center.position.y / costmap.info.resolution)\n\n            # Inflate obstacle (5x5 grid)\n            for dx in range(-2, 3):\n                for dy in range(-2, 3):\n                    idx = (y_pixel + dy) * 200 + (x_pixel + dx)\n                    if 0 <= idx < len(costmap.data):\n                        costmap.data[idx] = 100  # Occupied\n\n        self.costmap_pub.publish(costmap)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"person-tracking-and-following",children:"Person Tracking and Following"}),"\n",(0,s.jsx)(n.p,{children:"For assistive robots, tracking a designated person is a common task:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class PersonTracker(Node):\n    \"\"\"Track and follow a designated person\"\"\"\n\n    def __init__(self):\n        super().__init__('person_tracker')\n        self.target_person_id = None\n        self.last_detection_time = None\n\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.track_callback, 10\n        )\n        self.cmd_vel_pub = self.create_publisher(\n            Twist, '/cmd_vel', 10\n        )\n\n    def track_callback(self, msg):\n        if not msg.detections:\n            return\n\n        # Find closest person (simple heuristic)\n        closest_detection = min(\n            msg.detections,\n            key=lambda d: d.bbox.size_x * d.bbox.size_y  # Largest = closest\n        )\n\n        # Compute tracking command (proportional control)\n        image_center = 320  # Assuming 640x480 camera\n        x_center = closest_detection.bbox.center.position.x\n        error = x_center - image_center\n\n        cmd = Twist()\n        cmd.linear.x = 0.3  # Move forward at 0.3 m/s\n        cmd.angular.z = -0.002 * error  # Turn to center person\n\n        self.cmd_vel_pub.publish(cmd)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-performance-optimization",children:"Real-Time Performance Optimization"}),"\n",(0,s.jsx)(n.p,{children:"To achieve <100ms latency for real-time control:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use GPU acceleration"}),": Run models on NVIDIA Jetson or GPUs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model optimization"}),": Convert to TensorRT (2-3x speedup)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resolution trade-offs"}),": Use 640x480 instead of 1920x1080 (4x faster)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch processing"}),": If latency allows, batch multiple frames"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Asynchronous inference"}),": Run perception in separate thread from control"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validate models on real data"}),": Test on robot-collected images, not just benchmarks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor inference time"}),": Log latency for each pipeline stage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use confidence thresholds"}),": Filter low-confidence detections to reduce false positives"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fuse multiple sensors"}),": Combine camera + LiDAR for robust obstacle detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement fallback behaviors"}),": If perception fails, stop safely"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Key takeaways from this chapter:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object detection"})," with YOLOv8/RT-DETR enables real-time identification of objects at 30-80 FPS"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic segmentation"})," provides dense scene understanding for free space detection and terrain classification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"3D perception"})," from RGB-D cameras enables accurate object localization for manipulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration with Nav2"})," allows perception-based dynamic obstacle avoidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time optimization"})," through GPU acceleration and TensorRT is essential for high-frequency control loops"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These perception pipelines form the foundation for intelligent Physical AI systems that can understand and interact with complex, dynamic environments."}),"\n",(0,s.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the key differences between YOLOv8 and RT-DETR, and when would you choose one over the other?"}),"\n",(0,s.jsx)(n.li,{children:"How does semantic segmentation differ from object detection, and what are the use cases for each in robotics?"}),"\n",(0,s.jsx)(n.li,{children:"Explain the process of generating a 3D point cloud from an RGB-D image using camera intrinsics."}),"\n",(0,s.jsx)(n.li,{children:"Why is 6D pose estimation more challenging than 3D bounding box estimation?"}),"\n",(0,s.jsx)(n.li,{children:"Describe how you would integrate a person detection pipeline with Nav2 to enable dynamic obstacle avoidance."}),"\n",(0,s.jsx)(n.li,{children:"What are the trade-offs between using higher resolution images vs. faster inference in real-time perception?"}),"\n",(0,s.jsx)(n.li,{children:"How would you debug a perception pipeline that has high latency (>200ms)?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-yolov8-object-detection",children:"Exercise 1: YOLOv8 Object Detection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Deploy YOLOv8 on a ROS 2 robot and visualize detections in RViz."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Install Ultralytics YOLOv8: ",(0,s.jsx)(n.code,{children:"pip install ultralytics"})]}),"\n",(0,s.jsx)(n.li,{children:"Implement the YOLOv8DetectorNode from this chapter"}),"\n",(0,s.jsxs)(n.li,{children:["Run the node with a camera feed: ",(0,s.jsx)(n.code,{children:"ros2 run perception yolov8_detector"})]}),"\n",(0,s.jsxs)(n.li,{children:["Visualize detections in RViz using ",(0,s.jsx)(n.code,{children:"vision_msgs/Detection2DArray"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Outcome"}),": Real-time object detection at 30+ FPS with bounding boxes displayed."]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-free-space-segmentation",children:"Exercise 2: Free Space Segmentation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Build a semantic segmentation pipeline to detect drivable surfaces."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Fine-tune Segformer on custom indoor/outdoor dataset"}),"\n",(0,s.jsx)(n.li,{children:"Implement the FreeSpaceDetector node"}),"\n",(0,s.jsx)(n.li,{children:"Integrate with Nav2's costmap_2d for free space updates"}),"\n",(0,s.jsx)(n.li,{children:"Test navigation in cluttered environment"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Outcome"}),": Robot successfully navigates using segmentation-based free space detection."]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-3d-object-localization",children:"Exercise 3: 3D Object Localization"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Use RGB-D camera to estimate 3D positions of objects."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Intel RealSense D435 camera in ROS 2"}),"\n",(0,s.jsx)(n.li,{children:"Combine YOLOv8 detections with depth images"}),"\n",(0,s.jsxs)(n.li,{children:["Implement ",(0,s.jsx)(n.code,{children:"estimate_3d_bbox"})," function from this chapter"]}),"\n",(0,s.jsxs)(n.li,{children:["Publish 3D bounding boxes as ",(0,s.jsx)(n.code,{children:"visualization_msgs/MarkerArray"})," in RViz"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Outcome"}),": Accurate 3D localization of objects within 5cm error."]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"YOLOv8 Documentation"}),": ",(0,s.jsx)(n.a,{href:"https://docs.ultralytics.com/",children:"https://docs.ultralytics.com/"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-DETR Paper"}),': "DETRs Beat YOLOs on Real-time Object Detection" (2023)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Segformer Paper"}),': "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers" (2021)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOPE (6D Pose)"}),": ",(0,s.jsx)(n.a,{href:"https://github.com/NVlabs/Deep_Object_Pose",children:"https://github.com/NVlabs/Deep_Object_Pose"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Perception Tutorials"}),": ",(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Advanced/Computer-Vision.html",children:"https://docs.ros.org/en/humble/Tutorials/Advanced/Computer-Vision.html"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous"}),": ",(0,s.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-09/ch13-vslam-nav2",children:"Chapter 13 - Visual SLAM and Nav2"}),"\n",(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-10/ch15-rl-sim2real",children:"Chapter 15 - Reinforcement Learning and Sim-to-Real"})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);