"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4299],{1453:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"foundations/week-02/ch02-sensors","title":"Sensors in Physical AI Systems","description":"Learning Objectives","source":"@site/docs/foundations/week-02/ch02-sensors.md","sourceDirName":"foundations/week-02","slug":"/foundations/week-02/ch02-sensors","permalink":"/AI_COURSE_book/docs/foundations/week-02/ch02-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/foundations/week-02/ch02-sensors.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"ch02-sensors","title":"Sensors in Physical AI Systems","sidebar_label":"Sensors in Physical AI","sidebar_position":1},"sidebar":"textbookSidebar","previous":{"title":"Embodied Intelligence","permalink":"/AI_COURSE_book/docs/foundations/week-01/ch01-embodied-intelligence"},"next":{"title":"Actuators and Architectures","permalink":"/AI_COURSE_book/docs/foundations/week-02/ch03-actuators-robotics-arch"}}');var r=i(4848),l=i(8453);const t={id:"ch02-sensors",title:"Sensors in Physical AI Systems",sidebar_label:"Sensors in Physical AI",sidebar_position:1},o="Sensors in Physical AI Systems",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Vision Sensors (RGB and Depth)",id:"1-vision-sensors-rgb-and-depth",level:2},{value:"RGB Cameras",id:"rgb-cameras",level:3},{value:"Depth Cameras",id:"depth-cameras",level:3},{value:"Choosing Between RGB and Depth",id:"choosing-between-rgb-and-depth",level:3},{value:"2. LiDAR and Range Sensing",id:"2-lidar-and-range-sensing",level:2},{value:"What is LiDAR?",id:"what-is-lidar",level:3},{value:"Types of LiDAR",id:"types-of-lidar",level:3},{value:"LiDAR vs Other Sensors",id:"lidar-vs-other-sensors",level:3},{value:"Ultrasonic and IR Range Sensors",id:"ultrasonic-and-ir-range-sensors",level:3},{value:"3. Inertial Measurement Units (IMUs)",id:"3-inertial-measurement-units-imus",level:2},{value:"What is an IMU?",id:"what-is-an-imu",level:3},{value:"How IMUs Work",id:"how-imus-work",level:3},{value:"IMU Applications in Robotics",id:"imu-applications-in-robotics",level:3},{value:"4. Proprioceptive Sensors",id:"4-proprioceptive-sensors",level:2},{value:"Joint Encoders",id:"joint-encoders",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"Current/Torque Sensing",id:"currenttorque-sensing",level:3},{value:"5. Sensor Fusion Strategies",id:"5-sensor-fusion-strategies",level:2},{value:"Why Fuse Sensors?",id:"why-fuse-sensors",level:3},{value:"Common Fusion Approaches",id:"common-fusion-approaches",level:3},{value:"Example: Robot Localization Fusion",id:"example-robot-localization-fusion",level:3},{value:"Best Practices",id:"best-practices",level:3},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Camera vs Depth Camera",id:"exercise-1-camera-vs-depth-camera",level:3},{value:"Exercise 2: IMU Orientation Tracking",id:"exercise-2-imu-orientation-tracking",level:3},{value:"Exercise 3: Simple Sensor Fusion",id:"exercise-3-simple-sensor-fusion",level:3},{value:"Further Reading",id:"further-reading",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"sensors-in-physical-ai-systems",children:"Sensors in Physical AI Systems"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand different types of sensors used in robotics (vision, LiDAR, IMU, proprioceptive)"}),"\n",(0,r.jsx)(n.li,{children:"Compare RGB cameras vs depth cameras and their use cases"}),"\n",(0,r.jsx)(n.li,{children:"Explain how LiDAR and range sensors work"}),"\n",(0,r.jsx)(n.li,{children:"Understand inertial measurement units (IMUs) for orientation tracking"}),"\n",(0,r.jsx)(n.li,{children:"Describe proprioceptive sensors for internal state monitoring"}),"\n",(0,r.jsx)(n.li,{children:"Apply basic sensor fusion strategies"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Physical AI systems depend entirely on sensors to perceive and interact with the world. Unlike pure software AI that operates on digital data, embodied AI must bridge the physical-digital divide through sensors that convert real-world phenomena (light, distance, acceleration, force) into digital signals."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"The sensor challenge"}),": Robots must answer fundamental questions:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Where am I?"})," (localization) - Requires odometry, IMU, GPS, or visual landmarks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"What's around me?"})," (perception) - Requires cameras, LiDAR, or range sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"How am I moving?"})," (proprioception) - Requires joint encoders, force sensors, IMUs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"What am I touching?"})," (tactile sensing) - Requires force/torque sensors, tactile arrays"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The quality and diversity of sensors directly determine a robot's capabilities. A mobile robot with only forward-facing sensors cannot detect obstacles behind it. A manipulator without force sensing cannot grasp fragile objects safely."}),"\n",(0,r.jsx)(n.p,{children:"This chapter explores the main sensor categories used in Physical AI systems and how they enable intelligent embodied behavior."}),"\n",(0,r.jsx)(n.h2,{id:"1-vision-sensors-rgb-and-depth",children:"1. Vision Sensors (RGB and Depth)"}),"\n",(0,r.jsx)(n.h3,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,r.jsx)(n.p,{children:"RGB (Red-Green-Blue) cameras are the most common vision sensors, capturing 2D color images similar to smartphone cameras."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"How they work:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Light passes through a lens onto a sensor (CCD or CMOS)"}),"\n",(0,r.jsx)(n.li,{children:"Sensor array converts photons to electrical signals"}),"\n",(0,r.jsx)(n.li,{children:"Image processor creates digital image (e.g., 1920\xd71080 pixels, 3 channels)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Rich visual information (color, texture, patterns)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 High resolution (1080p, 4K, 8K)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Inexpensive and widely available"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Excellent for object recognition and scene understanding"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c No depth information (2D projection of 3D world)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Sensitive to lighting conditions"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Requires complex algorithms to extract 3D information"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Struggles in low light or high contrast"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common uses:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object detection and classification"}),"\n",(0,r.jsx)(n.li,{children:"Lane detection for autonomous vehicles"}),"\n",(0,r.jsx)(n.li,{children:"Visual SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,r.jsx)(n.li,{children:"QR code/AprilTag detection"}),"\n",(0,r.jsx)(n.li,{children:"Human pose estimation"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,r.jsx)(n.p,{children:"Depth cameras add a third dimension by measuring distance to each pixel."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Technologies:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Stereo Vision"})," (passive)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Uses two cameras (like human eyes)"}),"\n",(0,r.jsx)(n.li,{children:"Computes depth from disparity between images"}),"\n",(0,r.jsx)(n.li,{children:"Examples: ZED camera, OAK-D"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Structured Light"})," (active)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Projects known pattern onto scene"}),"\n",(0,r.jsx)(n.li,{children:"Measures pattern deformation to compute depth"}),"\n",(0,r.jsx)(n.li,{children:"Examples: Intel RealSense SR300 (discontinued)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time-of-Flight (ToF)"})," (active)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measures time for light pulse to return"}),"\n",(0,r.jsx)(n.li,{children:"Computes distance from speed of light"}),"\n",(0,r.jsx)(n.li,{children:"Examples: Intel RealSense D435, Azure Kinect"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Depth camera output:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB image"}),": Standard color image (H \xd7 W \xd7 3)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth map"}),": Distance values for each pixel (H \xd7 W \xd7 1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Point cloud"}),": 3D points in space (N \xd7 3), where N = H \xd7 W"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Direct 3D information"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Enables accurate distance measurement"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Useful for obstacle avoidance and manipulation"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Works in textureless environments"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Limited range (typically 0.3-10 meters)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Struggles with transparent/reflective surfaces"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Active sensors affected by sunlight"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Higher cost than RGB cameras"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common uses:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"3D object detection and pose estimation"}),"\n",(0,r.jsx)(n.li,{children:"Obstacle avoidance for mobile robots"}),"\n",(0,r.jsx)(n.li,{children:"Grasp point detection for manipulation"}),"\n",(0,r.jsx)(n.li,{children:"3D scene reconstruction"}),"\n",(0,r.jsx)(n.li,{children:"Person tracking and gesture recognition"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"choosing-between-rgb-and-depth",children:"Choosing Between RGB and Depth"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Task"}),(0,r.jsx)(n.th,{children:"RGB Camera"}),(0,r.jsx)(n.th,{children:"Depth Camera"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Object classification"}),(0,r.jsx)(n.td,{children:"\u2705 Excellent"}),(0,r.jsx)(n.td,{children:"\u26a0\ufe0f Can help"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Distance measurement"}),(0,r.jsx)(n.td,{children:"\u274c Difficult"}),(0,r.jsx)(n.td,{children:"\u2705 Direct"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Outdoor use"}),(0,r.jsx)(n.td,{children:"\u2705 Good"}),(0,r.jsx)(n.td,{children:"\u26a0\ufe0f Limited (sunlight)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Grasp planning"}),(0,r.jsx)(n.td,{children:"\u26a0\ufe0f Challenging"}),(0,r.jsx)(n.td,{children:"\u2705 Essential"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Visual SLAM"}),(0,r.jsx)(n.td,{children:"\u2705 Mature"}),(0,r.jsx)(n.td,{children:"\u2705 Emerging"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Low light"}),(0,r.jsx)(n.td,{children:"\u26a0\ufe0f Poor"}),(0,r.jsx)(n.td,{children:"\u2705 Better (active)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Best practice"}),": Use both! RGB for semantic understanding, depth for geometric reasoning."]}),"\n",(0,r.jsx)(n.h2,{id:"2-lidar-and-range-sensing",children:"2. LiDAR and Range Sensing"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-lidar",children:"What is LiDAR?"}),"\n",(0,r.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) measures distance by timing laser pulses."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Principle:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Emit laser pulse in a direction"}),"\n",(0,r.jsx)(n.li,{children:"Measure time until reflection returns"}),"\n",(0,r.jsxs)(n.li,{children:["Calculate distance: ",(0,r.jsx)(n.code,{children:"d = (c \xd7 t) / 2"})," where c = speed of light"]}),"\n",(0,r.jsx)(n.li,{children:"Rotate to scan 360\xb0 (2D) or entire environment (3D)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Point cloud of (x, y, z) coordinates showing obstacle locations"]}),"\n",(0,r.jsx)(n.h3,{id:"types-of-lidar",children:"Types of LiDAR"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. 2D LiDAR (Planar Scanning)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Scans in a single plane (e.g., horizontal)"}),"\n",(0,r.jsx)(n.li,{children:"Common in mobile robots for navigation"}),"\n",(0,r.jsx)(n.li,{children:"Examples: SICK TIM, Hokuyo URG, RPLiDAR A1"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Characteristics:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Range: 0.1-30 meters (varies by model)"}),"\n",(0,r.jsx)(n.li,{children:"Angular resolution: 0.25-1 degree"}),"\n",(0,r.jsx)(n.li,{children:"Scan rate: 5-15 Hz"}),"\n",(0,r.jsx)(n.li,{children:"Cost: $100-$5,000"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. 3D LiDAR (Volumetric Scanning)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Multiple scanning planes or rotating head"}),"\n",(0,r.jsx)(n.li,{children:"Creates 3D point cloud of environment"}),"\n",(0,r.jsx)(n.li,{children:"Examples: Velodyne VLP-16, Ouster OS1, Livox Mid-360"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Characteristics:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Range: 10-200 meters"}),"\n",(0,r.jsx)(n.li,{children:"Millions of points per second"}),"\n",(0,r.jsx)(n.li,{children:"Scan rate: 5-20 Hz"}),"\n",(0,r.jsx)(n.li,{children:"Cost: $1,000-$75,000+"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Solid-State LiDAR"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No moving parts (flash LiDAR or MEMS mirrors)"}),"\n",(0,r.jsx)(n.li,{children:"More durable, compact, cheaper"}),"\n",(0,r.jsx)(n.li,{children:"Emerging technology for autonomous vehicles"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lidar-vs-other-sensors",children:"LiDAR vs Other Sensors"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"LiDAR"}),(0,r.jsx)(n.th,{children:"Camera"}),(0,r.jsx)(n.th,{children:"Ultrasonic"}),(0,r.jsx)(n.th,{children:"Radar"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Range"}),(0,r.jsx)(n.td,{children:"0.1-200m"}),(0,r.jsx)(n.td,{children:"0-\u221e"}),(0,r.jsx)(n.td,{children:"0.02-5m"}),(0,r.jsx)(n.td,{children:"1-300m"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Resolution"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Very High"}),(0,r.jsx)(n.td,{children:"Low"}),(0,r.jsx)(n.td,{children:"Medium"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Weather"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"Poor"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cost"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Low"}),(0,r.jsx)(n.td,{children:"Very Low"}),(0,r.jsx)(n.td,{children:"Medium"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Sunlight"}),(0,r.jsx)(n.td,{children:"Immune"}),(0,r.jsx)(n.td,{children:"Sensitive"}),(0,r.jsx)(n.td,{children:"Immune"}),(0,r.jsx)(n.td,{children:"Immune"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Texture"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"No"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Accurate distance measurement (\xb12cm)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Works in any lighting condition"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Long range"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Direct 3D geometry"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c No color/texture information"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Expensive (especially 3D)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Struggles with transparent/absorptive surfaces"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Data processing overhead"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common uses:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,r.jsx)(n.li,{children:"Obstacle detection and avoidance"}),"\n",(0,r.jsx)(n.li,{children:"3D mapping and surveying"}),"\n",(0,r.jsx)(n.li,{children:"Autonomous vehicle perception"}),"\n",(0,r.jsx)(n.li,{children:"Warehouse robot navigation"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"ultrasonic-and-ir-range-sensors",children:"Ultrasonic and IR Range Sensors"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Ultrasonic sensors:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Emit high-frequency sound waves (40 kHz)"}),"\n",(0,r.jsx)(n.li,{children:"Measure time for echo to return"}),"\n",(0,r.jsx)(n.li,{children:"Range: 2cm - 5m"}),"\n",(0,r.jsx)(n.li,{children:"Very inexpensive ($1-10)"}),"\n",(0,r.jsx)(n.li,{children:"Wide beam angle (~15-30\xb0)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Infrared (IR) distance sensors:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Emit IR light and measure reflected intensity"}),"\n",(0,r.jsx)(n.li,{children:"Range: 4cm - 5m (varies by model)"}),"\n",(0,r.jsx)(n.li,{children:"Affected by surface color/reflectivity"}),"\n",(0,r.jsx)(n.li,{children:"Sharp GP2Y0A21YK example"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use cases:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Collision avoidance for small robots"}),"\n",(0,r.jsx)(n.li,{children:"Parking sensors in vehicles"}),"\n",(0,r.jsx)(n.li,{children:"Liquid level detection"}),"\n",(0,r.jsx)(n.li,{children:"Proximity detection"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"3-inertial-measurement-units-imus",children:"3. Inertial Measurement Units (IMUs)"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-an-imu",children:"What is an IMU?"}),"\n",(0,r.jsx)(n.p,{children:"An IMU combines multiple sensors to measure orientation, angular velocity, and linear acceleration."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Components:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Accelerometer"})," (3-axis)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measures linear acceleration in x, y, z"}),"\n",(0,r.jsx)(n.li,{children:"Detects gravity direction when stationary"}),"\n",(0,r.jsx)(n.li,{children:"Used to estimate tilt/roll"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Gyroscope"})," (3-axis)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measures angular velocity (rotation rate)"}),"\n",(0,r.jsx)(n.li,{children:"Detects how fast robot is rotating"}),"\n",(0,r.jsx)(n.li,{children:"Drifts over time (integration error)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Magnetometer"})," (3-axis) - optional"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measures magnetic field (like a compass)"}),"\n",(0,r.jsx)(n.li,{children:"Determines heading relative to Earth's magnetic north"}),"\n",(0,r.jsx)(n.li,{children:"Affected by ferromagnetic objects"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"9-DOF IMU"})," = 3-axis accel + 3-axis gyro + 3-axis mag"]}),"\n",(0,r.jsx)(n.h3,{id:"how-imus-work",children:"How IMUs Work"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor fusion:"}),"\nIMUs combine accelerometer and gyroscope data to estimate orientation:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accelerometer alone"}),": Good for tilt, but noisy and affected by movement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gyroscope alone"}),": Smooth but drifts over time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion (complementary filter, Kalman filter)"}),": Best of both"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common output:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Euler angles"}),": Roll, pitch, yaw (in degrees or radians)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quaternion"}),": 4D representation avoiding gimbal lock"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rotation matrix"}),": 3\xd73 matrix for transformations"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"imu-applications-in-robotics",children:"IMU Applications in Robotics"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Orientation tracking"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Drones: Maintain level flight"}),"\n",(0,r.jsx)(n.li,{children:"Humanoids: Balance control"}),"\n",(0,r.jsx)(n.li,{children:"Mobile robots: Heading estimation"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Dead reckoning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Estimate position by integrating acceleration"}),"\n",(0,r.jsx)(n.li,{children:"Useful when GPS/vision unavailable"}),"\n",(0,r.jsx)(n.li,{children:"Accumulates error quickly (double integration)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Sensor fusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Combine with wheel odometry for better localization"}),"\n",(0,r.jsx)(n.li,{children:"Merge with visual SLAM for robust navigation"}),"\n",(0,r.jsx)(n.li,{children:"Improve state estimation in Kalman filters"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Quadcopter stabilization"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class QuadcopterStabilizer:\n    def __init__(self):\n        self.imu = IMU()\n        self.target_roll = 0\n        self.target_pitch = 0\n\n    def update(self):\n        # Read current orientation from IMU\n        roll, pitch, yaw = self.imu.get_euler_angles()\n\n        # Compute error\n        roll_error = self.target_roll - roll\n        pitch_error = self.target_pitch - pitch\n\n        # Apply PD control\n        roll_correction = self.kp * roll_error + self.kd * self.imu.gyro_x\n        pitch_correction = self.kp * pitch_error + self.kd * self.imu.gyro_y\n\n        # Adjust motor speeds\n        self.adjust_motors(roll_correction, pitch_correction)\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common IMUs:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Cheap: MPU6050 ($2-5) - 6-DOF"}),"\n",(0,r.jsx)(n.li,{children:"Mid-range: BNO055 ($20-30) - 9-DOF with on-board fusion"}),"\n",(0,r.jsx)(n.li,{children:"High-end: VectorNav VN-100 ($500+) - Industrial-grade"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"4-proprioceptive-sensors",children:"4. Proprioceptive Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Proprioceptive sensors measure the robot's internal state (as opposed to exteroceptive sensors that measure the environment)."}),"\n",(0,r.jsx)(n.h3,{id:"joint-encoders",children:"Joint Encoders"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Measure joint angles or positions"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Types:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Incremental encoders"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Count pulses as shaft rotates"}),"\n",(0,r.jsx)(n.li,{children:"Relative position only"}),"\n",(0,r.jsx)(n.li,{children:"Need homing sequence"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Absolute encoders"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Know position immediately on power-up"}),"\n",(0,r.jsx)(n.li,{children:"More expensive"}),"\n",(0,r.jsx)(n.li,{children:"Common in industrial robots"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": Measured in counts per revolution (CPR)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Low: 100-500 CPR"}),"\n",(0,r.jsx)(n.li,{children:"Medium: 1000-2048 CPR"}),"\n",(0,r.jsx)(n.li,{children:"High: 4096-16384 CPR"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Applications:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Joint position feedback for control"}),"\n",(0,r.jsx)(n.li,{children:"Odometry (wheel rotation \u2192 distance traveled)"}),"\n",(0,r.jsx)(n.li,{children:"Gripper opening width"}),"\n",(0,r.jsx)(n.li,{children:"Elevator/lift position"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Measure forces and torques applied to robot"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"6-DOF F/T sensor"}),": Measures:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Forces: Fx, Fy, Fz"}),"\n",(0,r.jsx)(n.li,{children:"Torques: Tx, Ty, Tz"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Applications:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compliant manipulation"}),": Adjust grip force"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contact detection"}),": Detect collisions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force control"}),": Polish, assemble, sand"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human-robot collaboration"}),": Safety"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Adaptive grasping"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def adaptive_grasp(object, target_force=10.0):\n    """Close gripper until reaching target force"""\n    gripper.open()\n\n    while gripper.get_force() < target_force:\n        gripper.close(speed=0.01)  # Slow close\n        time.sleep(0.01)\n\n        if gripper.is_fully_closed():\n            print("Cannot grasp - object too large or missing")\n            return False\n\n    print(f"Grasped with {gripper.get_force()}N force")\n    return True\n'})}),"\n",(0,r.jsx)(n.h3,{id:"currenttorque-sensing",children:"Current/Torque Sensing"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Motor current monitoring:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measure current draw of motors"}),"\n",(0,r.jsx)(n.li,{children:"Infer applied torque (torque \u221d current)"}),"\n",(0,r.jsx)(n.li,{children:"Detect collisions (current spike)"}),"\n",(0,r.jsx)(n.li,{children:"Cheaper alternative to F/T sensors"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use cases:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Collision detection without external sensors"}),"\n",(0,r.jsx)(n.li,{children:"Torque limiting for safety"}),"\n",(0,r.jsx)(n.li,{children:"Detecting stuck motors"}),"\n",(0,r.jsx)(n.li,{children:"Battery monitoring"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"5-sensor-fusion-strategies",children:"5. Sensor Fusion Strategies"}),"\n",(0,r.jsx)(n.p,{children:"No single sensor is perfect. Sensor fusion combines multiple sensors to achieve better perception than any individual sensor."}),"\n",(0,r.jsx)(n.h3,{id:"why-fuse-sensors",children:"Why Fuse Sensors?"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Complementary strengths:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Camera: Rich semantic info, no depth"}),"\n",(0,r.jsx)(n.li,{children:"Depth camera: 3D info, limited range"}),"\n",(0,r.jsx)(n.li,{children:"LiDAR: Long range 3D, no color"}),"\n",(0,r.jsx)(n.li,{children:"IMU: Fast orientation, drifts over time"}),"\n",(0,r.jsx)(n.li,{children:"Wheel odometry: Smooth short-term, drifts long-term"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Goals:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Increase accuracy"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Increase robustness (sensor failure tolerance)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Expand operational envelope"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Reduce uncertainty"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"common-fusion-approaches",children:"Common Fusion Approaches"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Complementary Filter"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simple weighted average of sensors"}),"\n",(0,r.jsx)(n.li,{children:"Example: Fuse accelerometer (low-pass) + gyroscope (high-pass)"}),"\n",(0,r.jsx)(n.li,{children:"Fast, lightweight, good for IMU orientation"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Kalman Filter"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Optimal fusion under Gaussian noise assumptions"}),"\n",(0,r.jsx)(n.li,{children:"Predicts state, then corrects with measurements"}),"\n",(0,r.jsx)(n.li,{children:"Example: Fuse GPS + IMU + wheel odometry for robot position"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Particle Filter"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Non-parametric, handles non-Gaussian distributions"}),"\n",(0,r.jsx)(n.li,{children:"Used in Monte Carlo Localization (MCL)"}),"\n",(0,r.jsx)(n.li,{children:"Computationally expensive"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Sensor-specific fusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual-Inertial Odometry (VIO)"}),": Camera + IMU"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR-Inertial Odometry (LIO)"}),": LiDAR + IMU"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-modal SLAM"}),": Camera + LiDAR + wheel encoders"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-robot-localization-fusion",children:"Example: Robot Localization Fusion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class RobotLocalizer:\n    def __init__(self):\n        self.position = (0, 0, 0)  # x, y, theta\n        self.covariance = np.eye(3)\n\n    def predict(self, wheel_odom, dt):\n        """Prediction step using wheel odometry"""\n        # Move predicted position based on odometry\n        dx, dy, dtheta = wheel_odom\n        self.position = integrate_motion(self.position, dx, dy, dtheta)\n\n        # Increase uncertainty (odometry drifts)\n        self.covariance += process_noise(dt)\n\n    def update_lidar(self, lidar_scan):\n        """Correction step using LiDAR scan matching"""\n        # Match current scan to map\n        position_estimate = scan_match(lidar_scan, self.map)\n\n        # Fuse with prediction using Kalman update\n        self.position, self.covariance = kalman_update(\n            self.position, self.covariance,\n            position_estimate, lidar_noise\n        )\n\n    def update_imu(self, imu_data):\n        """Correction step for orientation using IMU"""\n        theta_estimate = imu_data.yaw\n\n        # Update only theta component\n        self.position[2] = kalman_update_1d(\n            self.position[2], self.covariance[2,2],\n            theta_estimate, imu_noise\n        )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor selection"}),": Choose sensors with complementary strengths"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Calibration"}),": Properly calibrate all sensors (intrinsics, extrinsics)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time synchronization"}),": Timestamp all measurements accurately"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Outlier rejection"}),": Detect and discard anomalous readings"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Graceful degradation"}),": System should work even if sensors fail"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Key takeaways from this chapter:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision sensors"})," provide rich semantic information but struggle with depth"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth cameras"})," add geometric understanding but have limited range"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"})," offers accurate long-range 3D sensing but is expensive"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMUs"})," track orientation and acceleration, essential for balance and navigation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Proprioceptive sensors"})," monitor internal state (joint angles, forces)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor fusion"})," combines strengths of multiple sensors for robust perception"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Choosing sensors depends on:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Environment (indoor/outdoor, lighting, weather)"}),"\n",(0,r.jsx)(n.li,{children:"Task requirements (precision, range, speed)"}),"\n",(0,r.jsx)(n.li,{children:"Budget constraints"}),"\n",(0,r.jsx)(n.li,{children:"Computational resources"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"What is the main advantage of depth cameras over RGB cameras?"}),"\n",(0,r.jsx)(n.li,{children:"Why does LiDAR work better than cameras in poor lighting?"}),"\n",(0,r.jsx)(n.li,{children:"What causes IMU gyroscopes to drift over time?"}),"\n",(0,r.jsx)(n.li,{children:"When would you use a force sensor instead of vision for grasping?"}),"\n",(0,r.jsx)(n.li,{children:"Why combine wheel odometry with IMU instead of using just one?"}),"\n",(0,r.jsx)(n.li,{children:"What is the difference between proprioceptive and exteroceptive sensors?"}),"\n",(0,r.jsx)(n.li,{children:"How does sensor fusion improve robot localization?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-camera-vs-depth-camera",children:"Exercise 1: Camera vs Depth Camera"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Capture RGB and depth images of the same scene"}),"\n",(0,r.jsx)(n.li,{children:"Attempt to measure object distance from RGB alone"}),"\n",(0,r.jsx)(n.li,{children:"Compare with direct depth measurement"}),"\n",(0,r.jsx)(n.li,{children:"Discuss accuracy and challenges"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-imu-orientation-tracking",children:"Exercise 2: IMU Orientation Tracking"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Read IMU data (accelerometer, gyroscope)"}),"\n",(0,r.jsx)(n.li,{children:"Implement complementary filter to estimate roll/pitch"}),"\n",(0,r.jsx)(n.li,{children:"Compare against gyroscope-only and accelerometer-only"}),"\n",(0,r.jsx)(n.li,{children:"Observe drift and noise characteristics"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-simple-sensor-fusion",children:"Exercise 3: Simple Sensor Fusion"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Collect wheel odometry and IMU yaw"}),"\n",(0,r.jsx)(n.li,{children:"Implement dead reckoning using odometry alone"}),"\n",(0,r.jsx)(n.li,{children:"Add IMU yaw to correct heading drift"}),"\n",(0,r.jsx)(n.li,{children:"Compare trajectories with/without fusion"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html",children:"Camera Calibration and 3D Reconstruction"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://velodynelidar.com/blog/lidar-applications-robotics/",children:"LiDAR for Robotics"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.nxp.com/docs/en/application-note/AN3461.pdf",children:"IMU Sensor Fusion"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.kalmanfilter.net/",children:"Kalman Filtering Tutorial"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/AI_COURSE_book/docs/foundations/week-02/ch03-actuators-robotics-arch",children:"Actuators and Robotics Architectures \u2192"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,r.jsx)(n.a,{href:"/AI_COURSE_book/docs/foundations/week-01/ch01-embodied-intelligence",children:"\u2190 Embodied Intelligence"})]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const r={},l=s.createContext(r);function t(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);