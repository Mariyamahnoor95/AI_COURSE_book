"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4066],{2743:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>g,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-02-digital-twin/week-06/ch08-sensor-modeling","title":"Sensor Modeling in Simulation","description":"Learning Objectives","source":"@site/docs/module-02-digital-twin/week-06/ch08-sensor-modeling.md","sourceDirName":"module-02-digital-twin/week-06","slug":"/module-02-digital-twin/week-06/ch08-sensor-modeling","permalink":"/AI_COURSE_book/docs/module-02-digital-twin/week-06/ch08-sensor-modeling","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-02-digital-twin/week-06/ch08-sensor-modeling.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"id":"ch08-sensor-modeling","title":"Sensor Modeling in Simulation","sidebar_label":"Sensor Modeling in Simulation","sidebar_position":9},"sidebar":"textbookSidebar","previous":{"title":"Gazebo Physics","permalink":"/AI_COURSE_book/docs/module-02-digital-twin/week-06/ch07-gazebo-physics"},"next":{"title":"Unity Visualization","permalink":"/AI_COURSE_book/docs/module-02-digital-twin/week-07/ch09-unity-viz"}}');var s=t(4848),l=t(8453);const r={id:"ch08-sensor-modeling",title:"Sensor Modeling in Simulation",sidebar_label:"Sensor Modeling in Simulation",sidebar_position:9},a="Sensor Modeling in Simulation",o={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Camera Simulation",id:"1-camera-simulation",level:2},{value:"Types of Simulated Cameras",id:"types-of-simulated-cameras",level:3},{value:"Configuring RGB Camera in Gazebo",id:"configuring-rgb-camera-in-gazebo",level:3},{value:"Depth Camera",id:"depth-camera",level:3},{value:"Camera Intrinsic Parameters",id:"camera-intrinsic-parameters",level:3},{value:"Visualizing Camera Output",id:"visualizing-camera-output",level:3},{value:"2. LiDAR and Ray Tracing",id:"2-lidar-and-ray-tracing",level:2},{value:"2D LiDAR (Planar Scan)",id:"2d-lidar-planar-scan",level:3},{value:"3D LiDAR (Volumetric)",id:"3d-lidar-volumetric",level:3},{value:"GPU vs CPU Ray Tracing",id:"gpu-vs-cpu-ray-tracing",level:3},{value:"LiDAR Point Cloud Processing",id:"lidar-point-cloud-processing",level:3},{value:"3. IMU and Noise Models",id:"3-imu-and-noise-models",level:2},{value:"What is an IMU in Simulation?",id:"what-is-an-imu-in-simulation",level:3},{value:"Configuring IMU in Gazebo",id:"configuring-imu-in-gazebo",level:3},{value:"Noise Model Parameters",id:"noise-model-parameters",level:3},{value:"Using IMU Data",id:"using-imu-data",level:3},{value:"4. Ground Truth Data",id:"4-ground-truth-data",level:2},{value:"What is Ground Truth?",id:"what-is-ground-truth",level:3},{value:"Accessing Ground Truth Pose",id:"accessing-ground-truth-pose",level:3},{value:"Semantic Segmentation Camera",id:"semantic-segmentation-camera",level:3},{value:"5. Sim-to-Real Considerations",id:"5-sim-to-real-considerations",level:2},{value:"The Sim-to-Real Gap",id:"the-sim-to-real-gap",level:3},{value:"Strategies to Bridge the Gap",id:"strategies-to-bridge-the-gap",level:3},{value:"Validation Workflow",id:"validation-workflow",level:3},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Camera Configuration",id:"exercise-1-camera-configuration",level:3},{value:"Exercise 2: LiDAR Obstacle Detection",id:"exercise-2-lidar-obstacle-detection",level:3},{value:"Exercise 3: Ground Truth Validation",id:"exercise-3-ground-truth-validation",level:3},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sensor-modeling-in-simulation",children:"Sensor Modeling in Simulation"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure realistic camera sensors in Gazebo (RGB, depth, fisheye)"}),"\n",(0,s.jsx)(n.li,{children:"Implement LiDAR sensors with ray tracing and GPU acceleration"}),"\n",(0,s.jsx)(n.li,{children:"Model IMU sensors with realistic noise characteristics"}),"\n",(0,s.jsx)(n.li,{children:"Leverage ground truth data for algorithm development and validation"}),"\n",(0,s.jsx)(n.li,{children:"Understand and address the sim-to-real gap in sensor modeling"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:["Real-world sensors are noisy, imperfect, and expensive. Cameras have lens distortion, depth sensors have limited range, LiDAR misses transparent objects, IMUs drift over time. ",(0,s.jsx)(n.strong,{children:"How do we develop and test perception algorithms without needing dozens of physical robots and sensors?"})]}),"\n",(0,s.jsxs)(n.p,{children:["This is where ",(0,s.jsx)(n.strong,{children:"sensor simulation"})," becomes critical. Modern simulators like Gazebo and NVIDIA Isaac Sim can model sensor physics with remarkable realism:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cameras"})," with proper optics, rolling shutter, motion blur"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR"})," with ray tracing, reflection properties, atmospheric effects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMUs"})," with gyro drift, accelerometer noise, temperature effects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ground truth"})," data that's impossible to get in the real world"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why sensor simulation matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Rapid prototyping"}),": Test algorithms without hardware"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Edge case testing"}),": Create scenarios hard to reproduce (rain, night, obstacles)"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Data generation"}),": Create labeled datasets for machine learning"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Safe testing"}),": Crash virtual robots, not real ones"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Parallel development"}),": Hardware and software teams work concurrently"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-world scenario"}),": You're developing a vision-based navigation system. With sensor simulation, you can:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Generate 10,000 labeled images in a day"}),"\n",(0,s.jsx)(n.li,{children:"Test performance in fog, rain, night conditions"}),"\n",(0,s.jsx)(n.li,{children:"Validate algorithms before hardware arrives"}),"\n",(0,s.jsx)(n.li,{children:"Identify edge cases that would take months to encounter"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["However, simulation isn't perfect - there's always a ",(0,s.jsx)(n.strong,{children:"sim-to-real gap"})," between virtual and physical sensors. This chapter teaches you how to model sensors realistically and minimize this gap."]}),"\n",(0,s.jsx)(n.h2,{id:"1-camera-simulation",children:"1. Camera Simulation"}),"\n",(0,s.jsx)(n.h3,{id:"types-of-simulated-cameras",children:"Types of Simulated Cameras"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"1. RGB Camera"})," - Standard color images\n",(0,s.jsx)(n.strong,{children:"2. Depth Camera"})," - Distance to each pixel\n",(0,s.jsx)(n.strong,{children:"3. Segmentation Camera"})," - Per-pixel labels (for ground truth)\n",(0,s.jsx)(n.strong,{children:"4. Fisheye Camera"})," - Wide field-of-view with distortion"]}),"\n",(0,s.jsx)(n.h3,{id:"configuring-rgb-camera-in-gazebo",children:"Configuring RGB Camera in Gazebo"}),"\n",(0,s.jsx)(n.p,{children:"In URDF/Xacro with Gazebo plugin:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'&lt;gazebo reference="camera_link"&gt;\n  &lt;sensor name="rgb_camera" type="camera"&gt;\n    &lt;always_on&gt;true&lt;/always_on&gt;\n    &lt;update_rate&gt;30&lt;/update_rate&gt;\n    &lt;visualize&gt;true&lt;/visualize&gt;\n\n    &lt;camera name="front_camera"&gt;\n      &lt;!-- Field of view (radians) --&gt;\n      &lt;horizontal_fov&gt;1.3962634&lt;/horizontal_fov&gt;  &lt;!-- 80 degrees --&gt;\n\n      &lt;!-- Image resolution --&gt;\n      &lt;image&gt;\n        &lt;width&gt;1920&lt;/width&gt;\n        &lt;height&gt;1080&lt;/height&gt;\n        &lt;format&gt;R8G8B8&lt;/format&gt;\n      &lt;/image&gt;\n\n      &lt;!-- Clipping planes --&gt;\n      &lt;clip&gt;\n        &lt;near&gt;0.1&lt;/near&gt;  &lt;!-- Min distance --&gt;\n        &lt;far&gt;100.0&lt;/far&gt;   &lt;!-- Max distance --&gt;\n      &lt;/clip&gt;\n\n      &lt;!-- Lens distortion (optional) --&gt;\n      &lt;distortion&gt;\n        &lt;k1&gt;0.1&lt;/k1&gt;  &lt;!-- Radial distortion --&gt;\n        &lt;k2&gt;0.05&lt;/k2&gt;\n        &lt;k3&gt;0.01&lt;/k3&gt;\n        &lt;p1&gt;0.001&lt;/p1&gt;  &lt;!-- Tangential distortion --&gt;\n        &lt;p2&gt;0.001&lt;/p2&gt;\n        &lt;center&gt;0.5 0.5&lt;/center&gt;\n      &lt;/distortion&gt;\n\n      &lt;!-- Image noise --&gt;\n      &lt;noise&gt;\n        &lt;type&gt;gaussian&lt;/type&gt;\n        &lt;mean&gt;0.0&lt;/mean&gt;\n        &lt;stddev&gt;0.007&lt;/stddev&gt;\n      &lt;/noise&gt;\n    &lt;/camera&gt;\n\n    &lt;!-- ROS 2 plugin to publish images --&gt;\n    &lt;plugin name="camera_controller" filename="libgazebo_ros_camera.so"&gt;\n      &lt;ros&gt;\n        &lt;namespace&gt;/camera&lt;/namespace&gt;\n        &lt;remapping&gt;image_raw:=rgb&lt;/remapping&gt;\n        &lt;remapping&gt;camera_info:=camera_info&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;camera_name&gt;front_camera&lt;/camera_name&gt;\n      &lt;frame_name&gt;camera_link&lt;/frame_name&gt;\n      &lt;hack_baseline&gt;0.07&lt;/hack_baseline&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/gazebo&gt;\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Published topics:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/camera/rgb"})," (sensor_msgs/Image) - Color images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/camera/camera_info"})," (sensor_msgs/CameraInfo) - Intrinsic parameters"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"depth-camera",children:"Depth Camera"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'&lt;gazebo reference="depth_camera_link"&gt;\n  &lt;sensor name="depth_camera" type="depth"&gt;\n    &lt;update_rate&gt;20&lt;/update_rate&gt;\n    &lt;camera&gt;\n      &lt;horizontal_fov&gt;1.047198&lt;/horizontal_fov&gt;\n      &lt;image&gt;\n        &lt;width&gt;640&lt;/width&gt;\n        &lt;height&gt;480&lt;/height&gt;\n      &lt;/image&gt;\n      &lt;clip&gt;\n        &lt;near&gt;0.3&lt;/near&gt;  &lt;!-- Depth cameras have limited min range --&gt;\n        &lt;far&gt;10.0&lt;/far&gt;\n      &lt;/clip&gt;\n    &lt;/camera&gt;\n\n    &lt;plugin name="depth_camera_controller" filename="libgazebo_ros_camera.so"&gt;\n      &lt;ros&gt;\n        &lt;namespace&gt;/depth_camera&lt;/namespace&gt;\n        &lt;remapping&gt;depth/image_raw:=depth&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;min_depth&gt;0.3&lt;/min_depth&gt;\n      &lt;max_depth&gt;10.0&lt;/max_depth&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/gazebo&gt;\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Published topics:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/depth_camera/depth"})," (sensor_msgs/Image, 32FC1) - Depth map in meters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/depth_camera/points"})," (sensor_msgs/PointCloud2) - 3D point cloud"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"camera-intrinsic-parameters",children:"Camera Intrinsic Parameters"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"camera_info"})," message contains calibration data:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Intrinsic matrix K\n# [ fx  0  cx ]\n# [  0 fy  cy ]\n# [  0  0   1 ]\n\n# fx, fy: focal length in pixels\n# cx, cy: principal point (image center)\n\n# Calculated from FOV:\n# fx = (image_width / 2) / tan(horizontal_fov / 2)\n# fy = (image_height / 2) / tan(vertical_fov / 2)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"visualizing-camera-output",children:"Visualizing Camera Output"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass CameraViewer(Node):\n    def __init__(self):\n        super().__init__('camera_viewer')\n        self.bridge = CvBridge()\n\n        self.rgb_sub = self.create_subscription(\n            Image, '/camera/rgb', self.rgb_callback, 10\n        )\n\n    def rgb_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Display\n        cv2.imshow('Camera', cv_image)\n        cv2.waitKey(1)\n\ndef main():\n    rclpy.init()\n    node = CameraViewer()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"2-lidar-and-ray-tracing",children:"2. LiDAR and Ray Tracing"}),"\n",(0,s.jsx)(n.h3,{id:"2d-lidar-planar-scan",children:"2D LiDAR (Planar Scan)"}),"\n",(0,s.jsx)(n.p,{children:"2D LiDAR scans in a horizontal plane (common for mobile robots):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'&lt;gazebo reference="lidar_link"&gt;\n  &lt;sensor name="lidar_2d" type="gpu_ray"&gt;\n    &lt;always_on&gt;true&lt;/always_on&gt;\n    &lt;update_rate&gt;10&lt;/update_rate&gt;\n    &lt;visualize&gt;true&lt;/visualize&gt;\n\n    &lt;ray&gt;\n      &lt;scan&gt;\n        &lt;horizontal&gt;\n          &lt;samples&gt;360&lt;/samples&gt;        &lt;!-- 360 laser beams --&gt;\n          &lt;resolution&gt;1&lt;/resolution&gt;    &lt;!-- Sample every beam --&gt;\n          &lt;min_angle&gt;-3.14159&lt;/min_angle&gt;  &lt;!-- -180 degrees --&gt;\n          &lt;max_angle&gt;3.14159&lt;/max_angle&gt;   &lt;!-- +180 degrees --&gt;\n        &lt;/horizontal&gt;\n      &lt;/scan&gt;\n\n      &lt;range&gt;\n        &lt;min&gt;0.1&lt;/min&gt;   &lt;!-- 10cm minimum --&gt;\n        &lt;max&gt;30.0&lt;/max&gt;  &lt;!-- 30m maximum --&gt;\n        &lt;resolution&gt;0.01&lt;/resolution&gt;  &lt;!-- 1cm resolution --&gt;\n      &lt;/range&gt;\n\n      &lt;!-- Ray properties --&gt;\n      &lt;noise&gt;\n        &lt;type&gt;gaussian&lt;/type&gt;\n        &lt;mean&gt;0.0&lt;/mean&gt;\n        &lt;stddev&gt;0.01&lt;/stddev&gt;  &lt;!-- 1cm noise --&gt;\n      &lt;/noise&gt;\n    &lt;/ray&gt;\n\n    &lt;plugin name="lidar_plugin" filename="libgazebo_ros_ray_sensor.so"&gt;\n      &lt;ros&gt;\n        &lt;namespace&gt;/&lt;/namespace&gt;\n        &lt;remapping&gt;~/out:=scan&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;output_type&gt;sensor_msgs/LaserScan&lt;/output_type&gt;\n      &lt;frame_name&gt;lidar_link&lt;/frame_name&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/gazebo&gt;\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Published topic:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/scan"})," (sensor_msgs/LaserScan) - 2D range data"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3d-lidar-volumetric",children:"3D LiDAR (Volumetric)"}),"\n",(0,s.jsx)(n.p,{children:"3D LiDAR with multiple vertical layers:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'&lt;gazebo reference="lidar_3d_link"&gt;\n  &lt;sensor name="lidar_3d" type="gpu_ray"&gt;\n    &lt;update_rate&gt;10&lt;/update_rate&gt;\n    &lt;ray&gt;\n      &lt;scan&gt;\n        &lt;horizontal&gt;\n          &lt;samples&gt;1024&lt;/samples&gt;\n          &lt;min_angle&gt;-3.14159&lt;/min_angle&gt;\n          &lt;max_angle&gt;3.14159&lt;/max_angle&gt;\n        &lt;/horizontal&gt;\n        &lt;vertical&gt;\n          &lt;samples&gt;16&lt;/samples&gt;  &lt;!-- 16 laser layers --&gt;\n          &lt;min_angle&gt;-0.2618&lt;/min_angle&gt;  &lt;!-- -15 degrees down --&gt;\n          &lt;max_angle&gt;0.2618&lt;/max_angle&gt;   &lt;!-- +15 degrees up --&gt;\n        &lt;/vertical&gt;\n      &lt;/scan&gt;\n      &lt;range&gt;\n        &lt;min&gt;0.5&lt;/min&gt;\n        &lt;max&gt;100.0&lt;/max&gt;\n      &lt;/range&gt;\n    &lt;/ray&gt;\n\n    &lt;plugin name="lidar_3d_plugin" filename="libgazebo_ros_ray_sensor.so"&gt;\n      &lt;ros&gt;\n        &lt;remapping&gt;~/out:=points&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;output_type&gt;sensor_msgs/PointCloud2&lt;/output_type&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/gazebo&gt;\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Published topic:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/points"})," (sensor_msgs/PointCloud2) - 3D point cloud"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"gpu-vs-cpu-ray-tracing",children:"GPU vs CPU Ray Tracing"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsxs)(n.th,{children:["CPU Ray (",(0,s.jsx)(n.code,{children:"ray"}),")"]}),(0,s.jsxs)(n.th,{children:["GPU Ray (",(0,s.jsx)(n.code,{children:"gpu_ray"}),")"]})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Speed"}),(0,s.jsx)(n.td,{children:"Slow"}),(0,s.jsx)(n.td,{children:"10-100x faster"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accuracy"}),(0,s.jsx)(n.td,{children:"Identical"}),(0,s.jsx)(n.td,{children:"Identical"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Samples"}),(0,s.jsx)(n.td,{children:"<100 rays"}),(0,s.jsx)(n.td,{children:"1000+ rays"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Use case"}),(0,s.jsx)(n.td,{children:"Simple 2D LiDAR"}),(0,s.jsx)(n.td,{children:"3D LiDAR, many sensors"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Always use ",(0,s.jsx)(n.code,{children:"gpu_ray"})," for:"]})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"3D LiDAR with >100 samples"}),"\n",(0,s.jsx)(n.li,{children:"Multiple LiDAR sensors"}),"\n",(0,s.jsx)(n.li,{children:"Real-time applications"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"lidar-point-cloud-processing",children:"LiDAR Point Cloud Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2\nimport sensor_msgs_py.point_cloud2 as pc2\n\nclass LidarProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n        self.sub = self.create_subscription(\n            PointCloud2, '/points', self.lidar_callback, 10\n        )\n\n    def lidar_callback(self, msg):\n        # Convert PointCloud2 to list of (x, y, z)\n        points = pc2.read_points(msg, field_names=(\"x\", \"y\", \"z\"), skip_nans=True)\n\n        # Process points\n        obstacle_count = 0\n        for x, y, z in points:\n            distance = (x**2 + y**2 + z**2) ** 0.5\n            if distance &lt; 1.0:  # Obstacle within 1m\n                obstacle_count += 1\n\n        if obstacle_count &gt; 10:\n            self.get_logger().warn(f'Obstacle detected: {obstacle_count} points')\n"})}),"\n",(0,s.jsx)(n.h2,{id:"3-imu-and-noise-models",children:"3. IMU and Noise Models"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-an-imu-in-simulation",children:"What is an IMU in Simulation?"}),"\n",(0,s.jsx)(n.p,{children:"IMUs measure:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Linear acceleration"})," (m/s\xb2) in x, y, z"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Angular velocity"})," (rad/s) in roll, pitch, yaw"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Orientation"})," (quaternion) - often from sensor fusion"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"configuring-imu-in-gazebo",children:"Configuring IMU in Gazebo"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'&lt;gazebo reference="imu_link"&gt;\n  &lt;sensor name="imu_sensor" type="imu"&gt;\n    &lt;always_on&gt;true&lt;/always_on&gt;\n    &lt;update_rate&gt;100&lt;/update_rate&gt;  &lt;!-- IMUs are high-frequency --&gt;\n\n    &lt;imu&gt;\n      &lt;!-- Angular velocity noise --&gt;\n      &lt;angular_velocity&gt;\n        &lt;x&gt;\n          &lt;noise type="gaussian"&gt;\n            &lt;mean&gt;0.0&lt;/mean&gt;\n            &lt;stddev&gt;0.0002&lt;/stddev&gt;\n            &lt;bias_mean&gt;0.00001&lt;/bias_mean&gt;  &lt;!-- Gyro drift --&gt;\n            &lt;bias_stddev&gt;0.000001&lt;/bias_stddev&gt;\n          &lt;/noise&gt;\n        &lt;/x&gt;\n        &lt;y&gt;\n          &lt;noise type="gaussian"&gt;\n            &lt;mean&gt;0.0&lt;/mean&gt;\n            &lt;stddev&gt;0.0002&lt;/stddev&gt;\n            &lt;bias_mean&gt;0.00001&lt;/bias_mean&gt;\n            &lt;bias_stddev&gt;0.000001&lt;/bias_stddev&gt;\n          &lt;/noise&gt;\n        &lt;/y&gt;\n        &lt;z&gt;\n          &lt;noise type="gaussian"&gt;\n            &lt;mean&gt;0.0&lt;/mean&gt;\n            &lt;stddev&gt;0.0002&lt;/stddev&gt;\n            &lt;bias_mean&gt;0.00001&lt;/bias_mean&gt;\n            &lt;bias_stddev&gt;0.000001&lt;/bias_stddev&gt;\n          &lt;/noise&gt;\n        &lt;/z&gt;\n      &lt;/angular_velocity&gt;\n\n      &lt;!-- Linear acceleration noise --&gt;\n      &lt;linear_acceleration&gt;\n        &lt;x&gt;\n          &lt;noise type="gaussian"&gt;\n            &lt;mean&gt;0.0&lt;/mean&gt;\n            &lt;stddev&gt;0.017&lt;/stddev&gt;  &lt;!-- Typical accelerometer --&gt;\n            &lt;bias_mean&gt;0.1&lt;/bias_mean&gt;\n            &lt;bias_stddev&gt;0.001&lt;/bias_stddev&gt;\n          &lt;/noise&gt;\n        &lt;/x&gt;\n        &lt;y&gt;\n          &lt;noise type="gaussian"&gt;\n            &lt;mean&gt;0.0&lt;/mean&gt;\n            &lt;stddev&gt;0.017&lt;/stddev&gt;\n            &lt;bias_mean&gt;0.1&lt;/bias_mean&gt;\n            &lt;bias_stddev&gt;0.001&lt;/bias_stddev&gt;\n          &lt;/noise&gt;\n        &lt;/y&gt;\n        &lt;z&gt;\n          &lt;noise type="gaussian"&gt;\n            &lt;mean&gt;0.0&lt;/mean&gt;\n            &lt;stddev&gt;0.017&lt;/stddev&gt;\n            &lt;bias_mean&gt;0.1&lt;/bias_mean&gt;\n            &lt;bias_stddev&gt;0.001&lt;/bias_stddev&gt;\n          &lt;/noise&gt;\n        &lt;/z&gt;\n      &lt;/linear_acceleration&gt;\n    &lt;/imu&gt;\n\n    &lt;plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so"&gt;\n      &lt;ros&gt;\n        &lt;namespace&gt;/imu&lt;/namespace&gt;\n        &lt;remapping&gt;~/out:=data&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;frame_name&gt;imu_link&lt;/frame_name&gt;\n      &lt;initial_orientation_as_reference&gt;false&lt;/initial_orientation_as_reference&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/gazebo&gt;\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Published topic:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/imu/data"})," (sensor_msgs/Imu)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"noise-model-parameters",children:"Noise Model Parameters"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Gaussian noise"}),": Random error around mean"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"mean"}),": Average offset (often 0)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"stddev"}),": Standard deviation (variability)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Bias"}),": Systematic error that drifts"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"bias_mean"}),": Average bias offset"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"bias_stddev"}),": How much bias changes"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Realistic values"})," (based on common IMUs):"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Sensor"}),(0,s.jsx)(n.th,{children:"Noise (stddev)"}),(0,s.jsx)(n.th,{children:"Bias Mean"}),(0,s.jsx)(n.th,{children:"Application"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Gyro (cheap)"}),(0,s.jsx)(n.td,{children:"0.01 rad/s"}),(0,s.jsx)(n.td,{children:"0.001 rad/s"}),(0,s.jsx)(n.td,{children:"Hobby drones"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Gyro (good)"}),(0,s.jsx)(n.td,{children:"0.0002 rad/s"}),(0,s.jsx)(n.td,{children:"0.00001 rad/s"}),(0,s.jsx)(n.td,{children:"Industrial robots"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accel (cheap)"}),(0,s.jsx)(n.td,{children:"0.1 m/s\xb2"}),(0,s.jsx)(n.td,{children:"0.5 m/s\xb2"}),(0,s.jsx)(n.td,{children:"Hobby projects"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accel (good)"}),(0,s.jsx)(n.td,{children:"0.017 m/s\xb2"}),(0,s.jsx)(n.td,{children:"0.1 m/s\xb2"}),(0,s.jsx)(n.td,{children:"Autonomous vehicles"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"using-imu-data",children:"Using IMU Data"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\n\nclass IMUReader(Node):\n    def __init__(self):\n        super().__init__('imu_reader')\n        self.sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\n\n    def imu_callback(self, msg):\n        # Orientation (quaternion)\n        q = msg.orientation\n        # Convert to Euler angles for human readability\n        roll, pitch, yaw = self.quaternion_to_euler(q.x, q.y, q.z, q.w)\n\n        # Angular velocity (rad/s)\n        gyro_x = msg.angular_velocity.x\n        gyro_y = msg.angular_velocity.y\n        gyro_z = msg.angular_velocity.z\n\n        # Linear acceleration (m/s\xb2)\n        accel_x = msg.linear_acceleration.x\n        accel_y = msg.linear_acceleration.y\n        accel_z = msg.linear_acceleration.z\n\n        self.get_logger().info(\n            f'Orientation: roll={roll:.2f}, pitch={pitch:.2f}, yaw={yaw:.2f}'\n        )\n"})}),"\n",(0,s.jsx)(n.h2,{id:"4-ground-truth-data",children:"4. Ground Truth Data"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-ground-truth",children:"What is Ground Truth?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ground truth"})," = perfect, noise-free data about the environment"]}),"\n",(0,s.jsx)(n.p,{children:"In simulation, you can access:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Exact robot pose (position + orientation)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Perfect object locations"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Semantic labels for every pixel"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Exact velocities and accelerations"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why ground truth is valuable:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Algorithm validation"}),": Compare estimated pose vs true pose"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dataset labeling"}),": Auto-label training data for ML"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance metrics"}),": Measure localization error"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Debugging"}),": Identify where algorithms fail"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"accessing-ground-truth-pose",children:"Accessing Ground Truth Pose"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from gazebo_msgs.srv import GetModelState\n\nclass GroundTruthTracker(Node):\n    def __init__(self):\n        super().__init__('ground_truth_tracker')\n\n        # Service client to get model state\n        self.client = self.create_client(GetModelState, '/get_model_state')\n\n        self.timer = self.create_timer(0.1, self.get_true_pose)\n\n    def get_true_pose(self):\n        request = GetModelState.Request()\n        request.model_name = 'my_robot'\n        request.relative_entity_name = 'world'\n\n        future = self.client.call_async(request)\n        future.add_done_callback(self.handle_response)\n\n    def handle_response(self, future):\n        try:\n            response = future.result()\n            pose = response.pose\n\n            self.get_logger().info(\n                f'True position: ({pose.position.x:.2f}, {pose.position.y:.2f})'\n            )\n        except Exception as e:\n            self.get_logger().error(f'Service call failed: {e}')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"semantic-segmentation-camera",children:"Semantic Segmentation Camera"}),"\n",(0,s.jsx)(n.p,{children:"Get per-pixel labels for object classes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'&lt;gazebo reference="seg_camera_link"&gt;\n  &lt;sensor name="segmentation_camera" type="segmentation"&gt;\n    &lt;update_rate&gt;10&lt;/update_rate&gt;\n    &lt;camera&gt;\n      &lt;image&gt;\n        &lt;width&gt;640&lt;/width&gt;\n        &lt;height&gt;480&lt;/height&gt;\n      &lt;/image&gt;\n    &lt;/camera&gt;\n    &lt;plugin name="seg_plugin" filename="libgazebo_ros_segmentation_camera.so"&gt;\n      &lt;ros&gt;\n        &lt;remapping&gt;~/out:=segmentation&lt;/remapping&gt;\n      &lt;/ros&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/gazebo&gt;\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": Image where pixel values = object class IDs"]}),"\n",(0,s.jsx)(n.h2,{id:"5-sim-to-real-considerations",children:"5. Sim-to-Real Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"the-sim-to-real-gap",children:"The Sim-to-Real Gap"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Algorithms that work perfectly in simulation often fail on real robots."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Causes:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u274c Physics approximations (friction, contact, deformation)"}),"\n",(0,s.jsx)(n.li,{children:"\u274c Sensor models don't match reality exactly"}),"\n",(0,s.jsx)(n.li,{children:"\u274c Simulation is too clean (no dust, wear, vibration)"}),"\n",(0,s.jsx)(n.li,{children:"\u274c Lighting and materials differ"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"strategies-to-bridge-the-gap",children:"Strategies to Bridge the Gap"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Domain Randomization"})}),"\n",(0,s.jsx)(n.p,{children:"Add random variations to simulation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Randomize lighting\nlight_intensity = random.uniform(0.5, 1.5)\n\n# Randomize object textures\ntexture = random.choice(['wood', 'metal', 'plastic'])\n\n# Randomize sensor noise\nnoise_stddev = random.uniform(0.01, 0.05)\n\n# Randomize friction\nfriction = random.uniform(0.5, 1.5)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why it works"}),": Algorithms learn to be robust to variations"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. System Identification"})}),"\n",(0,s.jsx)(n.p,{children:"Measure real robot parameters and update simulation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Measure real robot mass by weighing\nreal_mass = 12.3  # kg\n\n# Update URDF\n&lt;inertial&gt;\n  &lt;mass value="12.3"/&gt;  &lt;!-- Not default 10.0 --&gt;\n&lt;/inertial&gt;\n\n# Measure wheel friction by observing slip\nreal_friction_coefficient = 0.8\n\n# Update Gazebo\n&lt;gazebo reference="wheel"&gt;\n  &lt;mu1&gt;0.8&lt;/mu1&gt;\n  &lt;mu2&gt;0.8&lt;/mu2&gt;\n&lt;/gazebo&gt;\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Sim-to-Real Transfer Learning"})}),"\n",(0,s.jsx)(n.p,{children:"Train in simulation, fine-tune on real data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Stage 1: Pre-train in simulation (100k samples)\nmodel.train(sim_dataset)\n\n# Stage 2: Fine-tune on real data (1k samples)\nmodel.fine_tune(real_dataset, epochs=10, lr=0.0001)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"4. Conservative Tuning"})}),"\n",(0,s.jsx)(n.p,{children:"Design for robustness, not performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Bad: Aggressive PID gains (work in sim, crash in reality)\nkp_sim = 10.0\n\n# Good: Conservative gains (stable in both)\nkp_real = 3.0\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"5. Realistic Sensor Modeling"})}),"\n",(0,s.jsx)(n.p,{children:"Match simulation noise to real sensor specs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Measure real LiDAR noise\nreal_lidar_noise_cm = 2.5\n\n# Configure simulation\n&lt;noise&gt;\n  &lt;stddev&gt;0.025&lt;/stddev&gt;  &lt;!-- 2.5cm in meters --&gt;\n&lt;/noise&gt;\n"})}),"\n",(0,s.jsx)(n.h3,{id:"validation-workflow",children:"Validation Workflow"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"1. Develop algorithm in simulation\n2. Test with domain randomization\n3. Validate on ground truth data\n4. Deploy to real robot\n5. Measure performance gap\n6. Update simulation parameters\n7. Re-train if gap is large\n8. Iterate\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Key takeaways from this chapter:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Camera simulation"})," provides RGB, depth, and segmentation with realistic optics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR ray tracing"})," models 2D/3D sensors with GPU acceleration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU noise models"})," include gyro drift and accelerometer bias"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ground truth data"})," enables algorithm validation impossible in real world"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sim-to-real gap"})," requires domain randomization, system ID, and conservative design"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Best practices:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Always add realistic noise to sensors"}),"\n",(0,s.jsx)(n.li,{children:"Use ground truth only for validation, not in main algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Test with domain randomization before real deployment"}),"\n",(0,s.jsx)(n.li,{children:"Measure real sensor specs and match them in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Start conservative, tune aggressively only if needed"}),"\n",(0,s.jsx)(n.li,{children:"Document sim parameters for reproducibility"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What is the difference between RGB and depth cameras in simulation?"}),"\n",(0,s.jsxs)(n.li,{children:["Why use ",(0,s.jsx)(n.code,{children:"gpu_ray"})," instead of ",(0,s.jsx)(n.code,{children:"ray"})," for LiDAR?"]}),"\n",(0,s.jsx)(n.li,{children:"What causes IMU gyroscope drift over time?"}),"\n",(0,s.jsx)(n.li,{children:"How does ground truth data help with algorithm development?"}),"\n",(0,s.jsx)(n.li,{children:"Name three strategies to reduce the sim-to-real gap."}),"\n",(0,s.jsx)(n.li,{children:"Why add noise to simulated sensors instead of perfect data?"}),"\n",(0,s.jsx)(n.li,{children:"What is domain randomization and how does it help transfer learning?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-camera-configuration",children:"Exercise 1: Camera Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Create a robot with RGB and depth cameras:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure 30 Hz RGB camera with 90\xb0 FOV"}),"\n",(0,s.jsx)(n.li,{children:"Add Gaussian noise (stddev=0.01)"}),"\n",(0,s.jsxs)(n.li,{children:["Publish to ",(0,s.jsx)(n.code,{children:"/camera/rgb"})," and ",(0,s.jsx)(n.code,{children:"/camera/depth"})]}),"\n",(0,s.jsx)(n.li,{children:"Visualize in RViz and verify topics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-lidar-obstacle-detection",children:"Exercise 2: LiDAR Obstacle Detection"}),"\n",(0,s.jsx)(n.p,{children:"Implement LiDAR-based obstacle avoidance:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure 2D LiDAR (360\xb0, 10m range)"}),"\n",(0,s.jsxs)(n.li,{children:["Subscribe to ",(0,s.jsx)(n.code,{children:"/scan"})," topic"]}),"\n",(0,s.jsx)(n.li,{children:"Detect obstacles within 1m"}),"\n",(0,s.jsx)(n.li,{children:"Publish warning messages"}),"\n",(0,s.jsx)(n.li,{children:"Test in Gazebo with obstacles"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-ground-truth-validation",children:"Exercise 3: Ground Truth Validation"}),"\n",(0,s.jsx)(n.p,{children:"Compare odometry to ground truth:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Subscribe to ",(0,s.jsx)(n.code,{children:"/odom"})," (from wheel encoders)"]}),"\n",(0,s.jsx)(n.li,{children:"Query ground truth pose from Gazebo"}),"\n",(0,s.jsx)(n.li,{children:"Compute position error over time"}),"\n",(0,s.jsx)(n.li,{children:"Plot error growth (demonstrates drift)"}),"\n",(0,s.jsx)(n.li,{children:"Log results to file"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"http://gazebosim.org/tutorials?tut=ros2_installing&cat=connect_ros",children:"Gazebo Sensors Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/ros-simulation/gazebo_ros_pkgs",children:"ROS 2 Gazebo Plugins"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1703.06907",children:"Domain Randomization for Sim-to-Real Transfer"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/features/sensors_simulation/index.html",children:"NVIDIA Isaac Sim Sensor Models"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,s.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-02-digital-twin/week-07/ch09-unity-viz",children:"Unity Visualization \u2192"})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,s.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-02-digital-twin/week-06/ch07-gazebo-physics",children:"\u2190 Gazebo Physics Simulation"})]})]})}function g(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},l=i.createContext(s);function r(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);