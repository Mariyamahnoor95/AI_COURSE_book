"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9143],{2218:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-03-isaac/week-09/ch13-vslam-nav2","title":"Visual SLAM and Navigation","description":"Learning Objectives","source":"@site/docs/module-03-isaac/week-09/ch13-vslam-nav2.md","sourceDirName":"module-03-isaac/week-09","slug":"/module-03-isaac/week-09/ch13-vslam-nav2","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-09/ch13-vslam-nav2","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-isaac/week-09/ch13-vslam-nav2.md","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"id":"ch13-vslam-nav2","title":"Visual SLAM and Navigation","sidebar_label":"Visual SLAM and Navigation","sidebar_position":14},"sidebar":"textbookSidebar","previous":{"title":"Isaac ROS Perception","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-08/ch12-isaac-ros"},"next":{"title":"Perception Pipelines","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-09/ch14-perception"}}');var r=s(4848),o=s(8453);const a={id:"ch13-vslam-nav2",title:"Visual SLAM and Navigation",sidebar_label:"Visual SLAM and Navigation",sidebar_position:14},t="Visual SLAM and Navigation",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. VSLAM Algorithms (ORB-SLAM)",id:"1-vslam-algorithms-orb-slam",level:2},{value:"What is Visual SLAM?",id:"what-is-visual-slam",level:3},{value:"ORB-SLAM3",id:"orb-slam3",level:3},{value:"ORB Features",id:"orb-features",level:3},{value:"Installing ORB-SLAM3 with ROS 2",id:"installing-orb-slam3-with-ros-2",level:3},{value:"Running ORB-SLAM3",id:"running-orb-slam3",level:3},{value:"2. Visual Odometry",id:"2-visual-odometry",level:2},{value:"What is Visual Odometry?",id:"what-is-visual-odometry",level:3},{value:"Stereo Visual Odometry",id:"stereo-visual-odometry",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"3. Loop Closure",id:"3-loop-closure",level:2},{value:"What is Loop Closure?",id:"what-is-loop-closure",level:3},{value:"DBoW2 Place Recognition",id:"dbow2-place-recognition",level:3},{value:"Pose Graph Optimization",id:"pose-graph-optimization",level:3},{value:"Loop Closure Example",id:"loop-closure-example",level:3},{value:"4. Nav2 Integration",id:"4-nav2-integration",level:2},{value:"Connecting V-SLAM to Nav2",id:"connecting-v-slam-to-nav2",level:3},{value:"Converting V-SLAM Output to Nav2",id:"converting-v-slam-output-to-nav2",level:3},{value:"Launching V-SLAM + Nav2",id:"launching-v-slam--nav2",level:3},{value:"5. Mapping Strategies",id:"5-mapping-strategies",level:2},{value:"2D vs 3D Maps",id:"2d-vs-3d-maps",level:3},{value:"Octomap for 3D Mapping",id:"octomap-for-3d-mapping",level:3},{value:"RTAB-Map (Real-Time Appearance-Based Mapping)",id:"rtab-map-real-time-appearance-based-mapping",level:3},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Run ORB-SLAM3",id:"exercise-1-run-orb-slam3",level:3},{value:"Exercise 2: Visual Odometry Implementation",id:"exercise-2-visual-odometry-implementation",level:3},{value:"Exercise 3: V-SLAM + Nav2 Integration",id:"exercise-3-v-slam--nav2-integration",level:3},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"visual-slam-and-navigation",children:"Visual SLAM and Navigation"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand Visual SLAM algorithms (ORB-SLAM3, RTAB-Map)"}),"\n",(0,r.jsx)(n.li,{children:"Implement visual odometry for camera-based localization"}),"\n",(0,r.jsx)(n.li,{children:"Leverage loop closure for drift correction"}),"\n",(0,r.jsx)(n.li,{children:"Integrate Visual SLAM with ROS 2 Nav2 navigation stack"}),"\n",(0,r.jsx)(n.li,{children:"Apply different mapping strategies for autonomous navigation"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["LiDAR-based SLAM works great in structured environments, but what about robots that only have cameras? Indoor drones, handheld devices, and cost-sensitive robots rely entirely on vision. ",(0,r.jsx)(n.strong,{children:"How do we enable robots to navigate using only cameras, without expensive LiDAR sensors?"})]}),"\n",(0,r.jsxs)(n.p,{children:["This is where ",(0,r.jsx)(n.strong,{children:"Visual SLAM (V-SLAM)"})," becomes essential. V-SLAM uses cameras to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localize"}),": Estimate robot pose from visual features"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Map"}),": Build 3D maps of the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Navigate"}),": Plan paths through mapped spaces"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why Visual SLAM matters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Cost-effective"}),": Cameras are 10-100x cheaper than LiDAR"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Rich information"}),": Color, texture, object recognition"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Omnidirectional"}),": Fish-eye or 360\xb0 cameras see everywhere"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Versatile"}),": Works indoors, outdoors, underwater"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Challenges:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Computationally intensive (feature extraction, matching)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Sensitive to lighting changes"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Struggles with texture-less surfaces"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Scale ambiguity with monocular cameras"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Real-world applications:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AR/VR headsets"}),": Meta Quest, HoloLens use V-SLAM for tracking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Drones"}),": DJI drones use visual-inertial SLAM for indoor flight"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Warehouse robots"}),": Amazon robots combine V-SLAM with LiDAR"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autonomous cars"}),": Teslas use vision-based localization"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This chapter teaches you how to implement Visual SLAM and integrate it with ROS 2's Nav2 navigation stack."}),"\n",(0,r.jsx)(n.h2,{id:"1-vslam-algorithms-orb-slam",children:"1. VSLAM Algorithms (ORB-SLAM)"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-visual-slam",children:"What is Visual SLAM?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"})," estimates camera pose and builds a map simultaneously using:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature detection"}),": Find distinctive points in images"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature matching"}),": Match points across frames"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pose estimation"}),": Calculate camera movement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Map building"}),": Triangulate 3D positions of features"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop closure"}),": Detect revisited locations"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"orb-slam3",children:"ORB-SLAM3"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3"})," is the state-of-the-art open-source V-SLAM system supporting:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Monocular, stereo, RGB-D, and multi-camera setups"}),"\n",(0,r.jsx)(n.li,{children:"Visual-inertial SLAM (camera + IMU)"}),"\n",(0,r.jsx)(n.li,{children:"Real-time operation (30 FPS)"}),"\n",(0,r.jsx)(n.li,{children:"Loop closure and map optimization"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Pipeline:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Feature Extraction (ORB features)\n   \u2193\n2. Initial Pose Estimation (motion model)\n   \u2193\n3. Local Map Tracking\n   \u2193\n4. New Keyframe Decision\n   \u2193\n5. Local Mapping (bundle adjustment)\n   \u2193\n6. Loop Closure Detection\n   \u2193\n7. Global Pose Graph Optimization\n"})}),"\n",(0,r.jsx)(n.h3,{id:"orb-features",children:"ORB Features"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ORB (Oriented FAST and Rotated BRIEF)"})," features are:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fast to compute"}),": Real-time performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rotation invariant"}),": Works even if camera rotates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Binary descriptors"}),": Efficient matching"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import cv2\n\n# Detect ORB features in image\norb = cv2.ORB_create(nfeatures=1000)\nkeypoints, descriptors = orb.detectAndCompute(image, None)\n\n# Draw keypoints\nimage_with_keypoints = cv2.drawKeypoints(\n    image, keypoints, None, color=(0, 255, 0)\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"installing-orb-slam3-with-ros-2",children:"Installing ORB-SLAM3 with ROS 2"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Clone ORB-SLAM3\ncd ~/workspaces/ros2_ws/src\ngit clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git\n\n# Build ORB-SLAM3\ncd ORB_SLAM3\nchmod +x build.sh\n./build.sh\n\n# Clone ROS 2 wrapper\ncd ~/workspaces/ros2_ws/src\ngit clone https://github.com/zang09/ORB_SLAM3_ROS2.git\n\n# Build\ncd ~/workspaces/ros2_ws\ncolcon build --packages-select orb_slam3_ros2\n"})}),"\n",(0,r.jsx)(n.h3,{id:"running-orb-slam3",children:"Running ORB-SLAM3"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# RGB-D mode (with depth camera)\nros2 run orb_slam3_ros2 rgbd \\\n  --ros-args \\\n  -p voc_file:=/path/to/ORBvoc.txt \\\n  -p settings_file:=/path/to/RealSense_D435i.yaml \\\n  -r /camera/color/image_raw:=/camera/rgb \\\n  -r /camera/depth/image_raw:=/camera/depth\n\n# Stereo mode\nros2 run orb_slam3_ros2 stereo \\\n  --ros-args \\\n  -p voc_file:=/path/to/ORBvoc.txt \\\n  -p settings_file:=/path/to/Stereo.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Published topics:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/orb_slam3/camera_pose"})," (geometry_msgs/PoseStamped)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/orb_slam3/map_points"})," (sensor_msgs/PointCloud2)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/orb_slam3/tracking_state"})," (std_msgs/String)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"2-visual-odometry",children:"2. Visual Odometry"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-visual-odometry",children:"What is Visual Odometry?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual odometry (VO)"})," tracks camera movement between consecutive frames without building a global map."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Difference from SLAM:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual Odometry"}),": Local tracking only, no loop closure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"}),": Global map + loop closure"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"stereo-visual-odometry",children:"Stereo Visual Odometry"}),"\n",(0,r.jsx)(n.p,{children:"Stereo cameras provide depth directly, enabling accurate VO:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\nclass StereoVisualOdometry:\n    def __init__(self):\n        # Stereo matcher\n        self.stereo = cv2.StereoBM_create(numDisparities=16*6, blockSize=11)\n\n        # Feature detector\n        self.detector = cv2.ORB_create()\n\n        # Previous frame data\n        self.prev_keypoints = None\n        self.prev_descriptors = None\n        self.prev_points_3d = None\n\n    def process_frame(self, left_image, right_image, camera_matrix, baseline):\n        # Compute disparity\n        disparity = self.stereo.compute(left_image, right_image)\n\n        # Detect features in left image\n        keypoints, descriptors = self.detector.detectAndCompute(left_image, None)\n\n        # Compute 3D points from disparity\n        points_3d = self.triangulate_points(\n            keypoints, disparity, camera_matrix, baseline\n        )\n\n        # Match with previous frame\n        if self.prev_keypoints is not None:\n            matches = self.match_features(descriptors, self.prev_descriptors)\n\n            # Estimate transformation\n            R, t = self.estimate_motion(\n                self.prev_points_3d[matches[:, 0]],\n                points_3d[matches[:, 1]]\n            )\n\n            return R, t\n\n        # Store for next frame\n        self.prev_keypoints = keypoints\n        self.prev_descriptors = descriptors\n        self.prev_points_3d = points_3d\n\n        return np.eye(3), np.zeros(3)\n\n    def match_features(self, desc1, desc2):\n        # Brute-force matcher\n        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n        matches = bf.match(desc1, desc2)\n\n        # Sort by distance\n        matches = sorted(matches, key=lambda x: x.distance)\n\n        # Return indices\n        return np.array([[m.queryIdx, m.trainIdx] for m in matches[:100]])\n\n    def estimate_motion(self, points_3d_prev, points_3d_curr):\n        # Use RANSAC with PnP\n        success, rvec, tvec, inliers = cv2.solvePnPRansac(\n            points_3d_prev,\n            points_3d_curr,\n            camera_matrix,\n            None\n        )\n\n        R, _ = cv2.Rodrigues(rvec)\n        return R, tvec\n"})}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\n\nclass VisualOdometryNode(Node):\n    def __init__(self):\n        super().__init__('visual_odometry')\n\n        self.bridge = CvBridge()\n        self.vo = StereoVisualOdometry()\n\n        self.left_sub = self.create_subscription(\n            Image, '/camera/left/image_raw', self.left_callback, 10\n        )\n        self.right_sub = self.create_subscription(\n            Image, '/camera/right/image_raw', self.right_callback, 10\n        )\n\n        self.pose_pub = self.create_publisher(PoseStamped, '/vo/pose', 10)\n\n        self.cumulative_pose = np.eye(4)  # 4x4 transformation matrix\n\n    def process_stereo_pair(self, left_img, right_img):\n        R, t = self.vo.process_frame(left_img, right_img, K, baseline)\n\n        # Update cumulative pose\n        T = np.eye(4)\n        T[:3, :3] = R\n        T[:3, 3] = t.flatten()\n\n        self.cumulative_pose = self.cumulative_pose @ T\n\n        # Publish pose\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = 'odom'\n\n        # Extract position and orientation\n        pose_msg.pose.position.x = float(self.cumulative_pose[0, 3])\n        pose_msg.pose.position.y = float(self.cumulative_pose[1, 3])\n        pose_msg.pose.position.z = float(self.cumulative_pose[2, 3])\n\n        # Convert rotation matrix to quaternion\n        quat = self.rotation_matrix_to_quaternion(self.cumulative_pose[:3, :3])\n        pose_msg.pose.orientation.x = quat[0]\n        pose_msg.pose.orientation.y = quat[1]\n        pose_msg.pose.orientation.z = quat[2]\n        pose_msg.pose.orientation.w = quat[3]\n\n        self.pose_pub.publish(pose_msg)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"3-loop-closure",children:"3. Loop Closure"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-loop-closure",children:"What is Loop Closure?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Loop closure"})," detects when the robot revisits a previously mapped location and corrects accumulated drift."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why needed:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Visual odometry accumulates drift over time"}),"\n",(0,r.jsx)(n.li,{children:"Without loop closure, maps become increasingly inconsistent"}),"\n",(0,r.jsx)(n.li,{children:"Loop closure enforces global consistency"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"dbow2-place-recognition",children:"DBoW2 Place Recognition"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"DBoW2 (Bag of Words)"})," is a popular algorithm for detecting loop closures:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build vocabulary"}),': Cluster ORB descriptors into visual "words"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Image representation"}),": Each image = histogram of visual words"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Similarity search"}),": Compare current image to database"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop detection"}),": High similarity = loop closure"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"pose-graph-optimization",children:"Pose Graph Optimization"}),"\n",(0,r.jsx)(n.p,{children:"When a loop is detected:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Add loop closure constraint between poses\n   \u2193\n2. Build pose graph\n   Nodes = keyframe poses\n   Edges = odometry + loop closures\n   \u2193\n3. Optimize graph (g2o, Ceres)\n   Minimize reprojection error\n   \u2193\n4. Update all pose estimates\n   \u2193\n5. Rebuild map with corrected poses\n"})}),"\n",(0,r.jsx)(n.h3,{id:"loop-closure-example",children:"Loop Closure Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class LoopClosureDetector:\n    def __init__(self, vocabulary_file):\n        # Load pre-trained vocabulary\n        self.vocabulary = self.load_vocabulary(vocabulary_file)\n\n        # Database of keyframe descriptors\n        self.keyframe_database = []\n\n    def detect_loop(self, current_descriptors, current_pose, threshold=0.75):\n        # Compute BoW vector for current frame\n        current_bow = self.compute_bow_vector(current_descriptors)\n\n        # Search database for similar frames\n        best_match_score = 0\n        best_match_idx = -1\n\n        for idx, (bow_vec, pose) in enumerate(self.keyframe_database):\n            score = self.compute_similarity(current_bow, bow_vec)\n\n            if score &gt; best_match_score:\n                best_match_score = score\n                best_match_idx = idx\n\n        # Check if loop closure detected\n        if best_match_score &gt; threshold:\n            loop_pose = self.keyframe_database[best_match_idx][1]\n            return True, loop_pose\n\n        # Add current frame to database\n        self.keyframe_database.append((current_bow, current_pose))\n\n        return False, None\n\n    def optimize_pose_graph(self, loop_from, loop_to):\n        # Use g2o or Ceres to optimize\n        # Minimize error across all poses and loop closures\n        pass\n"})}),"\n",(0,r.jsx)(n.h2,{id:"4-nav2-integration",children:"4. Nav2 Integration"}),"\n",(0,r.jsx)(n.h3,{id:"connecting-v-slam-to-nav2",children:"Connecting V-SLAM to Nav2"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Nav2"})," (ROS 2 Navigation) requires:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Map"}),": 2D occupancy grid"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localization"}),": Robot pose in map frame"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TF transforms"}),": ",(0,r.jsx)(n.code,{children:"map"})," \u2192 ",(0,r.jsx)(n.code,{children:"odom"})," \u2192 ",(0,r.jsx)(n.code,{children:"base_link"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"converting-v-slam-output-to-nav2",children:"Converting V-SLAM Output to Nav2"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from nav_msgs.msg import OccupancyGrid\nimport numpy as np\n\nclass VSLAMToNav2Bridge(Node):\n    def __init__(self):\n        super().__init__('vslam_nav2_bridge')\n\n        # Subscribe to V-SLAM outputs\n        self.pose_sub = self.create_subscription(\n            PoseStamped, '/orb_slam3/camera_pose', self.pose_callback, 10\n        )\n        self.points_sub = self.create_subscription(\n            PointCloud2, '/orb_slam3/map_points', self.points_callback, 10\n        )\n\n        # Publish Nav2 inputs\n        self.map_pub = self.create_publisher(OccupancyGrid, '/map', 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n    def pose_callback(self, msg):\n        # Broadcast map \u2192 odom transform\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'map'\n        t.child_frame_id = 'odom'\n\n        # V-SLAM pose becomes map \u2192 base_link\n        # Compute map \u2192 odom by subtracting odom \u2192 base_link\n        t.transform.translation.x = msg.pose.position.x\n        t.transform.translation.y = msg.pose.position.y\n        t.transform.translation.z = msg.pose.position.z\n        t.transform.rotation = msg.pose.orientation\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def points_callback(self, msg):\n        # Convert 3D point cloud to 2D occupancy grid\n        occupancy_grid = self.project_points_to_grid(msg)\n        self.map_pub.publish(occupancy_grid)\n\n    def project_points_to_grid(self, point_cloud):\n        # Project 3D points to 2D grid\n        grid = OccupancyGrid()\n        grid.header.frame_id = 'map'\n        grid.info.resolution = 0.05  # 5 cm per cell\n        grid.info.width = 400  # 20m wide\n        grid.info.height = 400\n        grid.data = [0] * (400 * 400)  # Unknown\n\n        # Parse point cloud and mark occupied cells\n        for point in pc2.read_points(point_cloud):\n            x, y, z = point[:3]\n\n            # Project to grid\n            grid_x = int((x - grid.info.origin.position.x) / grid.info.resolution)\n            grid_y = int((y - grid.info.origin.position.y) / grid.info.resolution)\n\n            if 0 &lt;= grid_x &lt; grid.info.width and 0 &lt;= grid_y &lt; grid.info.height:\n                idx = grid_y * grid.info.width + grid_x\n                grid.data[idx] = 100  # Occupied\n\n        return grid\n"})}),"\n",(0,r.jsx)(n.h3,{id:"launching-v-slam--nav2",children:"Launching V-SLAM + Nav2"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# launch/vslam_nav2.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # ORB-SLAM3\n        Node(\n            package='orb_slam3_ros2',\n            executable='rgbd',\n            name='orb_slam3',\n            parameters=[{\n                'voc_file': '/path/to/ORBvoc.txt',\n                'settings_file': '/path/to/settings.yaml'\n            }]\n        ),\n\n        # V-SLAM to Nav2 bridge\n        Node(\n            package='my_package',\n            executable='vslam_nav2_bridge',\n            name='vslam_nav2_bridge'\n        ),\n\n        # Nav2\n        Node(\n            package='nav2_bringup',\n            executable='navigation_launch.py',\n            name='nav2'\n        )\n    ])\n"})}),"\n",(0,r.jsx)(n.h2,{id:"5-mapping-strategies",children:"5. Mapping Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"2d-vs-3d-maps",children:"2D vs 3D Maps"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Map Type"}),(0,r.jsx)(n.th,{children:"Use Case"}),(0,r.jsx)(n.th,{children:"Storage"}),(0,r.jsx)(n.th,{children:"Nav2 Compatible"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"2D Occupancy Grid"})}),(0,r.jsx)(n.td,{children:"Mobile robots"}),(0,r.jsx)(n.td,{children:"Efficient"}),(0,r.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"2.5D Height Map"})}),(0,r.jsx)(n.td,{children:"Rough terrain"}),(0,r.jsx)(n.td,{children:"Moderate"}),(0,r.jsx)(n.td,{children:"Partial"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"3D Voxel Map"})}),(0,r.jsx)(n.td,{children:"Drones, 3D planning"}),(0,r.jsx)(n.td,{children:"Large"}),(0,r.jsx)(n.td,{children:"\u274c No"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Point Cloud"})}),(0,r.jsx)(n.td,{children:"Visualization"}),(0,r.jsx)(n.td,{children:"Very large"}),(0,r.jsx)(n.td,{children:"\u274c No"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"octomap-for-3d-mapping",children:"Octomap for 3D Mapping"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install OctoMap\nsudo apt install ros-humble-octomap ros-humble-octomap-server\n\n# Run OctoMap server\nros2 run octomap_server octomap_server_node \\\n  --ros-args \\\n  -r cloud_in:=/camera/depth/points \\\n  -p resolution:=0.05 \\\n  -p frame_id:=map\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Published topics:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/octomap_binary"})," - Compressed 3D occupancy map"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/octomap_full"})," - Full 3D occupancy map"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"rtab-map-real-time-appearance-based-mapping",children:"RTAB-Map (Real-Time Appearance-Based Mapping)"}),"\n",(0,r.jsx)(n.p,{children:"RTAB-Map combines V-SLAM with powerful mapping:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install RTAB-Map\nsudo apt install ros-humble-rtabmap-ros\n\n# Launch RTAB-Map\nros2 launch rtabmap_ros rtabmap.launch.py \\\n  rgb_topic:=/camera/rgb \\\n  depth_topic:=/camera/depth \\\n  camera_info_topic:=/camera/camera_info \\\n  frame_id:=base_link \\\n  approx_sync:=true\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Long-term SLAM (handles large environments)"}),"\n",(0,r.jsx)(n.li,{children:"Loop closure out of the box"}),"\n",(0,r.jsx)(n.li,{children:"3D and 2D map outputs"}),"\n",(0,r.jsx)(n.li,{children:"Works with RGB-D, stereo, or LiDAR"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Key takeaways from this chapter:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"})," uses cameras for localization and mapping without LiDAR"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3"})," is the state-of-the-art V-SLAM framework (stereo, RGB-D, monocular)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual odometry"})," tracks camera motion frame-to-frame"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop closure"})," detects revisited locations and corrects drift"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Nav2 integration"})," requires converting 3D V-SLAM maps to 2D occupancy grids"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Best practices:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use stereo or RGB-D cameras (avoid monocular scale ambiguity)"}),"\n",(0,r.jsx)(n.li,{children:"Combine V-SLAM with IMU for robust tracking (visual-inertial SLAM)"}),"\n",(0,r.jsx)(n.li,{children:"Enable loop closure for long-term operation"}),"\n",(0,r.jsx)(n.li,{children:"Use RTAB-Map for production systems (more stable than ORB-SLAM3)"}),"\n",(0,r.jsx)(n.li,{children:"Fuse V-SLAM with wheel odometry using sensor fusion"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"What is the difference between Visual Odometry and Visual SLAM?"}),"\n",(0,r.jsx)(n.li,{children:"Why do stereo cameras provide better V-SLAM than monocular?"}),"\n",(0,r.jsx)(n.li,{children:"What problem does loop closure solve?"}),"\n",(0,r.jsx)(n.li,{children:"How do you convert 3D V-SLAM point clouds to 2D Nav2 maps?"}),"\n",(0,r.jsx)(n.li,{children:"What are ORB features and why are they used in V-SLAM?"}),"\n",(0,r.jsx)(n.li,{children:"When would you use RTAB-Map instead of ORB-SLAM3?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-run-orb-slam3",children:"Exercise 1: Run ORB-SLAM3"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Install ORB-SLAM3 and ROS 2 wrapper"}),"\n",(0,r.jsx)(n.li,{children:"Record a dataset with RGB-D camera"}),"\n",(0,r.jsx)(n.li,{children:"Run ORB-SLAM3 on recorded data"}),"\n",(0,r.jsx)(n.li,{children:"Visualize trajectory and map points in RViz"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-visual-odometry-implementation",children:"Exercise 2: Visual Odometry Implementation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Implement stereo visual odometry"}),"\n",(0,r.jsx)(n.li,{children:"Subscribe to stereo camera topics"}),"\n",(0,r.jsxs)(n.li,{children:["Publish estimated pose on ",(0,r.jsx)(n.code,{children:"/vo/pose"})]}),"\n",(0,r.jsx)(n.li,{children:"Compare against ground truth"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-v-slam--nav2-integration",children:"Exercise 3: V-SLAM + Nav2 Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Launch RTAB-Map with RGB-D camera"}),"\n",(0,r.jsx)(n.li,{children:"Convert RTAB-Map output to occupancy grid"}),"\n",(0,r.jsx)(n.li,{children:"Launch Nav2 navigation stack"}),"\n",(0,r.jsx)(n.li,{children:"Command robot to navigate autonomously"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2007.11898",children:"ORB-SLAM3 Paper"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://introlab.github.io/rtabmap/",children:"RTAB-Map Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/gaoxiang12/slambook-en",children:"Visual SLAM Tutorial"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/dorian3d/DBoW2",children:"DBoW2 Place Recognition"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-09/ch14-perception",children:"Perception with Isaac \u2192"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,r.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-08/ch12-isaac-ros",children:"\u2190 ROS 2 + Isaac Sim Integration"})]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>t});var i=s(6540);const r={},o=i.createContext(r);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);