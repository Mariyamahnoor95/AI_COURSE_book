"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1823],{1044:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-03-isaac/week-10/ch16-sensor-fusion","title":"Multi-Sensor Fusion","description":"Learning Objectives","source":"@site/docs/module-03-isaac/week-10/ch16-sensor-fusion.md","sourceDirName":"module-03-isaac/week-10","slug":"/module-03-isaac/week-10/ch16-sensor-fusion","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-10/ch16-sensor-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-isaac/week-10/ch16-sensor-fusion.md","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"id":"ch16-sensor-fusion","title":"Multi-Sensor Fusion","sidebar_label":"Multi-Sensor Fusion","sidebar_position":17},"sidebar":"textbookSidebar","previous":{"title":"Reinforcement Learning...","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-10/ch15-rl-sim2real"},"next":{"title":"Module 4: VLA Integration (Weeks 11-13)","permalink":"/AI_COURSE_book/docs/module-04-vla"}}');var r=i(4848),a=i(8453);const o={id:"ch16-sensor-fusion",title:"Multi-Sensor Fusion",sidebar_label:"Multi-Sensor Fusion",sidebar_position:17},t="Multi-Sensor Fusion",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Kalman Filter Fundamentals",id:"kalman-filter-fundamentals",level:2},{value:"The Kalman Filter Algorithm",id:"the-kalman-filter-algorithm",level:3},{value:"Extended Kalman Filter (EKF)",id:"extended-kalman-filter-ekf",level:3},{value:"Unscented Kalman Filter (UKF)",id:"unscented-kalman-filter-ukf",level:3},{value:"Robot Localization Package in ROS 2",id:"robot-localization-package-in-ros-2",level:2},{value:"ekf_localization_node Configuration",id:"ekf_localization_node-configuration",level:3},{value:"Tuning Covariance Parameters",id:"tuning-covariance-parameters",level:3},{value:"Launch File",id:"launch-file",level:3},{value:"Visual-Inertial Odometry (VIO)",id:"visual-inertial-odometry-vio",level:2},{value:"ORB-SLAM3 with IMU",id:"orb-slam3-with-imu",level:3},{value:"Isaac ROS Visual-Inertial SLAM",id:"isaac-ros-visual-inertial-slam",level:3},{value:"Multi-Sensor Localization Pipeline",id:"multi-sensor-localization-pipeline",level:2},{value:"Complete Sensor Fusion Architecture",id:"complete-sensor-fusion-architecture",level:3},{value:"Weighted Fusion Based on Uncertainty",id:"weighted-fusion-based-on-uncertainty",level:3},{value:"Detecting Sensor Failures",id:"detecting-sensor-failures",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: EKF Sensor Fusion",id:"exercise-1-ekf-sensor-fusion",level:3},{value:"Exercise 2: GPS + IMU Fusion",id:"exercise-2-gps--imu-fusion",level:3},{value:"Exercise 3: Visual-Inertial SLAM",id:"exercise-3-visual-inertial-slam",level:3},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fuse data from multiple sensors (camera, LiDAR, IMU, GPS, wheel odometry)"}),"\n",(0,r.jsx)(n.li,{children:"Implement Kalman filters (EKF, UKF) for state estimation"}),"\n",(0,r.jsx)(n.li,{children:"Use the robot_localization package for sensor fusion in ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"Build robust localization pipelines with fault detection"}),"\n",(0,r.jsx)(n.li,{children:"Understand uncertainty propagation and covariance tuning"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["No single sensor provides complete, reliable information for robot localization. Cameras suffer from motion blur and lighting changes; LiDAR can miss transparent surfaces and struggles in fog; wheel odometry drifts over time due to wheel slippage; GPS fails indoors and near buildings. ",(0,r.jsx)(n.strong,{children:"Multi-sensor fusion"})," combines measurements from complementary sensors to produce more accurate, robust state estimates than any individual sensor."]}),"\n",(0,r.jsx)(n.p,{children:"A typical mobile robot sensor suite includes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Wheel odometry"}),": High-frequency (50-100 Hz), low-drift over short distances, fails on slippery surfaces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU (Inertial Measurement Unit)"}),": High-frequency (100-500 Hz), measures acceleration and angular velocity, drifts over time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"}),": Medium-frequency (10-20 Hz), accurate range measurements, sensitive to dynamic obstacles"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera (visual odometry)"}),": Medium-frequency (10-30 Hz), rich feature information, affected by lighting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPS"}),": Low-frequency (1-10 Hz), absolute position outdoors, 2-5m accuracy (RTK GPS: 2-5cm)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The challenges of sensor fusion include:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Different update rates"}),": IMU @ 200 Hz, camera @ 30 Hz, GPS @ 5 Hz"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Different coordinate frames"}),": Each sensor has its own reference frame"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Measurement noise"}),": Each sensor has different noise characteristics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Outlier detection"}),": Sensors can produce incorrect measurements (GPS multipath, LiDAR reflections)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, we'll explore Kalman filtering\u2014the foundation of sensor fusion\u2014and implement production-ready multi-sensor localization pipelines using ROS 2."}),"\n",(0,r.jsx)(n.h2,{id:"kalman-filter-fundamentals",children:"Kalman Filter Fundamentals"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"Kalman filter"})," is an optimal state estimator that recursively combines ",(0,r.jsx)(n.strong,{children:"predictions"})," (from a motion model) with ",(0,r.jsx)(n.strong,{children:"measurements"})," (from sensors) to estimate the true state of a system."]}),"\n",(0,r.jsx)(n.h3,{id:"the-kalman-filter-algorithm",children:"The Kalman Filter Algorithm"}),"\n",(0,r.jsx)(n.p,{children:"At each time step, the filter performs two operations:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"1. Prediction Step"})," (using motion model):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"x\u0302_{k|k-1} = F_k * x\u0302_{k-1|k-1} + B_k * u_k\nP_{k|k-1} = F_k * P_{k-1|k-1} * F_k^T + Q_k\n"})}),"\n",(0,r.jsx)(n.p,{children:"where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"x\u0302"}),": state estimate (position, velocity, orientation)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"F"}),": state transition matrix (how state evolves)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"u"}),": control input (wheel velocities, motor commands)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"P"}),": estimation error covariance (uncertainty)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Q"}),": process noise covariance (model uncertainty)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"2. Update Step"})," (using sensor measurement):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"K_k = P_{k|k-1} * H_k^T * (H_k * P_{k|k-1} * H_k^T + R_k)^{-1}\nx\u0302_{k|k} = x\u0302_{k|k-1} + K_k * (z_k - H_k * x\u0302_{k|k-1})\nP_{k|k} = (I - K_k * H_k) * P_{k|k-1}\n"})}),"\n",(0,r.jsx)(n.p,{children:"where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"z"}),": sensor measurement (GPS position, LiDAR range)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"H"}),": measurement matrix (maps state to measurement space)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"R"}),": measurement noise covariance (sensor uncertainty)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"K"}),": Kalman gain (optimal weighting of prediction vs measurement)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"Kalman gain"})," balances trust between the prediction and measurement:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["If ",(0,r.jsx)(n.strong,{children:"P"})," (prediction uncertainty) is high \u2192 ",(0,r.jsx)(n.strong,{children:"K"})," is large \u2192 trust measurement more"]}),"\n",(0,r.jsxs)(n.li,{children:["If ",(0,r.jsx)(n.strong,{children:"R"})," (measurement uncertainty) is high \u2192 ",(0,r.jsx)(n.strong,{children:"K"})," is small \u2192 trust prediction more"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"extended-kalman-filter-ekf",children:"Extended Kalman Filter (EKF)"}),"\n",(0,r.jsxs)(n.p,{children:["For ",(0,r.jsx)(n.strong,{children:"nonlinear"})," systems (most robots), we use the ",(0,r.jsx)(n.strong,{children:"Extended Kalman Filter"}),", which linearizes the motion and measurement models:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Nonlinear motion model: x_{k+1} = f(x_k, u_k)\n# EKF linearizes: F_k = \u2202f/\u2202x |_{x\u0302_k}\n\ndef ekf_predict(x, P, u, dt, Q):\n    """\n    EKF prediction step for differential drive robot\n\n    State: x = [x, y, theta, v, omega]^T\n    Control: u = [v_cmd, omega_cmd]^T\n    """\n    # Nonlinear motion model\n    x_pred = x.copy()\n    x_pred[0] += x[3] * np.cos(x[2]) * dt  # x += v*cos(theta)*dt\n    x_pred[1] += x[3] * np.sin(x[2]) * dt  # y += v*sin(theta)*dt\n    x_pred[2] += x[4] * dt  # theta += omega*dt\n    x_pred[3] = u[0]  # v = v_cmd\n    x_pred[4] = u[1]  # omega = omega_cmd\n\n    # Jacobian of motion model\n    F = np.eye(5)\n    F[0, 2] = -x[3] * np.sin(x[2]) * dt  # \u2202x/\u2202theta\n    F[0, 3] = np.cos(x[2]) * dt          # \u2202x/\u2202v\n    F[1, 2] = x[3] * np.cos(x[2]) * dt   # \u2202y/\u2202theta\n    F[1, 3] = np.sin(x[2]) * dt          # \u2202y/\u2202v\n    F[2, 4] = dt                         # \u2202theta/\u2202omega\n\n    # Covariance prediction\n    P_pred = F @ P @ F.T + Q\n\n    return x_pred, P_pred\n'})}),"\n",(0,r.jsx)(n.h3,{id:"unscented-kalman-filter-ukf",children:"Unscented Kalman Filter (UKF)"}),"\n",(0,r.jsxs)(n.p,{children:["For highly nonlinear systems, the ",(0,r.jsx)(n.strong,{children:"Unscented Kalman Filter"})," uses ",(0,r.jsx)(n.strong,{children:"sigma points"})," to approximate the state distribution, avoiding linearization errors:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def ukf_predict(x, P, f, Q, alpha=0.001, beta=2, kappa=0):\n    """\n    UKF prediction using unscented transform\n\n    Args:\n        x: state mean (n,)\n        P: state covariance (n, n)\n        f: nonlinear state transition function\n        Q: process noise covariance (n, n)\n    """\n    n = len(x)\n    lambda_ = alpha**2 * (n + kappa) - n\n\n    # Generate sigma points\n    sigma_points = np.zeros((2*n + 1, n))\n    sigma_points[0] = x\n\n    sqrt_P = np.linalg.cholesky((n + lambda_) * P)\n    for i in range(n):\n        sigma_points[i+1] = x + sqrt_P[i]\n        sigma_points[n+i+1] = x - sqrt_P[i]\n\n    # Propagate sigma points through nonlinear function\n    sigma_points_prop = np.array([f(sp) for sp in sigma_points])\n\n    # Compute mean and covariance\n    weights_m = np.full(2*n+1, 1/(2*(n+lambda_)))\n    weights_m[0] = lambda_ / (n + lambda_)\n    weights_c = weights_m.copy()\n    weights_c[0] += (1 - alpha**2 + beta)\n\n    x_pred = sigma_points_prop.T @ weights_m\n    P_pred = np.zeros((n, n))\n    for i in range(2*n+1):\n        diff = sigma_points_prop[i] - x_pred\n        P_pred += weights_c[i] * np.outer(diff, diff)\n\n    P_pred += Q\n\n    return x_pred, P_pred\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to use EKF vs UKF"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"EKF"}),": Mildly nonlinear systems, computational efficiency critical (mobile robots, drones)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"UKF"}),": Highly nonlinear systems, better accuracy needed (manipulators, underwater vehicles)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"robot-localization-package-in-ros-2",children:"Robot Localization Package in ROS 2"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"robot_localization"})," package provides production-ready implementations of EKF and UKF for fusing odometry, IMU, and GPS data in ROS 2."]}),"\n",(0,r.jsx)(n.h3,{id:"ekf_localization_node-configuration",children:"ekf_localization_node Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Example configuration for fusing wheel odometry + IMU:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# File: config/ekf.yaml\n\nekf_localization_node:\n  ros__parameters:\n    frequency: 30  # Hz (state estimation rate)\n    sensor_timeout: 0.1  # seconds\n    two_d_mode: true  # Ignore z, roll, pitch for 2D robots\n\n    # Frame IDs\n    map_frame: 'map'\n    odom_frame: 'odom'\n    base_link_frame: 'base_link'\n    world_frame: 'odom'  # Can be 'odom' or 'map'\n\n    # Odometry source 0 (wheel odometry)\n    odom0: '/wheel_odom'\n    odom0_config: [\n      false, false, false,  # x, y, z (position)\n      false, false, false,  # roll, pitch, yaw (orientation)\n      true,  true,  false,  # vx, vy, vz (linear velocity)\n      false, false, true,   # vroll, vpitch, vyaw (angular velocity)\n      false, false, false   # ax, ay, az (linear acceleration)\n    ]\n    odom0_queue_size: 10\n    odom0_differential: false  # Use absolute measurements\n    odom0_relative: false\n\n    # IMU source 0\n    imu0: '/imu/data'\n    imu0_config: [\n      false, false, false,  # x, y, z\n      true,  true,  true,   # roll, pitch, yaw (orientation)\n      false, false, false,  # vx, vy, vz\n      true,  true,  true,   # vroll, vpitch, vyaw (angular velocity)\n      true,  true,  false   # ax, ay, az (linear acceleration)\n    ]\n    imu0_differential: false\n    imu0_remove_gravitational_acceleration: true\n\n    # Process noise covariance (Q matrix)\n    process_noise_covariance: [\n      0.05, 0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,\n      0.0,  0.05, 0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,\n      0.0,  0.0,  0.06, 0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,\n      # ... (15x15 matrix)\n    ]\n\n    # Initial estimate covariance (P_0 matrix)\n    initial_estimate_covariance: [\n      1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n      # ... (15x15 matrix, larger values = more uncertainty)\n    ]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tuning-covariance-parameters",children:"Tuning Covariance Parameters"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Process noise covariance (Q)"}),": Represents uncertainty in the motion model"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large Q"}),": Trust measurements more (filter tracks sensor noise)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Small Q"}),": Trust model more (filter is smooth but may lag)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tuning"}),": Start with diagonal values ~0.05, increase for axes with high model error"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Measurement covariance (R)"}),": Defined in sensor messages (",(0,r.jsx)(n.code,{children:"Imu.angular_velocity_covariance"}),", ",(0,r.jsx)(n.code,{children:"Odometry.pose.covariance"}),")"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Set based on sensor specs (e.g., IMU gyro noise density: 0.001 rad/s/\u221aHz)"}),"\n",(0,r.jsx)(n.li,{children:"If unknown, start with conservative values (e.g., 0.1 for orientation, 0.01 for velocity)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"launch-file",children:"Launch File"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: launch/localization.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='robot_localization',\n            executable='ekf_node',\n            name='ekf_localization',\n            output='screen',\n            parameters=[\n                {'use_sim_time': False},\n                'config/ekf.yaml'\n            ],\n            remappings=[\n                ('/odometry/filtered', '/odom')\n            ]\n        )\n    ])\n"})}),"\n",(0,r.jsx)(n.h2,{id:"visual-inertial-odometry-vio",children:"Visual-Inertial Odometry (VIO)"}),"\n",(0,r.jsxs)(n.p,{children:["Fusing camera visual features with IMU accelerometer/gyroscope measurements provides ",(0,r.jsx)(n.strong,{children:"visual-inertial odometry"})," (VIO), which is more robust than pure visual odometry."]}),"\n",(0,r.jsx)(n.h3,{id:"orb-slam3-with-imu",children:"ORB-SLAM3 with IMU"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3"})," is a state-of-the-art SLAM system supporting monocular, stereo, RGB-D, and monocular-inertial modes:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# File: config/orb_slam3_imu.yaml\n\nORB_SLAM3:\n  # Camera calibration\n  Camera.fx: 615.0\n  Camera.fy: 615.0\n  Camera.cx: 320.0\n  Camera.cy: 240.0\n\n  # IMU parameters\n  IMU.NoiseGyro: 0.001  # rad/s/\u221aHz (gyroscope noise)\n  IMU.NoiseAcc: 0.01    # m/s\xb2/\u221aHz (accelerometer noise)\n  IMU.GyroWalk: 0.0001  # rad/s\xb2/\u221aHz (gyro random walk)\n  IMU.AccWalk: 0.001    # m/s\xb3/\u221aHz (accel random walk)\n  IMU.Frequency: 200    # Hz\n\n  # Visualization\n  Viewer.KeyFrameSize: 0.05\n  Viewer.CameraSize: 0.08\n  Viewer.PointSize: 2\n"})}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-visual-inertial-slam",children:"Isaac ROS Visual-Inertial SLAM"}),"\n",(0,r.jsxs)(n.p,{children:["For GPU-accelerated VIO, use ",(0,r.jsx)(n.strong,{children:"Isaac ROS visual_slam"})," with IMU fusion:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='isaac_ros_visual_slam',\n            name='visual_slam',\n            parameters=[{\n                'enable_imu': True,\n                'enable_slam_visualization': True,\n                'num_cameras': 1,  # Monocular\n                'rectified_images': True,\n                'enable_localization_n_mapping': True\n            }],\n            remappings=[\n                ('visual_slam/image_0', '/camera/image_raw'),\n                ('visual_slam/camera_info_0', '/camera/camera_info'),\n                ('visual_slam/imu', '/imu/data')\n            ]\n        )\n    ])\n"})}),"\n",(0,r.jsx)(n.p,{children:"VIO advantages:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No drift in orientation"})," (IMU provides absolute gravity reference)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robust to rapid motion"})," (IMU fills gaps when visual tracking fails)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Faster initialization"})," (IMU provides scale estimate for monocular cameras)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"multi-sensor-localization-pipeline",children:"Multi-Sensor Localization Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"For production systems, fuse all available sensors: wheel odometry, IMU, LiDAR SLAM, and GPS."}),"\n",(0,r.jsx)(n.h3,{id:"complete-sensor-fusion-architecture",children:"Complete Sensor Fusion Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# File: config/ekf_complete.yaml\n\nekf_localization_node:\n  ros__parameters:\n    frequency: 50\n\n    # Wheel odometry (high frequency, drifts)\n    odom0: '/wheel_odom'\n    odom0_config: [false, false, false,\n                   false, false, false,\n                   true, true, false,  # vx, vy\n                   false, false, true,  # vyaw\n                   false, false, false]\n\n    # IMU (orientation and angular velocity)\n    imu0: '/imu/data'\n    imu0_config: [false, false, false,\n                  true, true, true,  # roll, pitch, yaw\n                  false, false, false,\n                  true, true, true,  # angular velocities\n                  false, false, false]\n    imu0_remove_gravitational_acceleration: true\n\n    # LiDAR odometry (accurate but medium frequency)\n    odom1: '/lidar_odom'\n    odom1_config: [true, true, false,  # x, y (absolute position)\n                   false, false, true,  # yaw\n                   false, false, false,\n                   false, false, false,\n                   false, false, false]\n\n    # GPS (outdoor only, low frequency)\n    odom2: '/gps/odom'\n    odom2_config: [true, true, false,  # x, y (absolute position from GPS)\n                   false, false, false,\n                   false, false, false,\n                   false, false, false,\n                   false, false, false]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"weighted-fusion-based-on-uncertainty",children:"Weighted Fusion Based on Uncertainty"}),"\n",(0,r.jsx)(n.p,{children:"The robot_localization package automatically weights sensors based on their covariance. To manually adjust trust:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\nimport numpy as np\n\nclass AdaptiveCovarianceNode(Node):\n    """Adjust sensor covariance based on operating conditions"""\n\n    def __init__(self):\n        super().__init__(\'adaptive_covariance\')\n\n        self.wheel_odom_sub = self.create_subscription(\n            Odometry, \'/wheel_odom_raw\', self.odom_callback, 10\n        )\n        self.wheel_odom_pub = self.create_publisher(\n            Odometry, \'/wheel_odom\', 10\n        )\n\n    def odom_callback(self, msg):\n        """Increase wheel odometry uncertainty on slippery surfaces"""\n\n        # Detect wheel slip (compare commanded vs actual velocity)\n        velocity_error = abs(self.cmd_vel - msg.twist.twist.linear.x)\n\n        if velocity_error > 0.2:  # Slipping\n            # Increase uncertainty (less trust in wheel odometry)\n            inflation_factor = 10.0\n        else:\n            inflation_factor = 1.0\n\n        # Modify covariance\n        msg.pose.covariance[0] *= inflation_factor  # x uncertainty\n        msg.pose.covariance[7] *= inflation_factor  # y uncertainty\n\n        self.wheel_odom_pub.publish(msg)\n\ndef main():\n    rclpy.init()\n    node = AdaptiveCovarianceNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"detecting-sensor-failures",children:"Detecting Sensor Failures"}),"\n",(0,r.jsx)(n.p,{children:"Implement outlier detection and automatic sensor switching:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class SensorHealthMonitor(Node):\n    """Monitor sensor health and disable faulty sensors"""\n\n    def __init__(self):\n        super().__init__(\'sensor_health_monitor\')\n\n        self.gps_timeout = 2.0  # seconds\n        self.last_gps_time = self.get_clock().now()\n\n        self.gps_sub = self.create_subscription(\n            Odometry, \'/gps/odom\', self.gps_callback, 10\n        )\n\n        self.create_timer(0.5, self.health_check)\n\n    def gps_callback(self, msg):\n        self.last_gps_time = self.get_clock().now()\n\n        # Check for GPS outliers (sudden jumps > 10m)\n        if hasattr(self, \'last_gps_pos\'):\n            dx = msg.pose.pose.position.x - self.last_gps_pos[0]\n            dy = msg.pose.pose.position.y - self.last_gps_pos[1]\n            distance = np.sqrt(dx**2 + dy**2)\n\n            if distance > 10.0:\n                self.get_logger().warn(f\'GPS outlier detected: {distance:.1f}m jump\')\n                return  # Reject measurement\n\n        self.last_gps_pos = (msg.pose.pose.position.x, msg.pose.pose.position.y)\n\n    def health_check(self):\n        """Check for sensor timeouts"""\n        time_since_gps = (self.get_clock().now() - self.last_gps_time).nanoseconds / 1e9\n\n        if time_since_gps > self.gps_timeout:\n            self.get_logger().error(\'GPS timeout - indoors or signal lost\')\n            # Disable GPS in robot_localization config dynamically\n'})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Calibrate sensor extrinsics"}),": Measure precise transforms between sensor frames"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tune covariances systematically"}),": Start conservative, then reduce based on real data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Log raw sensor data"}),": Replay for offline tuning and debugging"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validate fusion accuracy"}),": Compare fused estimate to ground truth (motion capture, RTK GPS)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement watchdogs"}),": Detect and handle sensor failures gracefully"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Key takeaways from this chapter:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-sensor fusion"})," combines complementary sensors (wheel odometry, IMU, LiDAR, camera, GPS) for robust localization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kalman filters"})," (EKF, UKF) optimally fuse predictions and measurements by balancing uncertainty"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"robot_localization"})," provides production-ready sensor fusion for ROS 2 with flexible configuration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual-inertial odometry"})," (VIO) fuses camera and IMU for drift-free orientation and scale"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Weighted fusion"})," based on covariance allows automatic trust adjustment, while outlier detection ensures robustness"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These techniques form the foundation of reliable state estimation for Physical AI systems operating in complex, uncertain environments."}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Why is multi-sensor fusion necessary for mobile robots? Provide three specific failure modes of individual sensors."}),"\n",(0,r.jsx)(n.li,{children:"Explain the prediction and update steps of the Kalman filter. What is the role of the Kalman gain?"}),"\n",(0,r.jsx)(n.li,{children:"What is the key difference between EKF and UKF? When would you choose UKF over EKF?"}),"\n",(0,r.jsx)(n.li,{children:"How does the robot_localization package weight different sensors? How can you adjust trust in a specific sensor?"}),"\n",(0,r.jsx)(n.li,{children:"What are the advantages of visual-inertial odometry (VIO) over pure visual odometry?"}),"\n",(0,r.jsx)(n.li,{children:"Describe how you would detect and handle a GPS outlier measurement in real-time."}),"\n",(0,r.jsx)(n.li,{children:"How would you tune the process noise covariance (Q) and measurement noise covariance (R) for a new robot platform?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-ekf-sensor-fusion",children:"Exercise 1: EKF Sensor Fusion"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Configure robot_localization to fuse wheel odometry and IMU."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up a differential drive robot in Gazebo with wheel encoders and IMU"}),"\n",(0,r.jsx)(n.li,{children:"Create ekf.yaml configuration file"}),"\n",(0,r.jsx)(n.li,{children:"Launch ekf_localization_node"}),"\n",(0,r.jsx)(n.li,{children:"Drive the robot in a square path"}),"\n",(0,r.jsxs)(n.li,{children:["Compare fused odometry ",(0,r.jsx)(n.code,{children:"/odometry/filtered"})," to ground truth"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": Fused estimate has <5% position error after 10m traveled."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-gps--imu-fusion",children:"Exercise 2: GPS + IMU Fusion"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Integrate GPS for outdoor localization."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Add GPS sensor to robot URDF"}),"\n",(0,r.jsx)(n.li,{children:"Configure robot_localization with GPS as odom2"}),"\n",(0,r.jsx)(n.li,{children:"Drive robot outdoors and record GPS + IMU + wheel odometry"}),"\n",(0,r.jsx)(n.li,{children:"Compare fused estimate to RTK GPS ground truth"}),"\n",(0,r.jsx)(n.li,{children:"Test GPS outlier rejection by injecting fake measurements"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": Fused estimate maintains <2m error even with GPS outages."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-visual-inertial-slam",children:"Exercise 3: Visual-Inertial SLAM"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Deploy Isaac ROS visual_slam with IMU fusion."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Mount a camera and IMU on robot with calibrated extrinsics"}),"\n",(0,r.jsx)(n.li,{children:"Launch Isaac ROS visual_slam with IMU enabled"}),"\n",(0,r.jsx)(n.li,{children:"Drive robot through indoor environment"}),"\n",(0,r.jsx)(n.li,{children:"Measure drift after loop closure"}),"\n",(0,r.jsx)(n.li,{children:"Compare to wheel odometry drift"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": VIO drift <1% of distance traveled, significantly better than wheel odometry."]}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Probabilistic Robotics"})," (Thrun, Burgard, Fox): Chapters on Kalman filtering and sensor fusion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"robot_localization Documentation"}),": ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/humble/p/robot_localization/",children:"http://docs.ros.org/en/humble/p/robot_localization/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3 Paper"}),': "ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM" (2021)']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kalman Filter Tutorial"}),": ",(0,r.jsx)(n.a,{href:"https://www.kalmanfilter.net/",children:"https://www.kalmanfilter.net/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": ",(0,r.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_visual_slam/",children:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_visual_slam/"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous"}),": ",(0,r.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-10/ch15-rl-sim2real",children:"Chapter 15 - Reinforcement Learning and Sim-to-Real"}),"\n",(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-04-vla/week-11/ch17-humanoid-urdf",children:"Chapter 17 - Humanoid Robot URDF and Kinematics"})]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);