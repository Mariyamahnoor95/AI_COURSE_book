"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[2514],{4338:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-03-isaac/week-08/ch12-isaac-ros","title":"Isaac ROS Perception","description":"Learning Objectives","source":"@site/docs/module-03-isaac/week-08/ch12-isaac-ros.md","sourceDirName":"module-03-isaac/week-08","slug":"/module-03-isaac/week-08/ch12-isaac-ros","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-08/ch12-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-isaac/week-08/ch12-isaac-ros.md","tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"id":"ch12-isaac-ros","title":"Isaac ROS Perception","sidebar_label":"Isaac ROS Perception","sidebar_position":13},"sidebar":"textbookSidebar","previous":{"title":"NVIDIA Isaac Sim","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-08/ch11-isaac-sim"},"next":{"title":"Visual SLAM and Navigation","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-09/ch13-vslam-nav2"}}');var t=s(4848),r=s(8453);const a={id:"ch12-isaac-ros",title:"Isaac ROS Perception",sidebar_label:"Isaac ROS Perception",sidebar_position:13},o="Isaac ROS Perception",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Isaac ROS Architecture",id:"isaac-ros-architecture",level:2},{value:"Available Isaac ROS Packages",id:"available-isaac-ros-packages",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Docker-Based Installation",id:"docker-based-installation",level:3},{value:"Native Installation on Jetson",id:"native-installation-on-jetson",level:3},{value:"Verifying Installation",id:"verifying-installation",level:3},{value:"Visual SLAM with Isaac ROS",id:"visual-slam-with-isaac-ros",level:2},{value:"Features and Configuration",id:"features-and-configuration",level:3},{value:"Integration with Nav2",id:"integration-with-nav2",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:3},{value:"Object Detection with Isaac ROS",id:"object-detection-with-isaac-ros",level:2},{value:"Using Pre-Trained Models",id:"using-pre-trained-models",level:3},{value:"Launch File for DetectNet",id:"launch-file-for-detectnet",level:3},{value:"Custom Model Training",id:"custom-model-training",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Isaac ROS Visual SLAM",id:"exercise-1-isaac-ros-visual-slam",level:3},{value:"Exercise 2: Object Detection for Navigation",id:"exercise-2-object-detection-for-navigation",level:3},{value:"Exercise 3: Custom Object Detection",id:"exercise-3-custom-object-detection",level:3},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"isaac-ros-perception",children:"Isaac ROS Perception"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand Isaac ROS architecture and GPU-accelerated packages"}),"\n",(0,t.jsx)(n.li,{children:"Install and configure Isaac ROS on Ubuntu or NVIDIA Jetson platforms"}),"\n",(0,t.jsx)(n.li,{children:"Integrate Isaac ROS with existing ROS 2 systems"}),"\n",(0,t.jsx)(n.li,{children:"Use Isaac ROS for visual SLAM and object detection"}),"\n",(0,t.jsx)(n.li,{children:"Optimize perception pipelines with GPU acceleration"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS"})," is NVIDIA's collection of hardware-accelerated ROS 2 packages that dramatically improve the performance of perception and navigation tasks. While ",(0,t.jsx)(n.strong,{children:"Isaac Sim"})," (covered in Chapter 11) is a simulation environment, ",(0,t.jsx)(n.strong,{children:"Isaac ROS"})," is a set of production-ready ROS 2 nodes that run on real robots, leveraging GPU acceleration to achieve 10-100x speedup compared to CPU-only implementations."]}),"\n",(0,t.jsx)(n.p,{children:"For Physical AI systems, real-time perception is critical. Autonomous mobile robots must process camera streams, LiDAR point clouds, and sensor fusion at high frame rates (30+ FPS) while simultaneously running navigation, localization, and control algorithms. Traditional CPU-based perception pipelines struggle to meet these latency requirements, especially on edge devices. Isaac ROS solves this problem by offloading compute-intensive tasks\u2014such as deep neural network inference, image processing, and SLAM\u2014to the GPU, enabling real-time performance on platforms like the NVIDIA Jetson Orin."}),"\n",(0,t.jsx)(n.p,{children:"Common use cases for Isaac ROS include visual odometry for mobile robots, object detection for warehouse automation, semantic segmentation for outdoor navigation, and AprilTag detection for precision docking. In this chapter, we'll explore the Isaac ROS architecture, set up the development environment, and build GPU-accelerated perception pipelines for ROS 2 robots."}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-architecture",children:"Isaac ROS Architecture"}),"\n",(0,t.jsxs)(n.p,{children:["Isaac ROS packages are built on a ",(0,t.jsx)(n.strong,{children:"hardware acceleration layer"})," that consists of three key components:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CUDA"}),": NVIDIA's parallel computing platform for GPU acceleration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TensorRT"}),": High-performance deep learning inference optimizer"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Triton Inference Server"}),": Multi-framework model deployment platform"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["These components are abstracted through ",(0,t.jsx)(n.strong,{children:"GXF (Graph Execution Framework)"}),", NVIDIA's graph-based pipeline framework. GXF pipelines are integrated into ROS 2 using ",(0,t.jsx)(n.strong,{children:"composition"})," (rclcpp components), allowing Isaac ROS nodes to appear as standard ROS 2 nodes while internally leveraging GPU acceleration."]}),"\n",(0,t.jsx)(n.h3,{id:"available-isaac-ros-packages",children:"Available Isaac ROS Packages"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS provides a comprehensive suite of perception and navigation packages:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Package"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Speedup (vs CPU)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"isaac_ros_visual_slam"})}),(0,t.jsx)(n.td,{children:"GPU-accelerated visual SLAM (stereo/monocular)"}),(0,t.jsx)(n.td,{children:"30-50x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"isaac_ros_image_segmentation"})}),(0,t.jsx)(n.td,{children:"Semantic segmentation (UNET, PeopleSemSegNet)"}),(0,t.jsx)(n.td,{children:"15-25x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"isaac_ros_object_detection"})}),(0,t.jsx)(n.td,{children:"Object detection (DetectNet, DOPE 6D pose)"}),(0,t.jsx)(n.td,{children:"10-20x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"isaac_ros_depth_estimation"})}),(0,t.jsx)(n.td,{children:"Stereo depth estimation (SGM, ESS models)"}),(0,t.jsx)(n.td,{children:"40-60x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"isaac_ros_apriltag"})}),(0,t.jsx)(n.td,{children:"AprilTag fiducial marker detection"}),(0,t.jsx)(n.td,{children:"8-12x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"isaac_ros_nvblox"})}),(0,t.jsx)(n.td,{children:"3D scene reconstruction and mapping"}),(0,t.jsx)(n.td,{children:"20-30x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"isaac_ros_dnn_inference"})}),(0,t.jsx)(n.td,{children:"TensorRT-accelerated DNN inference"}),(0,t.jsx)(n.td,{children:"10-15x"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"All packages follow ROS 2 best practices, supporting composition, lifecycle management, and QoS (Quality of Service) policies. The performance gains are achieved through:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-copy memory sharing"})," between GPU nodes (avoiding CPU-GPU transfers)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TensorRT optimization"})," (layer fusion, precision calibration, kernel auto-tuning)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Batched processing"})," for high-throughput scenarios"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimized CUDA kernels"})," for image preprocessing and postprocessing"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,t.jsxs)(n.p,{children:["Isaac ROS nodes integrate seamlessly with standard ROS 2 ecosystems. For example, ",(0,t.jsx)(n.code,{children:"isaac_ros_visual_slam"})," publishes odometry on ",(0,t.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})," (compatible with Nav2), while ",(0,t.jsx)(n.code,{children:"isaac_ros_detectnet"})," publishes ",(0,t.jsx)(n.code,{children:"vision_msgs/Detection2DArray"})," messages that any ROS 2 node can consume."]}),"\n",(0,t.jsx)(n.h2,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,t.jsxs)(n.p,{children:["Isaac ROS supports two platforms: ",(0,t.jsx)(n.strong,{children:"x86_64 workstations"})," with NVIDIA GPUs and ",(0,t.jsx)(n.strong,{children:"NVIDIA Jetson"})," (Orin, Xavier, Nano). The recommended installation method is ",(0,t.jsx)(n.strong,{children:"Docker-based"}),", as it bundles all dependencies (CUDA, TensorRT, GXF) and ensures compatibility."]}),"\n",(0,t.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"x86_64"}),": Ubuntu 22.04, NVIDIA GPU (GTX 1060+ or better), CUDA 11.8+, 16GB+ RAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Jetson"}),": JetPack 6.0+ (includes CUDA, TensorRT pre-installed)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Software"}),": Docker, NVIDIA Container Toolkit, ROS 2 Humble"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"docker-based-installation",children:"Docker-Based Installation"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA provides a pre-configured Docker image with all Isaac ROS dependencies:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Clone Isaac ROS common repository\ncd ~/workspaces\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\n\n# Launch development container\ncd isaac_ros_common\n./scripts/run_dev.sh\n\n# Inside container, clone desired Isaac ROS packages\ncd /workspaces/isaac_ros-dev/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_object_detection.git\n\n# Build workspace\ncd /workspaces/isaac_ros-dev\ncolcon build --symlink-install\nsource install/setup.bash\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"run_dev.sh"})," script automatically:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Mounts your workspace into ",(0,t.jsx)(n.code,{children:"/workspaces/isaac_ros-dev"})]}),"\n",(0,t.jsxs)(n.li,{children:["Configures GPU access with ",(0,t.jsx)(n.code,{children:"--gpus all"})]}),"\n",(0,t.jsx)(n.li,{children:"Sets up X11 forwarding for visualization (RViz, image_view)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"native-installation-on-jetson",children:"Native Installation on Jetson"}),"\n",(0,t.jsx)(n.p,{children:"For Jetson platforms, you can install Isaac ROS natively (recommended for production deployments):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Install dependencies\nsudo apt-get install -y ros-humble-isaac-ros-common\n\n# Add Isaac ROS APT repository\necho "deb https://isaac.download.nvidia.com/isaac-ros/release-3 $(lsb_release -cs) main" | \\\n  sudo tee /etc/apt/sources.list.d/isaac-ros.list\n\n# Install desired packages\nsudo apt-get update\nsudo apt-get install -y ros-humble-isaac-ros-visual-slam\n'})}),"\n",(0,t.jsx)(n.h3,{id:"verifying-installation",children:"Verifying Installation"}),"\n",(0,t.jsx)(n.p,{children:"Test GPU access and Isaac ROS installation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Verify GPU is accessible\nnvidia-smi\n\n# Test Isaac ROS example\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If the launch file starts without errors and you see GPU utilization in ",(0,t.jsx)(n.code,{children:"nvidia-smi"}),", the installation is successful."]}),"\n",(0,t.jsx)(n.h2,{id:"visual-slam-with-isaac-ros",children:"Visual SLAM with Isaac ROS"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Visual SLAM"})," (Simultaneous Localization and Mapping) is a core capability for autonomous robots. The ",(0,t.jsx)(n.code,{children:"isaac_ros_visual_slam"})," package implements GPU-accelerated stereo and monocular visual odometry based on NVIDIA's cuVSLAM library."]}),"\n",(0,t.jsx)(n.h3,{id:"features-and-configuration",children:"Features and Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Key features:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stereo and monocular support"})," (stereo provides better scale estimation)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Loop closure detection"})," for drift correction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration with Nav2"})," via ",(0,t.jsx)(n.code,{children:"nav_msgs/Odometry"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time performance"}),": 60+ FPS on Jetson Orin"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Configuration parameters (in ",(0,t.jsx)(n.code,{children:"visual_slam_node.yaml"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"visual_slam_node:\n  ros__parameters:\n    num_cameras: 2  # 1 for monocular, 2 for stereo\n    min_num_images: 4  # Minimum images for initialization\n    enable_imu: true  # Use IMU for improved accuracy\n    enable_localization_n_mapping: true\n    enable_slam_visualization: true\n    enable_landmarks_view: true\n    enable_observations_view: true\n    map_frame: 'map'\n    odom_frame: 'odom'\n    base_frame: 'base_link'\n    camera_optical_frames: ['left_cam', 'right_cam']\n"})}),"\n",(0,t.jsx)(n.h3,{id:"integration-with-nav2",children:"Integration with Nav2"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS visual SLAM publishes odometry that Nav2's AMCL can fuse with wheel odometry:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\n\nclass VisualOdomMonitor(Node):\n    \"\"\"Monitor visual SLAM odometry quality\"\"\"\n    def __init__(self):\n        super().__init__('visual_odom_monitor')\n        self.subscription = self.create_subscription(\n            Odometry,\n            '/visual_slam/tracking/odometry',\n            self.odom_callback,\n            10\n        )\n\n    def odom_callback(self, msg):\n        # Extract pose covariance (lower = better tracking)\n        cov = msg.pose.covariance\n        position_uncertainty = (cov[0] + cov[7] + cov[14]) / 3.0\n\n        if position_uncertainty > 0.5:\n            self.get_logger().warn(\n                f'High tracking uncertainty: {position_uncertainty:.3f}m'\n            )\n        else:\n            self.get_logger().info(\n                f'Tracking quality: {position_uncertainty:.4f}m'\n            )\n\ndef main():\n    rclpy.init()\n    node = VisualOdomMonitor()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,t.jsx)(n.p,{children:"Comparison on NVIDIA Jetson Orin (stereo 640x480 @ 30 FPS):"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric"}),(0,t.jsx)(n.th,{children:"CPU (ORB-SLAM3)"}),(0,t.jsx)(n.th,{children:"GPU (Isaac ROS)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Frame rate"}),(0,t.jsx)(n.td,{children:"8 FPS"}),(0,t.jsx)(n.td,{children:"60 FPS"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Latency"}),(0,t.jsx)(n.td,{children:"125ms"}),(0,t.jsx)(n.td,{children:"16ms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Power"}),(0,t.jsx)(n.td,{children:"15W"}),(0,t.jsx)(n.td,{children:"10W (GPU more efficient)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Initialization time"}),(0,t.jsx)(n.td,{children:"5-10s"}),(0,t.jsx)(n.td,{children:"1-2s"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:["The GPU-accelerated pipeline achieves ",(0,t.jsx)(n.strong,{children:"7.5x higher frame rate"})," with ",(0,t.jsx)(n.strong,{children:"8x lower latency"}),", making it suitable for high-speed navigation tasks."]}),"\n",(0,t.jsx)(n.h2,{id:"object-detection-with-isaac-ros",children:"Object Detection with Isaac ROS"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"isaac_ros_object_detection"})," package provides GPU-accelerated object detection using NVIDIA's ",(0,t.jsx)(n.strong,{children:"DetectNet"})," architecture, optimized with TensorRT."]}),"\n",(0,t.jsx)(n.h3,{id:"using-pre-trained-models",children:"Using Pre-Trained Models"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS includes pre-trained models for common robotics scenarios:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom vision_msgs.msg import Detection2DArray\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\n\nclass ObjectDetectionDemo(Node):\n    \"\"\"Subscribe to Isaac ROS DetectNet detections\"\"\"\n    def __init__(self):\n        super().__init__('object_detection_demo')\n\n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            '/detections_output',\n            self.detection_callback,\n            10\n        )\n\n        self.bridge = CvBridge()\n\n    def detection_callback(self, msg):\n        self.get_logger().info(\n            f'Detected {len(msg.detections)} objects'\n        )\n\n        for detection in msg.detections:\n            # Extract bounding box\n            bbox = detection.bbox\n            x_center = bbox.center.position.x\n            y_center = bbox.center.position.y\n            width = bbox.size_x\n            height = bbox.size_y\n\n            # Get class label and confidence\n            if detection.results:\n                hypothesis = detection.results[0]\n                label = hypothesis.hypothesis.class_id\n                score = hypothesis.hypothesis.score\n\n                self.get_logger().info(\n                    f'Object: {label}, Score: {score:.2f}, '\n                    f'BBox: ({x_center:.0f}, {y_center:.0f}, '\n                    f'{width:.0f}x{height:.0f})'\n                )\n\ndef main():\n    rclpy.init()\n    node = ObjectDetectionDemo()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"launch-file-for-detectnet",children:"Launch File for DetectNet"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='isaac_ros_detectnet',\n            executable='isaac_ros_detectnet',\n            name='detectnet_node',\n            parameters=[{\n                'network_image_width': 640,\n                'network_image_height': 480,\n                'model_name': 'peoplenet',  # Pre-trained for person detection\n                'confidence_threshold': 0.5,\n                'class_labels_file': '/workspaces/isaac_ros-dev/src/models/peoplenet_labels.txt'\n            }],\n            remappings=[\n                ('image', '/camera/color/image_raw'),\n                ('detections_output', '/detections')\n            ]\n        )\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"custom-model-training",children:"Custom Model Training"}),"\n",(0,t.jsx)(n.p,{children:"For domain-specific objects, you can train custom DetectNet models using NVIDIA TAO Toolkit:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collect training data"})," (images + bounding box annotations in KITTI format)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Train DetectNet model"})," with TAO Toolkit (transfer learning from pre-trained backbone)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Export to TensorRT"})," (optimized ",(0,t.jsx)(n.code,{children:".engine"})," file)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy in Isaac ROS"})," by updating ",(0,t.jsx)(n.code,{children:"model_name"})," parameter"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Typical training time: 2-4 hours on RTX 3090 for 10K images."}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Docker for development"}),": Ensures consistent CUDA/TensorRT versions across team"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tune QoS policies"}),": Use ",(0,t.jsx)(n.code,{children:"BEST_EFFORT"})," for high-throughput sensors, ",(0,t.jsx)(n.code,{children:"RELIABLE"})," for critical commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitor GPU memory"}),": Use ",(0,t.jsx)(n.code,{children:"nvidia-smi"})," to check VRAM usage; reduce batch size if OOM errors occur"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Profile pipelines"}),": Use ",(0,t.jsx)(n.code,{children:"ros2 topic hz"})," and ",(0,t.jsx)(n.code,{children:"ros2 topic bw"})," to identify bottlenecks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Leverage zero-copy"}),": Keep entire pipeline on GPU (avoid CPU nodes in critical path)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Key takeaways from this chapter:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS provides GPU-accelerated ROS 2 packages"})," for perception and navigation, achieving 10-100x speedup over CPU implementations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Docker-based installation"})," simplifies setup on both x86_64 and Jetson platforms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"isaac_ros_visual_slam"})," enables real-time visual odometry at 60+ FPS, integrating seamlessly with Nav2"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"isaac_ros_detectnet"})," provides TensorRT-optimized object detection with support for custom models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-copy GPU pipelines"})," minimize latency and maximize throughput for real-time robotics"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These capabilities are essential for Physical AI systems that require real-time perception in dynamic environments."}),"\n",(0,t.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"What are the three key components of the Isaac ROS hardware acceleration layer, and what role does each play?"}),"\n",(0,t.jsx)(n.li,{children:"Why does Isaac ROS achieve significantly better power efficiency than CPU-based perception on Jetson platforms?"}),"\n",(0,t.jsxs)(n.li,{children:["How does ",(0,t.jsx)(n.code,{children:"isaac_ros_visual_slam"})," integrate with the Nav2 navigation stack?"]}),"\n",(0,t.jsx)(n.li,{children:"What is the purpose of TensorRT optimization in Isaac ROS DNN inference nodes?"}),"\n",(0,t.jsx)(n.li,{children:"Describe a scenario where you would choose monocular visual SLAM over stereo visual SLAM, and vice versa."}),"\n",(0,t.jsx)(n.li,{children:"How would you debug high latency in an Isaac ROS perception pipeline?"}),"\n",(0,t.jsx)(n.li,{children:"What are the trade-offs between using pre-trained DetectNet models vs. training custom models?"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-isaac-ros-visual-slam",children:"Exercise 1: Isaac ROS Visual SLAM"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Set up GPU-accelerated visual SLAM and compare performance with CPU-based ORB-SLAM3."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Install Isaac ROS visual SLAM using Docker"}),"\n",(0,t.jsxs)(n.li,{children:["Record a rosbag with stereo camera data (",(0,t.jsx)(n.code,{children:"/left/image_raw"}),", ",(0,t.jsx)(n.code,{children:"/right/image_raw"}),")"]}),"\n",(0,t.jsx)(n.li,{children:"Run Isaac ROS visual SLAM on the rosbag and measure frame rate"}),"\n",(0,t.jsx)(n.li,{children:"Run ORB-SLAM3 on the same rosbag and compare results"}),"\n",(0,t.jsx)(n.li,{children:"Visualize trajectories in RViz"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Outcome"}),": Isaac ROS achieves 5-10x higher frame rate with lower latency."]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-object-detection-for-navigation",children:"Exercise 2: Object Detection for Navigation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Use Isaac ROS DetectNet to detect people and publish dynamic obstacles to Nav2's costmap."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Launch Isaac ROS DetectNet with ",(0,t.jsx)(n.code,{children:"peoplenet"})," model"]}),"\n",(0,t.jsxs)(n.li,{children:["Create a ROS 2 node that subscribes to ",(0,t.jsx)(n.code,{children:"/detections"})]}),"\n",(0,t.jsxs)(n.li,{children:["For each person detection, publish a ",(0,t.jsx)(n.code,{children:"CostmapUpdate"})," message to add a dynamic obstacle"]}),"\n",(0,t.jsx)(n.li,{children:"Launch Nav2 and observe the robot avoiding detected people"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Outcome"}),": Robot successfully navigates around dynamically detected people."]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-custom-object-detection",children:"Exercise 3: Custom Object Detection"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Train a custom DetectNet model for warehouse objects (boxes, pallets)."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Collect 500+ images of boxes and pallets in your environment"}),"\n",(0,t.jsx)(n.li,{children:"Annotate bounding boxes using LabelImg or CVAT"}),"\n",(0,t.jsx)(n.li,{children:"Train DetectNet model using NVIDIA TAO Toolkit"}),"\n",(0,t.jsx)(n.li,{children:"Export to TensorRT and deploy in Isaac ROS"}),"\n",(0,t.jsx)(n.li,{children:"Test detection accuracy in real-time"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Outcome"}),": Custom model achieves >90% mAP on validation set."]}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Documentation"}),": ",(0,t.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"https://nvidia-isaac-ros.github.io/"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"cuVSLAM Technical Report"}),": NVIDIA's GPU-accelerated SLAM algorithm design"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TensorRT Developer Guide"}),": ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/",children:"https://docs.nvidia.com/deeplearning/tensorrt/"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"NVIDIA TAO Toolkit"}),": ",(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/tao-toolkit",children:"https://developer.nvidia.com/tao-toolkit"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS GitHub"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"https://github.com/NVIDIA-ISAAC-ROS"})]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Previous"}),": ",(0,t.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-08/ch11-isaac-sim",children:"Chapter 11 - Isaac Sim Fundamentals"}),"\n",(0,t.jsx)(n.strong,{children:"Next"}),": ",(0,t.jsx)(n.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-09/ch13-vslam-nav2",children:"Chapter 13 - Visual SLAM and Nav2"})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var i=s(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);