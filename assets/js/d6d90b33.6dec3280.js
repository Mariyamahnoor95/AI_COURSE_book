"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3476],{6438:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-03-isaac/week-10/ch15-rl-sim2real","title":"Reinforcement Learning and Sim-to-Real","description":"Learning Objectives","source":"@site/docs/module-03-isaac/week-10/ch15-rl-sim2real.md","sourceDirName":"module-03-isaac/week-10","slug":"/module-03-isaac/week-10/ch15-rl-sim2real","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-10/ch15-rl-sim2real","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-isaac/week-10/ch15-rl-sim2real.md","tags":[],"version":"current","sidebarPosition":16,"frontMatter":{"id":"ch15-rl-sim2real","title":"Reinforcement Learning and Sim-to-Real","sidebar_label":"Reinforcement Learning...","sidebar_position":16},"sidebar":"textbookSidebar","previous":{"title":"Perception Pipelines","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-09/ch14-perception"},"next":{"title":"Multi-Sensor Fusion","permalink":"/AI_COURSE_book/docs/module-03-isaac/week-10/ch16-sensor-fusion"}}');var r=i(4848),o=i(8453);const a={id:"ch15-rl-sim2real",title:"Reinforcement Learning and Sim-to-Real",sidebar_label:"Reinforcement Learning...",sidebar_position:16},t="Reinforcement Learning and Sim-to-Real",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Reinforcement Learning Fundamentals",id:"reinforcement-learning-fundamentals",level:2},{value:"Policy Gradient Methods",id:"policy-gradient-methods",level:3},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Soft Actor-Critic (SAC)",id:"soft-actor-critic-sac",level:3},{value:"Reward Shaping",id:"reward-shaping",level:3},{value:"Training in Isaac Sim",id:"training-in-isaac-sim",level:2},{value:"IsaacGymEnvs Architecture",id:"isaacgymenvs-architecture",level:3},{value:"Training Script",id:"training-script",level:3},{value:"Monitoring Training",id:"monitoring-training",level:3},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Physical Randomization",id:"physical-randomization",level:3},{value:"Visual Randomization",id:"visual-randomization",level:3},{value:"Sensor Noise Injection",id:"sensor-noise-injection",level:3},{value:"Why Randomization Works",id:"why-randomization-works",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"Exporting the Policy",id:"exporting-the-policy",level:3},{value:"Running Policy on Real Robot",id:"running-policy-on-real-robot",level:3},{value:"Fine-Tuning with Real Data",id:"fine-tuning-with-real-data",level:3},{value:"Common Failure Modes",id:"common-failure-modes",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Train Navigation Policy",id:"exercise-1-train-navigation-policy",level:3},{value:"Exercise 2: Apply Domain Randomization",id:"exercise-2-apply-domain-randomization",level:3},{value:"Exercise 3: Deploy Policy on Real Robot",id:"exercise-3-deploy-policy-on-real-robot",level:3},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"reinforcement-learning-and-sim-to-real",children:"Reinforcement Learning and Sim-to-Real"})}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understand reinforcement learning fundamentals (PPO, SAC algorithms)"}),"\n",(0,r.jsx)(e.li,{children:"Train robot policies in Isaac Sim using IsaacGymEnvs"}),"\n",(0,r.jsx)(e.li,{children:"Apply domain randomization for sim-to-real transfer"}),"\n",(0,r.jsx)(e.li,{children:"Deploy trained policies to real robots"}),"\n",(0,r.jsx)(e.li,{children:"Analyze and bridge the reality gap"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(e.p,{children:["The ",(0,r.jsx)(e.strong,{children:"sim-to-real problem"})," is one of the grand challenges in robotics: policies trained in simulation often fail catastrophically when deployed to real robots. This ",(0,r.jsx)(e.strong,{children:"reality gap"})," arises from differences in physics (friction, contact dynamics), sensors (noise, latency), and environment (lighting, textures). For example, a quadruped robot that walks perfectly in Isaac Sim might fall immediately on real hardware due to unmodeled actuator delays or ground compliance."]}),"\n",(0,r.jsxs)(e.p,{children:["Reinforcement learning (RL) is particularly dependent on simulation because learning requires millions of environment interactions\u2014far too many to collect on real hardware. A single RL training run for quadruped locomotion might require 100 million steps, equivalent to months of continuous real-world operation. Simulation enables ",(0,r.jsx)(e.strong,{children:"massively parallel training"})," (thousands of robots in parallel), ",(0,r.jsx)(e.strong,{children:"safe exploration"})," (no hardware damage), and ",(0,r.jsx)(e.strong,{children:"rapid iteration"})," (reset to any state instantly)."]}),"\n",(0,r.jsx)(e.p,{children:"Successful sim-to-real examples demonstrate what's possible: Boston Dynamics' quadrupeds learned agile locomotion in simulation before real-world deployment, OpenAI's Dactyl robot learned in-hand manipulation with domain randomization, and Agility Robotics' Digit humanoid uses RL-trained policies for dynamic walking. In this chapter, we'll explore the foundations of RL, train policies in Isaac Sim, and apply proven techniques for sim-to-real transfer."}),"\n",(0,r.jsx)(e.h2,{id:"reinforcement-learning-fundamentals",children:"Reinforcement Learning Fundamentals"}),"\n",(0,r.jsxs)(e.p,{children:["Reinforcement learning frames robot control as a ",(0,r.jsx)(e.strong,{children:"Markov Decision Process (MDP)"}),", defined by:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"State (s)"}),": Current robot configuration and environment observations (e.g., joint positions, velocities, LiDAR scans)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Action (a)"}),": Control commands (e.g., joint torques, velocities)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reward (r)"}),": Scalar feedback signal (+1 for reaching goal, -0.1 for falling)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Transition (s' ~ P(s'|s,a))"}),": Next state after taking action in current state"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["The goal is to learn a ",(0,r.jsx)(e.strong,{children:"policy \u03c0(a|s)"})," that maximizes cumulative reward: ",(0,r.jsx)(e.strong,{children:"E[\u03a3 \u03b3^t r_t]"}),", where \u03b3 is the discount factor."]}),"\n",(0,r.jsx)(e.h3,{id:"policy-gradient-methods",children:"Policy Gradient Methods"}),"\n",(0,r.jsxs)(e.p,{children:["Modern RL for robotics uses ",(0,r.jsx)(e.strong,{children:"policy gradient"})," methods, which directly optimize the policy parameters \u03b8:"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u2207_\u03b8 J(\u03b8) = E[ \u03a3 \u2207_\u03b8 log \u03c0_\u03b8(a|s) * A(s,a) ]"})}),"\n",(0,r.jsxs)(e.p,{children:["where ",(0,r.jsx)(e.strong,{children:"A(s,a)"})," is the ",(0,r.jsx)(e.strong,{children:"advantage function"})," (how much better action a is than average)."]}),"\n",(0,r.jsx)(e.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"PPO"})," (Schulman et al., 2017) is the most popular RL algorithm for robotics, offering:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sample efficiency"}),": Reuses experience via importance sampling"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Stability"}),": Clips policy updates to prevent large destructive changes"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simplicity"}),": Easy to implement and tune"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"PPO update rule:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"L(\u03b8) = E[ min( r_t(\u03b8) * A_t,  clip(r_t(\u03b8), 1-\u03b5, 1+\u03b5) * A_t ) ]\n"})}),"\n",(0,r.jsxs)(e.p,{children:["where ",(0,r.jsx)(e.strong,{children:"r_t(\u03b8) = \u03c0_\u03b8(a|s) / \u03c0_old(a|s)"})," is the importance ratio, clipped to [1-\u03b5, 1+\u03b5] (typically \u03b5=0.2)."]}),"\n",(0,r.jsx)(e.h3,{id:"soft-actor-critic-sac",children:"Soft Actor-Critic (SAC)"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"SAC"})," (Haarnoja et al., 2018) is an off-policy algorithm that maximizes both reward and entropy:"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"J(\u03b8) = E[ \u03a3 (r_t + \u03b1 * H(\u03c0(\xb7|s_t))) ]"})}),"\n",(0,r.jsx)(e.p,{children:"SAC advantages:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sample efficient"}),": Learns from replay buffer (off-policy)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Robust"}),": Entropy term encourages exploration"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Continuous control"}),": Designed for continuous action spaces"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"SAC is preferred for manipulation tasks where sample efficiency is critical."}),"\n",(0,r.jsx)(e.h3,{id:"reward-shaping",children:"Reward Shaping"}),"\n",(0,r.jsx)(e.p,{children:"Designing reward functions is the art of RL. For quadruped walking:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"def compute_reward(self, obs, actions):\n    \"\"\"\n    Reward function for quadruped locomotion\n\n    Args:\n        obs: dict with 'base_lin_vel', 'base_ang_vel', 'joint_pos', etc.\n        actions: commanded joint positions\n    \"\"\"\n    # Target: walk forward at 1 m/s\n    target_velocity = 1.0\n    velocity_error = torch.abs(obs['base_lin_vel'][:, 0] - target_velocity)\n    velocity_reward = torch.exp(-2.0 * velocity_error)  # Smooth reward\n\n    # Penalty for falling (base height < threshold)\n    fallen = obs['base_height'] < 0.3\n    fall_penalty = -10.0 * fallen.float()\n\n    # Penalty for large joint accelerations (smooth motion)\n    joint_accel = (actions - self.prev_actions) / self.dt\n    smoothness_penalty = -0.01 * torch.sum(joint_accel ** 2, dim=-1)\n\n    # Penalty for energy consumption\n    power = torch.sum(obs['joint_vel'] * obs['joint_torque'], dim=-1)\n    energy_penalty = -0.001 * torch.abs(power)\n\n    total_reward = (\n        velocity_reward +\n        fall_penalty +\n        smoothness_penalty +\n        energy_penalty\n    )\n\n    return total_reward\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Key principles"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Dense rewards"}),": Provide feedback at every step (not just terminal)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Shaped rewards"}),": Use smooth functions (exp, gaussian) instead of thresholds"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Penalty balancing"}),": Tune weights to balance competing objectives"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Normalization"}),": Keep rewards in [-1, 1] range for stable learning"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"training-in-isaac-sim",children:"Training in Isaac Sim"}),"\n",(0,r.jsxs)(e.p,{children:["NVIDIA's ",(0,r.jsx)(e.strong,{children:"IsaacGymEnvs"})," provides a framework for massively parallel RL training in Isaac Sim, capable of simulating ",(0,r.jsx)(e.strong,{children:"thousands of robots simultaneously"})," on a single GPU."]}),"\n",(0,r.jsx)(e.h3,{id:"isaacgymenvs-architecture",children:"IsaacGymEnvs Architecture"}),"\n",(0,r.jsx)(e.p,{children:"Key components:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"VecEnv"}),": Vectorized environment running N robots in parallel"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Observation/Action spaces"}),": Defined in ",(0,r.jsx)(e.code,{children:"Gym"})," interface"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reward computation"}),": Custom reward function per task"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reset logic"}),": Randomize initial states and environment parameters"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Example training configuration for quadruped:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# File: isaacgymenvs/tasks/anymal.py\n\nclass Anymal(VecTask):\n    """Anymal quadruped locomotion task"""\n\n    def __init__(self, cfg, sim_device, graphics_device_id, headless):\n        self.cfg = cfg\n\n        # Define observation space (48-dim: joint states, base velocity, etc.)\n        self.num_obs = 48\n        # Define action space (12-dim: 12 joint position targets)\n        self.num_actions = 12\n\n        super().__init__(\n            config=self.cfg,\n            sim_device=sim_device,\n            graphics_device_id=graphics_device_id,\n            headless=headless\n        )\n\n        # Create environments (4096 parallel robots)\n        self.num_envs = self.cfg["env"]["numEnvs"]  # e.g., 4096\n\n    def compute_observations(self):\n        """Return current state observations"""\n        self.obs_buf[:, :12] = self.dof_pos  # Joint positions\n        self.obs_buf[:, 12:24] = self.dof_vel  # Joint velocities\n        self.obs_buf[:, 24:27] = self.base_lin_vel  # Base linear velocity\n        self.obs_buf[:, 27:30] = self.base_ang_vel  # Base angular velocity\n        # ... more observations\n\n        return self.obs_buf\n\n    def compute_reward(self):\n        """Compute reward for each environment"""\n        # Use reward function from previous section\n        self.rew_buf[:] = self._compute_locomotion_reward()\n\n    def reset_idx(self, env_ids):\n        """Reset specified environments"""\n        # Randomize initial joint positions\n        self.dof_pos[env_ids] = torch_rand_float(\n            self.dof_lower_limits, self.dof_upper_limits\n        )\n        # Set base position and orientation\n        self.root_states[env_ids, :3] = self.initial_root_pos\n        # ...\n'})}),"\n",(0,r.jsx)(e.h3,{id:"training-script",children:"Training Script"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# File: train.py\nimport isaacgymenvs\nfrom rl_games.torch_runner import Runner\n\n# Load configuration\ncfg = {\n    "task": {\n        "name": "Anymal",\n        "physics_engine": "physx",\n        "num_envs": 4096,\n        "num_observations": 48,\n        "num_actions": 12\n    },\n    "train": {\n        "params": {\n            "algo": {\n                "name": "a2c_continuous"  # PPO variant\n            },\n            "network": {\n                "name": "actor_critic",\n                "separate": False,\n                "mlp": {\n                    "units": [256, 128, 64],  # Hidden layers\n                    "activation": "elu"\n                }\n            },\n            "config": {\n                "learning_rate": 3e-4,\n                "gamma": 0.99,\n                "tau": 0.95,  # GAE parameter\n                "entropy_coef": 0.01,\n                "num_actors": 4096,\n                "horizon_length": 24,  # Steps per rollout\n                "minibatch_size": 16384,\n                "num_minibatches": 4,\n                "epochs_num": 5,\n                "max_epochs": 10000\n            }\n        }\n    }\n}\n\n# Create environment\nenvs = isaacgymenvs.make(\n    seed=cfg["seed"],\n    task=cfg["task"]["name"],\n    num_envs=cfg["task"]["num_envs"],\n    sim_device="cuda:0",\n    rl_device="cuda:0"\n)\n\n# Train\nrunner = Runner()\nrunner.load(cfg)\nrunner.reset()\nrunner.run({\n    "train": True,\n    "play": False,\n    "checkpoint": None\n})\n'})}),"\n",(0,r.jsx)(e.h3,{id:"monitoring-training",children:"Monitoring Training"}),"\n",(0,r.jsxs)(e.p,{children:["Track training progress with ",(0,r.jsx)(e.strong,{children:"TensorBoard"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"tensorboard --logdir runs/Anymal_PPO\n"})}),"\n",(0,r.jsx)(e.p,{children:"Key metrics:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reward"}),": Should increase over time (e.g., -5 \u2192 +10)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Episode length"}),": Should increase as robot learns to walk longer"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Policy loss"}),": Should decrease and stabilize"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Value loss"}),": Should decrease and converge"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Training time"}),": 2-6 hours on RTX 4090 for 10M steps."]}),"\n",(0,r.jsx)(e.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Domain randomization"})," (DR) is the key technique for sim-to-real transfer: by training on a ",(0,r.jsx)(e.strong,{children:"distribution of simulations"})," (varying physics, visuals, sensors), the policy learns to be robust to the specific reality it encounters."]}),"\n",(0,r.jsx)(e.h3,{id:"physical-randomization",children:"Physical Randomization"}),"\n",(0,r.jsx)(e.p,{children:"Randomize physics parameters at each episode reset:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def randomize_physics(self, env_ids):\n    """Randomize physical properties for specified environments"""\n\n    # Mass randomization (\xb120%)\n    for body_id in self.rigid_body_ids:\n        mass = self.default_mass[body_id]\n        rand_mass = mass * torch_rand_float(0.8, 1.2, (len(env_ids),))\n        self.gym.set_rigid_body_mass(env_ids, body_id, rand_mass)\n\n    # Friction randomization (0.5 to 1.5)\n    rand_friction = torch_rand_float(0.5, 1.5, (len(env_ids),))\n    self.gym.set_actor_friction(env_ids, rand_friction)\n\n    # Joint damping randomization (\xb130%)\n    for joint_id in range(self.num_dof):\n        damping = self.default_damping[joint_id]\n        rand_damping = damping * torch_rand_float(0.7, 1.3, (len(env_ids),))\n        self.gym.set_dof_damping(env_ids, joint_id, rand_damping)\n\n    # Actuator strength randomization (\xb115%)\n    for joint_id in range(self.num_dof):\n        max_force = self.default_max_force[joint_id]\n        rand_force = max_force * torch_rand_float(0.85, 1.15, (len(env_ids),))\n        self.gym.set_dof_max_force(env_ids, joint_id, rand_force)\n'})}),"\n",(0,r.jsx)(e.h3,{id:"visual-randomization",children:"Visual Randomization"}),"\n",(0,r.jsx)(e.p,{children:"Randomize visual appearance to prevent overfitting to specific textures/colors:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def randomize_visuals(self, env_ids):\n    """Randomize visual properties"""\n\n    # Ground texture randomization\n    texture_files = [\n        "textures/concrete.jpg",\n        "textures/wood.jpg",\n        "textures/carpet.jpg"\n    ]\n    texture_id = np.random.choice(len(texture_files))\n    self.gym.set_actor_texture(env_ids, texture_id)\n\n    # Lighting randomization\n    rand_intensity = torch_rand_float(0.5, 1.5, (1,))\n    self.gym.set_light_intensity(env_ids, rand_intensity)\n\n    # Robot color randomization (HSV jitter)\n    for link_id in self.robot_link_ids:\n        rand_color = torch_rand_float(0.0, 1.0, (3,))  # RGB\n        self.gym.set_rigid_body_color(env_ids, link_id, rand_color)\n'})}),"\n",(0,r.jsx)(e.h3,{id:"sensor-noise-injection",children:"Sensor Noise Injection"}),"\n",(0,r.jsx)(e.p,{children:"Add realistic sensor noise:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def add_sensor_noise(self, obs):\n    """Add sensor noise to observations"""\n\n    # IMU noise (gyroscope drift, accelerometer noise)\n    imu_noise = torch.randn_like(obs[:, :6]) * 0.05  # 5% noise\n    obs[:, :6] += imu_noise\n\n    # Joint encoder noise (0.01 rad position, 0.1 rad/s velocity)\n    joint_pos_noise = torch.randn_like(obs[:, 6:18]) * 0.01\n    joint_vel_noise = torch.randn_like(obs[:, 18:30]) * 0.1\n    obs[:, 6:18] += joint_pos_noise\n    obs[:, 18:30] += joint_vel_noise\n\n    # Latency simulation (delay observations by 1-2 timesteps)\n    if np.random.rand() < 0.3:\n        obs = self.obs_buffer_prev  # Use previous observation\n\n    return obs\n'})}),"\n",(0,r.jsx)(e.h3,{id:"why-randomization-works",children:"Why Randomization Works"}),"\n",(0,r.jsxs)(e.p,{children:["DR works because the policy learns an ",(0,r.jsx)(e.strong,{children:"ensemble of skills"})," that work across the randomized distribution. When deployed to reality, the real world is likely within this distribution, so the policy generalizes. Empirical studies show DR improves sim-to-real success rates from 20-30% to 70-90%."]}),"\n",(0,r.jsx)(e.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,r.jsx)(e.h3,{id:"exporting-the-policy",children:"Exporting the Policy"}),"\n",(0,r.jsx)(e.p,{children:"After training, export the policy for deployment:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Export policy as TorchScript (for fast inference)\nimport torch\n\npolicy = runner.get_policy()\npolicy.eval()\n\n# Trace the model\nexample_input = torch.randn(1, 48)  # num_obs\ntraced_policy = torch.jit.trace(policy, example_input)\n\n# Save to file\ntraced_policy.save("anymal_policy.pt")\n'})}),"\n",(0,r.jsx)(e.h3,{id:"running-policy-on-real-robot",children:"Running Policy on Real Robot"}),"\n",(0,r.jsx)(e.p,{children:"Integrate the policy into a ROS 2 node:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Float64MultiArray\nimport torch\nimport numpy as np\n\nclass RLPolicyNode(Node):\n    """Deploy RL policy on real robot"""\n\n    def __init__(self):\n        super().__init__(\'rl_policy_node\')\n\n        # Load TorchScript policy\n        self.policy = torch.jit.load("anymal_policy.pt")\n        self.policy.eval()\n\n        # ROS interfaces\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.state_callback, 10\n        )\n        self.cmd_pub = self.create_publisher(\n            Float64MultiArray, \'/joint_position_commands\', 10\n        )\n\n        self.obs = np.zeros(48)\n        self.create_timer(0.02, self.control_loop)  # 50 Hz\n\n    def state_callback(self, msg):\n        """Update observations from robot state"""\n        self.obs[:12] = msg.position  # Joint positions\n        self.obs[12:24] = msg.velocity  # Joint velocities\n        # Update other obs from IMU, base velocity estimate, etc.\n\n    def control_loop(self):\n        """Run policy inference and publish commands"""\n        with torch.no_grad():\n            obs_tensor = torch.tensor(self.obs, dtype=torch.float32).unsqueeze(0)\n            actions = self.policy(obs_tensor).squeeze(0).numpy()\n\n        # Publish joint commands\n        cmd = Float64MultiArray()\n        cmd.data = actions.tolist()\n        self.cmd_pub.publish(cmd)\n\ndef main():\n    rclpy.init()\n    node = RLPolicyNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"fine-tuning-with-real-data",children:"Fine-Tuning with Real Data"}),"\n",(0,r.jsxs)(e.p,{children:["If sim-to-real gap is large, ",(0,r.jsx)(e.strong,{children:"fine-tune"})," the policy with real-world data:"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Collect real trajectories"}),": Run policy on real robot, record (s, a, s', r)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Fine-tune with RL"}),": Continue PPO training with real data (100-1000 episodes)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Residual learning"}),": Train a residual policy \u0394\u03c0 that corrects sim policy"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Policy ignores critical features"}),": Train longer or increase network capacity"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Actuator saturation"}),": Reduce action magnitude or add action penalties"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unmodeled delays"}),": Increase latency randomization in sim"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Contact dynamics mismatch"}),": Tune ground friction/compliance parameters"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Start simple"}),": Train walking before running, grasping before manipulation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Validate in sim first"}),": Achieve high success rate (>90%) in sim before real deployment"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Monitor safety"}),": Implement emergency stop if policy diverges"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Log everything"}),": Record all sensor data and actions for post-hoc analysis"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Iterate"}),": Analyze real failures, add corresponding randomization, retrain"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"Key takeaways from this chapter:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reinforcement learning"})," enables robots to learn complex behaviors through trial-and-error in simulation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"PPO and SAC"})," are the dominant RL algorithms for robotics, balancing sample efficiency and stability"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"IsaacGymEnvs"})," enables massively parallel training (thousands of robots) for rapid policy learning"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Domain randomization"})," is essential for sim-to-real transfer, training policies robust to physics and sensor variations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Successful deployment"})," requires careful reward shaping, extensive randomization, and real-world fine-tuning"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"These techniques have enabled breakthrough achievements in quadruped locomotion, dexterous manipulation, and bipedal walking for Physical AI systems."}),"\n",(0,r.jsx)(e.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"What is the reality gap, and why does it make sim-to-real transfer challenging?"}),"\n",(0,r.jsx)(e.li,{children:"Explain the difference between on-policy (PPO) and off-policy (SAC) RL algorithms. When would you choose each?"}),"\n",(0,r.jsx)(e.li,{children:"How does domain randomization help with sim-to-real transfer? Provide three specific examples of parameters to randomize."}),"\n",(0,r.jsx)(e.li,{children:"Why is reward shaping critical for RL, and what are the principles for designing good reward functions?"}),"\n",(0,r.jsx)(e.li,{children:"Describe the IsaacGymEnvs workflow for training a quadruped locomotion policy from scratch."}),"\n",(0,r.jsx)(e.li,{children:"What are the trade-offs between training in simulation vs. fine-tuning on real hardware?"}),"\n",(0,r.jsx)(e.li,{children:"How would you debug a policy that works in simulation but fails on real hardware?"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,r.jsx)(e.h3,{id:"exercise-1-train-navigation-policy",children:"Exercise 1: Train Navigation Policy"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Objective"}),": Train a PPO policy for point-to-point navigation in Isaac Sim."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Set up IsaacGymEnvs environment for mobile robot"}),"\n",(0,r.jsx)(e.li,{children:"Define observation space (LiDAR, goal position, current velocity)"}),"\n",(0,r.jsx)(e.li,{children:"Define action space (linear velocity, angular velocity)"}),"\n",(0,r.jsx)(e.li,{children:"Design reward function (distance to goal, collision penalty)"}),"\n",(0,r.jsx)(e.li,{children:"Train for 5M steps and evaluate success rate"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Expected Outcome"}),": Policy achieves >80% success rate on navigation tasks in sim."]}),"\n",(0,r.jsx)(e.h3,{id:"exercise-2-apply-domain-randomization",children:"Exercise 2: Apply Domain Randomization"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Objective"}),": Improve sim-to-real transfer with domain randomization."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Implement physics randomization (mass, friction, damping)"}),"\n",(0,r.jsx)(e.li,{children:"Implement visual randomization (textures, lighting)"}),"\n",(0,r.jsx)(e.li,{children:"Add sensor noise (IMU, encoders)"}),"\n",(0,r.jsx)(e.li,{children:"Retrain policy with randomization"}),"\n",(0,r.jsx)(e.li,{children:"Compare sim-to-real transfer with and without DR"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Expected Outcome"}),": DR improves real-world success rate by 30-50%."]}),"\n",(0,r.jsx)(e.h3,{id:"exercise-3-deploy-policy-on-real-robot",children:"Exercise 3: Deploy Policy on Real Robot"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Objective"}),": Export and deploy trained policy on real hardware."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Export policy as TorchScript"}),"\n",(0,r.jsx)(e.li,{children:"Create ROS 2 node for policy inference"}),"\n",(0,r.jsx)(e.li,{children:"Test in controlled environment (soft ground, low speed)"}),"\n",(0,r.jsx)(e.li,{children:"Collect failure data and analyze"}),"\n",(0,r.jsx)(e.li,{children:"Retrain with additional randomization based on failures"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Expected Outcome"}),": Successful deployment with >70% success rate on real robot."]}),"\n",(0,r.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"PPO Paper"}),': "Proximal Policy Optimization Algorithms" (Schulman et al., 2017)']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"SAC Paper"}),': "Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL" (Haarnoja et al., 2018)']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"IsaacGymEnvs Documentation"}),": ",(0,r.jsx)(e.a,{href:"https://github.com/NVIDIA-Omniverse/IsaacGymEnvs",children:"https://github.com/NVIDIA-Omniverse/IsaacGymEnvs"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Domain Randomization"}),': "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization" (Peng et al., 2018)']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Learning Dexterity"}),": OpenAI's Dactyl project (",(0,r.jsx)(e.a,{href:"https://openai.com/research/learning-dexterity",children:"https://openai.com/research/learning-dexterity"}),")"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Previous"}),": ",(0,r.jsx)(e.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-09/ch14-perception",children:"Chapter 14 - Perception Pipelines"}),"\n",(0,r.jsx)(e.strong,{children:"Next"}),": ",(0,r.jsx)(e.a,{href:"/AI_COURSE_book/docs/module-03-isaac/week-10/ch16-sensor-fusion",children:"Chapter 16 - Sensor Fusion for Robotics"})]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>t});var s=i(6540);const r={},o=s.createContext(r);function a(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);